{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import  SystemMessage,HumanMessage,AIMessage\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"YOUR_API_KEY\"\n",
    "\n",
    "# load url\n",
    "def load_url(url):\n",
    "    loader = RecursiveUrlLoader(url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "    text_spilter = RecursiveCharacterTextSplitter(chunk_size=1000 , chunk_overlap=100)\n",
    "    docs = text_spilter.split_documents(loader.load())\n",
    "    return docs\n",
    "\n",
    "# class Chat_with_ai():\n",
    "#     def __init__(self,data,query,k=3):\n",
    "#         self.data = data\n",
    "#         self.query = query\n",
    "#         self.k = k\n",
    "#         self.messages = [\n",
    "#             SystemMessage(content=\"You are a helpful assistant for LangChain python framework.\"),\n",
    "#             SystemMessage(content=\"**Reply in zh-tw**\"),\n",
    "#         ]\n",
    "#         self.db = self.data_to_db(data)\n",
    "#         self.transcript = self.get()\n",
    "    \n",
    "#     def data_to_db(self,data)->FAISS:\n",
    "#         embedding = OpenAIEmbeddings()\n",
    "#         db = FAISS.from_documents(data,embedding)\n",
    "#         return db\n",
    "    \n",
    "#     def get(self):\n",
    "#         docs =self.db.similarity_search(query = self.query, k = self.k)\n",
    "#         # print(docs)\n",
    "#         docs_page_content = ' '.join([doc for doc in docs])\n",
    "#         return docs_page_content\n",
    "    \n",
    "#     def chat(self):\n",
    "#         model = ChatOpenAI(\n",
    "#             openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#             model='gpt-3.5-turbo'\n",
    "#         )\n",
    "#         self.messages.append(SystemMessage(content=self.transcript))\n",
    "#         self.messages.append(HumanMessage(content=self.query))\n",
    "#         response =model.invoke(self.messages).content\n",
    "#         self.messages.append(AIMessage(content=response))\n",
    "#         return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', metadata={'source': 'https://python.langchain.com/docs/expression_language', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTube🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:Streaming support', metadata={'source': 'https://python.langchain.com/docs/expression_language', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks', metadata={'source': 'https://python.langchain.com/docs/expression_language', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing integration', metadata={'source': 'https://python.langchain.com/docs/expression_language', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment integration\\nAny chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousSecurityNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTube🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:Streaming support', metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks', metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing integration', metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment integration\\nAny chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousSecurityNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}), Document(page_content='How to | 🦜️🔗 Langchain', metadata={'source': 'https://python.langchain.com/docs/expression_language/how_to/', 'title': 'How to | 🦜️🔗 Langchain', 'language': 'en'}), Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTube🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toRunnableParallel: Manipulating dataRunnablePassthrough: Passing data throughRunnableLambda: Run Custom FunctionsRunnableBranch: Dynamically route logic based on inputBind runtime argsConfigure chain internals at runtimeCreate a runnable with the `@chain` decoratorAdd fallbacksStream custom generator functionsInspect your runnablesAdd message history (memory)CookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageHow toHow to📄️ RunnableParallel: Manipulating datamanipulating-inputs-output}📄️ RunnablePassthrough: Passing data', metadata={'source': 'https://python.langchain.com/docs/expression_language/how_to/', 'title': 'How to | 🦜️🔗 Langchain', 'language': 'en'}), Document(page_content='RunnableParallel: Manipulating datamanipulating-inputs-output}📄️ RunnablePassthrough: Passing data throughpassing-data-through}📄️ RunnableLambda: Run Custom Functionsrun-custom-functions}📄️ RunnableBranch: Dynamically route logic based on inputdynamically-route-logic-based-on-input}📄️ Bind runtime argsSometimes we want to invoke a Runnable within a Runnable sequence with📄️ Configure chain internals at runtimeOftentimes you may want to experiment with, or even expose to the end📄️ Create a runnable with the `@chain` decoratorYou can also turn an arbitrary function into a chain by adding a📄️ Add fallbacksThere are many possible points of failure in an LLM application, whether📄️ Stream custom generator functionsYou can use generator functions (ie. functions that use the yield📄️ Inspect your runnablesOnce you create a runnable with LCEL, you may often want to inspect it📄️ Add message history (memory)The RunnableWithMessageHistory lets us add message history to certainHelp us out by', metadata={'source': 'https://python.langchain.com/docs/expression_language/how_to/', 'title': 'How to | 🦜️🔗 Langchain', 'language': 'en'}), Document(page_content='history (memory)The RunnableWithMessageHistory lets us add message history to certainHelp us out by providing feedback on this documentation page:PreviousStreamingNextRunnableParallel: Manipulating dataCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/how_to/', 'title': 'How to | 🦜️🔗 Langchain', 'language': 'en'}), Document(page_content='Get started | 🦜️🔗 Langchain', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTube🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageGet startedOn this pageGet startedLCEL makes it easy to build complex chains from basic components, and\\nsupports out of the box functionality such as streaming, parallelism,\\nand logging.Basic example: prompt + model + output parser\\u200bThe most basic and common use case is chaining a prompt template and a\\nmodel together. To see how this works, let’s create a chain that takes a', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='model together. To see how this works, let’s create a chain that takes a\\ntopic and generates a joke:%pip install --upgrade --quiet  langchain-core langchain-community langchain-openaifrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIprompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")model = ChatOpenAI(model=\"gpt-4\")output_parser = StrOutputParser()chain = prompt | model | output_parserchain.invoke({\"topic\": \"ice cream\"})\"Why don\\'t ice creams ever get invited to parties?\\\\n\\\\nBecause they always drip when things heat up!\"Notice this line of this code, where we piece together then different\\ncomponents into a single chain using LCEL:chain = prompt | model | output_parserThe | symbol is similar to a unix pipe\\noperator, which chains\\ntogether the different components feeds the output from one component as', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='operator, which chains\\ntogether the different components feeds the output from one component as\\ninput into the next component.In this chain the user input is passed to the prompt template, then the\\nprompt template output is passed to the model, then the model output is\\npassed to the output parser. Let’s take a look at each component\\nindividually to really understand what’s going on.1. Prompt\\u200bprompt is a BasePromptTemplate, which means it takes in a dictionary\\nof template variables and produces a PromptValue. A PromptValue is a\\nwrapper around a completed prompt that can be passed to either an LLM\\n(which takes a string as input) or ChatModel (which takes a sequence\\nof messages as input). It can work with either language model type\\nbecause it defines logic both for producing BaseMessages and for', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='because it defines logic both for producing BaseMessages and for\\nproducing a string.prompt_value = prompt.invoke({\"topic\": \"ice cream\"})prompt_valueChatPromptValue(messages=[HumanMessage(content=\\'tell me a short joke about ice cream\\')])prompt_value.to_messages()[HumanMessage(content=\\'tell me a short joke about ice cream\\')]prompt_value.to_string()\\'Human: tell me a short joke about ice cream\\'2. Model\\u200bThe PromptValue is then passed to model. In this case our model is\\na ChatModel, meaning it will output a BaseMessage.message = model.invoke(prompt_value)messageAIMessage(content=\"Why don\\'t ice creams ever get invited to parties?\\\\n\\\\nBecause they always bring a melt down!\")If our model was an LLM, it would output a string.from langchain_openai.llms import OpenAIllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")llm.invoke(prompt_value)\\'\\\\n\\\\nRobot: Why did the ice cream truck break down? Because it had a meltdown!\\'3. Output parser\\u200bAnd lastly we pass our model output to the output_parser, which is a', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='BaseOutputParser meaning it takes either a string or a BaseMessage\\nas input. The StrOutputParser specifically simple converts any input\\ninto a string.output_parser.invoke(message)\"Why did the ice cream go to therapy? \\\\n\\\\nBecause it had too many toppings and couldn\\'t find its cone-fidence!\"4. Entire Pipeline\\u200bTo follow the steps along:We pass in user input on the desired topic as\\n{\"topic\": \"ice cream\"}The prompt component takes the user input, which is then used to\\nconstruct a PromptValue after using the topic to construct the\\nprompt.The model component takes the generated prompt, and passes into\\nthe OpenAI LLM model for evaluation. The generated output from the\\nmodel is a ChatMessage object.Finally, the output_parser component takes in a ChatMessage, and\\ntransforms this into a Python string, which is returned from the\\ninvoke method.Note that if you’re curious about the output of any components, you can\\nalways test out a smaller version of the chain such as prompt or', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='always test out a smaller version of the chain such as prompt or\\nprompt | model to see the intermediate results:input = {\"topic\": \"ice cream\"}prompt.invoke(input)# > ChatPromptValue(messages=[HumanMessage(content=\\'tell me a short joke about ice cream\\')])(prompt | model).invoke(input)# > AIMessage(content=\"Why did the ice cream go to therapy?\\\\nBecause it had too many toppings and couldn\\'t cone-trol itself!\")RAG Search Example\\u200bFor our next example, we want to run a retrieval-augmented generation', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='chain to add some context when responding to questions.# Requires:# pip install langchain docarray tiktokenfrom langchain_community.vectorstores import DocArrayInMemorySearchfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableParallel, RunnablePassthroughfrom langchain_openai.chat_models import ChatOpenAIfrom langchain_openai.embeddings import OpenAIEmbeddingsvectorstore = DocArrayInMemorySearch.from_texts(    [\"harrison worked at kensho\", \"bears like to eat honey\"],    embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()output_parser = StrOutputParser()setup_and_retrieval = RunnableParallel(    {\"context\": retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | prompt | model |', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | prompt | model | output_parserchain.invoke(\"where did harrison work?\")In this case, the composed chain is:chain = setup_and_retrieval | prompt | model | output_parserTo explain this, we first can see that the prompt template above takes', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='in context and question as values to be substituted in the prompt.\\nBefore building the prompt template, we want to retrieve relevant\\ndocuments to the search and include them as part of the context.As a preliminary step, we’ve setup the retriever using an in memory\\nstore, which can retrieve documents based on a query. This is a runnable\\ncomponent as well that can be chained together with other components,\\nbut you can also try to run it separately:retriever.invoke(\"where did harrison work?\")We then use the RunnableParallel to prepare the expected inputs into\\nthe prompt by using the entries for the retrieved documents as well as\\nthe original user question, using the retriever for document search, and', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='the original user question, using the retriever for document search, and\\nRunnablePassthrough to pass the user’s question:setup_and_retrieval = RunnableParallel(    {\"context\": retriever, \"question\": RunnablePassthrough()})To review, the complete chain is:setup_and_retrieval = RunnableParallel(    {\"context\": retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | prompt | model | output_parserWith the flow being:The first steps create a RunnableParallel object with two entries.\\nThe first entry, context will include the document results fetched\\nby the retriever. The second entry, question will contain the\\nuser’s original question. To pass on the question, we use\\nRunnablePassthrough to copy this entry.Feed the dictionary from the step above to the prompt component.\\nIt then takes the user input which is question as well as the\\nretrieved document which is context to construct a prompt and', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='retrieved document which is context to construct a prompt and\\noutput a PromptValue.  The model component takes the generated prompt, and passes into\\nthe OpenAI LLM model for evaluation. The generated output from the\\nmodel is a ChatMessage object.Finally, the output_parser component takes in a ChatMessage, and\\ntransforms this into a Python string, which is returned from the\\ninvoke method.Next steps\\u200bWe recommend reading our Why use LCEL\\nsection next to see a side-by-side comparison of the code needed to\\nproduce common functionality with and without LCEL.Help us out by providing feedback on this documentation page:PreviousLangChain Expression Language (LCEL)NextWhy use LCELBasic example: prompt + model + output parser1. Prompt2. Model3. Output parser4. Entire PipelineRAG Search ExampleNext stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | 🦜️🔗 Langchain', 'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en'}), Document(page_content='Why use LCEL | 🦜️🔗 Langchain', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTube🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageWhy use LCELOn this pageWhy use LCELWe recommend reading the LCEL Get\\nstarted section first.LCEL makes it easy to build complex chains from basic components. It\\ndoes this by providing: 1. A unified interface: Every LCEL object\\nimplements the Runnable interface, which defines a common set of\\ninvocation methods (invoke, batch, stream, ainvoke, …). This\\nmakes it possible for chains of LCEL objects to also automatically\\nsupport these invocations. That is, every chain of LCEL objects is', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='support these invocations. That is, every chain of LCEL objects is\\nitself an LCEL object. 2. Composition primitives: LCEL provides a\\nnumber of primitives that make it easy to compose chains, parallelize\\ncomponents, add fallbacks, dynamically configure chain internal, and\\nmore.To better understand the value of LCEL, it’s helpful to see it in action\\nand think about how we might recreate similar functionality without it.\\nIn this walkthrough we’ll do just that with our basic\\nexample from the\\nget started section. We’ll take our simple prompt + model chain, which\\nunder the hood already defines a lot of functionality, and see what it\\nwould take to recreate all of it.%pip install –upgrade –quiet langchain-core langchain-openai\\nlangchain-anthropicInvoke\\u200bIn the simplest case, we just want to pass in a topic string and get', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='back a joke string:Without LCEL\\u200bfrom typing import Listimport openaiprompt_template = \"Tell me a short joke about {topic}\"client = openai.OpenAI()def call_chat_model(messages: List[dict]) -> str:    response = client.chat.completions.create(        model=\"gpt-3.5-turbo\",         messages=messages,    )    return response.choices[0].message.contentdef invoke_chain(topic: str) -> str:    prompt_value = prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", \"content\": prompt_value}]    return call_chat_model(messages)invoke_chain(\"ice cream\")LCEL\\u200bfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughprompt = ChatPromptTemplate.from_template(    \"Tell me a short joke about {topic}\")output_parser = StrOutputParser()model = ChatOpenAI(model=\"gpt-3.5-turbo\")chain = (    {\"topic\": RunnablePassthrough()}     | prompt    | model    |', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='= (    {\"topic\": RunnablePassthrough()}     | prompt    | model    | output_parser)chain.invoke(\"ice cream\") Stream\\u200bIf we want to stream results instead, we’ll need to change our function: Without LCEL\\u200bfrom typing import Iteratordef stream_chat_model(messages: List[dict]) -> Iterator[str]:    stream = client.chat.completions.create(        model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    for response in stream:        content = response.choices[0].delta.content        if content is not None:            yield contentdef stream_chain(topic: str) -> Iterator[str]:    prompt_value = prompt.format(topic=topic)    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])for chunk in stream_chain(\"ice cream\"):    print(chunk, end=\"\", flush=True)LCEL\\u200bfor chunk in chain.stream(\"ice cream\"):    print(chunk, end=\"\", flush=True) Batch\\u200bIf we want to run on a batch of inputs in parallel, we’ll again need a', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='new function: Without LCEL\\u200bfrom concurrent.futures import ThreadPoolExecutordef batch_chain(topics: list) -> list:    with ThreadPoolExecutor(max_workers=5) as executor:        return list(executor.map(invoke_chain, topics))batch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])LCEL\\u200bchain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"]) Async\\u200bIf we need an asynchronous version: Without LCEL\\u200basync_client = openai.AsyncOpenAI()async def acall_chat_model(messages: List[dict]) -> str:    response = await async_client.chat.completions.create(        model=\"gpt-3.5-turbo\",         messages=messages,    )    return response.choices[0].message.contentasync def ainvoke_chain(topic: str) -> str:    prompt_value = prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", \"content\": prompt_value}]    return await acall_chat_model(messages)await ainvoke_chain(\"ice cream\")LCEL\\u200bchain.ainvoke(\"ice cream\") LLM instead of chat model\\u200bIf we want to use a completion endpoint instead of a chat', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='cream\") LLM instead of chat model\\u200bIf we want to use a completion endpoint instead of a chat endpoint: Without LCEL\\u200bdef call_llm(prompt_value: str) -> str:    response = client.completions.create(        model=\"gpt-3.5-turbo-instruct\",        prompt=prompt_value,    )    return response.choices[0].textdef invoke_llm_chain(topic: str) -> str:    prompt_value = prompt_template.format(topic=topic)    return call_llm(prompt_value)invoke_llm_chain(\"ice cream\")LCEL\\u200bfrom langchain_openai import OpenAIllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")llm_chain = (    {\"topic\": RunnablePassthrough()}     | prompt    | llm    | output_parser)llm_chain.invoke(\"ice cream\") Different model provider\\u200bIf we want to use Anthropic instead of OpenAI: Without LCEL\\u200bimport anthropicanthropic_template = f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"anthropic_client = anthropic.Anthropic()def call_anthropic(prompt_value: str) -> str:    response = anthropic_client.completions.create(        model=\"claude-2\",', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='str) -> str:    response = anthropic_client.completions.create(        model=\"claude-2\",        prompt=prompt_value,        max_tokens_to_sample=256,    )    return response.completion    def invoke_anthropic_chain(topic: str) -> str:    prompt_value = anthropic_template.format(topic=topic)    return call_anthropic(prompt_value)invoke_anthropic_chain(\"ice cream\")LCEL\\u200bfrom langchain_anthropic import ChatAnthropicanthropic = ChatAnthropic(model=\"claude-2\")anthropic_chain = (    {\"topic\": RunnablePassthrough()}     | prompt     | anthropic    | output_parser)anthropic_chain.invoke(\"ice cream\") Runtime configurability\\u200bIf we wanted to make the choice of chat model or LLM configurable at', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='runtime: Without LCEL\\u200bdef invoke_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> str:    if model == \"chat_openai\":        return invoke_chain(topic)    elif model == \"openai\":        return invoke_llm_chain(topic)    elif model == \"anthropic\":        return invoke_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def stream_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> Iterator[str]:    if model == \"chat_openai\":        return stream_chain(topic)    elif model == \"openai\":        # Note we haven\\'t implemented this yet.        return stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we haven\\'t implemented this yet        return stream_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai,', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def batch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    # You get the idea    ...async def abatch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    ...invoke_configurable_chain(\"ice cream\", model=\"openai\")stream = stream_configurable_chain(    \"ice_cream\",     model=\"anthropic\")for chunk in stream:    print(chunk, end=\"\", flush=True)# batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])# await ainvoke_configurable_chain(\"ice cream\")With LCEL\\u200bfrom langchain_core.runnables import ConfigurableFieldconfigurable_model = model.configurable_alternatives(    ConfigurableField(id=\"model\"),     default_key=\"chat_openai\",     openai=llm,    anthropic=anthropic,)configurable_chain = (    {\"topic\": RunnablePassthrough()}     | prompt     | configurable_model     |', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='= (    {\"topic\": RunnablePassthrough()}     | prompt     | configurable_model     | output_parser)configurable_chain.invoke(    \"ice cream\",     config={\"model\": \"openai\"})stream = configurable_chain.stream(    \"ice cream\",     config={\"model\": \"anthropic\"})for chunk in stream:    print(chunk, end=\"\", flush=True)configurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])# await configurable_chain.ainvoke(\"ice cream\") Logging\\u200bIf we want to log our intermediate results: Without LCEL\\u200bWe’ll print intermediate steps for illustrative purposesdef invoke_anthropic_chain_with_logging(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = anthropic_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    output = call_anthropic(prompt_value)    print(f\"Output: {output}\")    return outputinvoke_anthropic_chain_with_logging(\"ice cream\")LCEL\\u200bEvery component has built-in integrations with LangSmith. If we set the', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='following two environment variables, all chain traces are logged to\\nLangSmith.import osos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"anthropic_chain.invoke(\"ice cream\")Here’s what our LangSmith trace looks like:', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/r Fallbacks\\u200bIf we wanted to add fallback logic, in case one model API is down: Without LCEL\\u200bdef invoke_chain_with_fallback(topic: str) -> str:    try:        return invoke_chain(topic)    except Exception:        return invoke_anthropic_chain(topic)async def ainvoke_chain_with_fallback(topic: str) -> str:    try:        return await ainvoke_chain(topic)    except Exception:        # Note: we haven\\'t actually implemented this.        return ainvoke_anthropic_chain(topic)async def batch_chain_with_fallback(topics: List[str]) -> str:    try:        return batch_chain(topics)    except Exception:        # Note: we haven\\'t actually implemented this.        return batch_anthropic_chain(topics)invoke_chain_with_fallback(\"ice cream\")# await ainvoke_chain_with_fallback(\"ice cream\")batch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"]))LCEL\\u200bfallback_chain =', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='cream\")batch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"]))LCEL\\u200bfallback_chain = chain.with_fallbacks([anthropic_chain])fallback_chain.invoke(\"ice cream\")# await fallback_chain.ainvoke(\"ice cream\")fallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"]) Full code comparison\\u200bEven in this simple case, our LCEL chain succinctly packs in a lot of', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='functionality. As chains become more complex, this becomes especially', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='valuable. Without LCEL\\u200bfrom concurrent.futures import ThreadPoolExecutorfrom typing import Iterator, List, Tupleimport anthropicimport openaiprompt_template = \"Tell me a short joke about {topic}\"anthropic_template = f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"client = openai.OpenAI()async_client = openai.AsyncOpenAI()anthropic_client = anthropic.Anthropic()def call_chat_model(messages: List[dict]) -> str:    response = client.chat.completions.create(        model=\"gpt-3.5-turbo\",         messages=messages,    )    return response.choices[0].message.contentdef invoke_chain(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = prompt_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    messages = [{\"role\": \"user\", \"content\": prompt_value}]    output = call_chat_model(messages)    print(f\"Output: {output}\")    return outputdef stream_chat_model(messages: List[dict]) -> Iterator[str]:    stream = client.chat.completions.create(', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='List[dict]) -> Iterator[str]:    stream = client.chat.completions.create(        model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    for response in stream:        content = response.choices[0].delta.content        if content is not None:            yield contentdef stream_chain(topic: str) -> Iterator[str]:    print(f\"Input: {topic}\")    prompt_value = prompt.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])    for chunk in stream:        print(f\"Token: {chunk}\", end=\"\")        yield chunkdef batch_chain(topics: list) -> list:    with ThreadPoolExecutor(max_workers=5) as executor:        return list(executor.map(invoke_chain, topics))def call_llm(prompt_value: str) -> str:    response = client.completions.create(        model=\"gpt-3.5-turbo-instruct\",        prompt=prompt_value,    )    return response.choices[0].textdef invoke_llm_chain(topic: str) -> str:', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content=')    return response.choices[0].textdef invoke_llm_chain(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = promtp_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    output = call_llm(prompt_value)    print(f\"Output: {output}\")    return outputdef call_anthropic(prompt_value: str) -> str:    response = anthropic_client.completions.create(        model=\"claude-2\",        prompt=prompt_value,        max_tokens_to_sample=256,    )    return response.completion   def invoke_anthropic_chain(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = anthropic_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    output = call_anthropic(prompt_value)    print(f\"Output: {output}\")    return outputasync def ainvoke_anthropic_chain(topic: str) -> str:    ...def stream_anthropic_chain(topic: str) -> Iterator[str]:    ...def batch_anthropic_chain(topics: List[str]) -> List[str]:    ...def invoke_configurable_chain(', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='batch_anthropic_chain(topics: List[str]) -> List[str]:    ...def invoke_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> str:    if model == \"chat_openai\":        return invoke_chain(topic)    elif model == \"openai\":        return invoke_llm_chain(topic)    elif model == \"anthropic\":        return invoke_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def stream_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> Iterator[str]:    if model == \"chat_openai\":        return stream_chain(topic)    elif model == \"openai\":        # Note we haven\\'t implemented this yet.        return stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we haven\\'t implemented this yet        return stream_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def batch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    ...async def abatch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    ...def invoke_chain_with_fallback(topic: str) -> str:    try:        return invoke_chain(topic)    except Exception:        return invoke_anthropic_chain(topic)async def ainvoke_chain_with_fallback(topic: str) -> str:    try:        return await ainvoke_chain(topic)    except Exception:        return ainvoke_anthropic_chain(topic)async def batch_chain_with_fallback(topics: List[str]) -> str:    try:        return batch_chain(topics)    except Exception:        return batch_anthropic_chain(topics)LCEL\\u200bimport osfrom langchain_anthropic import ChatAnthropicfrom langchain_openai import ChatOpenAIfrom langchain_openai', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='import ChatAnthropicfrom langchain_openai import ChatOpenAIfrom langchain_openai import OpenAIfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthrough, ConfigurableFieldos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"prompt = ChatPromptTemplate.from_template(    \"Tell me a short joke about {topic}\")chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")anthropic = ChatAnthropic(model=\"claude-2\")model = (    chat_openai    .with_fallbacks([anthropic])    .configurable_alternatives(        ConfigurableField(id=\"model\"),        default_key=\"chat_openai\",        openai=openai,        anthropic=anthropic,    ))chain = (    {\"topic\": RunnablePassthrough()}     | prompt     | model     | StrOutputParser()) Next steps\\u200bTo continue learning about LCEL, we recommend: - Reading up on the full', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='LCEL Interface, which we’ve only\\npartially covered here. - Exploring the\\nHow-to section to learn about\\nadditional composition primitives that LCEL provides. - Looking through\\nthe Cookbook section to see LCEL\\nin action for common use cases. A good next use case to look at would be\\nRetrieval-augmented\\ngeneration.Help us out by providing feedback on this documentation page:PreviousGet startedNextInterfaceInvokeStreamBatchAsyncLLM instead of chat modelDifferent model providerRuntime configurabilityLoggingFallbacksFull code comparisonNext stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/why', 'title': 'Why use LCEL | 🦜️🔗 Langchain', 'description': 'We recommend reading the LCEL [Get', 'language': 'en'}), Document(page_content='Streaming | 🦜️🔗 Langchain', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTube🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageStreamingOn this pageStreaming With LangChainStreaming is critical in making applications based on LLMs feel\\nresponsive to end-users.Important LangChain primitives like LLMs, parsers, prompts, retrievers,\\nand agents implement the LangChain Runnable\\nInterface.This interface provides two general approaches to stream content:sync stream and async astream: a default implementation of', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='streaming that streams the final output from the chain.async astream_events and async astream_log: these provide a way\\nto stream both intermediate steps and final output from the\\nchain.Let’s take a look at both approaches, and try to understand how to use\\nthem. 🥷Using Stream\\u200bAll Runnable objects implement a sync method called stream and an\\nasync variant called astream.These methods are designed to stream the final output in chunks,\\nyielding each chunk as soon as it is available.Streaming is only possible if all steps in the program know how to\\nprocess an input stream; i.e., process an input chunk one at a time,\\nand yield a corresponding output chunk.The complexity of this processing can vary, from straightforward tasks\\nlike emitting tokens produced by an LLM, to more challenging ones like\\nstreaming parts of JSON results before the entire JSON is complete.The best place to start exploring streaming is with the single most', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='important components in LLMs apps– the LLMs themselves!LLMs and Chat Models\\u200bLarge language models and their chat variants are the primary bottleneck\\nin LLM based apps. 🙊Large language models can take several seconds to generate a\\ncomplete response to a query. This is far slower than the ~200-300\\nms threshold at which an application feels responsive to an end user.The key strategy to make the application feel more responsive is to show\\nintermediate progress; viz., to stream the output from the model token\\nby token.We will show examples of streaming using the chat model from\\nAnthropic. To use the model,\\nyou will need to install the langchain-anthropic package. You can do', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='Anthropic. To use the model,\\nyou will need to install the langchain-anthropic package. You can do\\nthis with the following command:pip install -qU langchain-anthropic# Showing the example using anthropic, but you can use# your favorite chat model!from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic()chunks = []async for chunk in model.astream(\"hello. tell me something about yourself\"):    chunks.append(chunk)    print(chunk.content, end=\"|\", flush=True) Hello|!| My| name| is| Claude|.| I|\\'m| an| AI| assistant| created| by| An|throp|ic| to| be| helpful|,| harmless|,| and| honest|.||Let’s inspect one of the chunkschunks[0]AIMessageChunk(content=\\' Hello\\')We got back something called an AIMessageChunk. This chunk represents\\na part of an AIMessage.Message chunks are additive by design – one can simply add them up to', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"a part of an AIMessage.Message chunks are additive by design – one can simply add them up to\\nget the state of the response so far!chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]AIMessageChunk(content=' Hello! My name is')Chains\\u200bVirtually all LLM applications involve more steps than just a call to a\\nlanguage model.Let’s build a simple chain using LangChain Expression Language\\n(LCEL) that combines a prompt, model and a parser and verify that\\nstreaming works.We will use StrOutputParser to parse the output from the model. This\\nis a simple parser that extracts the content field from an\\nAIMessageChunk, giving us the token returned by the model.tipLCEL is a declarative way to specify a “program” by chainining\\ntogether different LangChain primitives. Chains created using LCEL\\nbenefit from an automatic implementation of stream and astream\\nallowing streaming of the final output. In fact, chains created with\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='allowing streaming of the final output. In fact, chains created with\\nLCEL implement the entire standard Runnable interface.from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = StrOutputParser()chain = prompt | model | parserasync for chunk in chain.astream({\"topic\": \"parrot\"}):    print(chunk, end=\"|\", flush=True) Here|\\'s| a| silly| joke| about| a| par|rot|:|What| kind| of| teacher| gives| good| advice|?| An| ap|-|parent| (|app|arent|)| one|!||noteYou do not have to use the LangChain Expression Language to use\\nLangChain and can instead rely on a standard imperative programming\\napproach by caling invoke, batch or stream on each component\\nindividually, assigning the results to variables and then using them', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='individually, assigning the results to variables and then using them\\ndownstream as you see fit.If that works for your needs, then that’s fine by us 👌!Working with Input Streams\\u200bWhat if you wanted to stream JSON from the output as it was being\\ngenerated?If you were to rely on json.loads to parse the partial json, the\\nparsing would fail as the partial json wouldn’t be valid json.You’d likely be at a complete loss of what to do and claim that it\\nwasn’t possible to stream JSON.Well, turns out there is a way to do it – the parser needs to operate on\\nthe input stream, and attempt to “auto-complete” the partial json', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='into a valid state.Let’s see such a parser in action to understand what this means.from langchain_core.output_parsers import JsonOutputParserchain = (    model | JsonOutputParser())  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some modelsasync for text in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'):    print(text, flush=True){}{\\'countries\\': []}{\\'countries\\': [{}]}{\\'countries\\': [{\\'name\\': \\'\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 6739}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 673915}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {}]}{\\'countries\\': [{\\'name\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"67391582}]}{'countries': [{'name': 'France', 'population': 67391582}, {}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': ''}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Sp'}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain'}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 4675}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 467547}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': ''}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"{'name': ''}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan'}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 12}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 12647}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 1264764}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 126476461}]}Now, let’s break streaming. We’ll use the previous example and\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='append an extraction function at the end that extracts the country names\\nfrom the finalized JSON.dangerAny steps in the chain that operate on finalized inputs rather than\\non input streams can break streaming functionality via stream or\\nastream.tipLater, we will discuss the astream_events API which streams results\\nfrom intermediate steps. This API will stream results from intermediate\\nsteps even if the chain contains steps that only operate on finalized', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='inputs.from langchain_core.output_parsers import (    JsonOutputParser,)# A function that operates on finalized inputs# rather than on an input_streamdef _extract_country_names(inputs):    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if \"countries\" not in inputs:        return \"\"    countries = inputs[\"countries\"]    if not isinstance(countries, list):        return \"\"    country_names = [        country.get(\"name\") for country in countries if isinstance(country, dict)    ]    return country_nameschain = model | JsonOutputParser() | _extract_country_namesasync for text in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'):    print(text, end=\"|\", flush=True)[\\'France\\', \\'Spain\\', \\'Japan\\']|Generator', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='and `population`\\'):    print(text, end=\"|\", flush=True)[\\'France\\', \\'Spain\\', \\'Japan\\']|Generator Functions\\u200bLe’ts fix the streaming using a generator function that can operate on', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='the input stream.tipA generator function (a function that uses yield) allows writing code', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='that operators on input streamsfrom langchain_core.output_parsers import JsonOutputParserasync def _extract_country_names_streaming(input_stream):    \"\"\"A function that operates on input streams.\"\"\"    country_names_so_far = set()    async for input in input_stream:        if not isinstance(input, dict):            continue        if \"countries\" not in input:            continue        countries = input[\"countries\"]        if not isinstance(countries, list):            continue        for country in countries:            name = country.get(\"name\")            if not name:                continue            if name not in country_names_so_far:                yield name                country_names_so_far.add(name)chain = model | JsonOutputParser() | _extract_country_names_streamingasync for text in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries.', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'):    print(text, end=\"|\", flush=True)France|Sp|Spain|Japan|noteBecause the code above is relying on JSON auto-completion, you may see', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='partial names of countries (e.g., Sp and Spain), which is not what\\none would want for an extraction result!We’re focusing on streaming concepts, not necessarily the results of the\\nchains.Non-streaming components\\u200bSome built-in components like Retrievers do not offer any streaming.', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='What happens if we try to stream them? 🤨from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    [\"harrison worked at kensho\", \"harrison likes spicy food\"],    embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]chunks[[Document(page_content=\\'harrison worked at kensho\\'),  Document(page_content=\\'harrison likes spicy food\\')]]Stream just yielded the final result from that component.This is OK \\U0001f979! Not all components have to implement streaming – in some', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='cases streaming is either unnecessary, difficult or just doesn’t make\\nsense.tipAn LCEL chain constructed using non-streaming components, will still be\\nable to stream in a lot of cases, with streaming of partial output', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='able to stream in a lot of cases, with streaming of partial output\\nstarting after the last non-streaming step in the chain.retrieval_chain = (    {        \"context\": retriever.with_config(run_name=\"Docs\"),        \"question\": RunnablePassthrough(),    }    | prompt    | model    | StrOutputParser())for chunk in retrieval_chain.stream(    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"):    print(chunk, end=\"|\", flush=True) Based| on| the| given| context|,| the| only| information| provided| about| where| Harrison| worked| is| that| he| worked| at| Ken|sh|o|.| Since| there| are| no| other| details| provided| about| Ken|sh|o|,| I| do| not| have| enough| information| to| write| 3| additional| made| up| sentences| about| this| place|.| I| can| only| state| that| Harrison| worked| at| Ken|sh|o|.||Now that we’ve seen how stream and astream work, let’s venture into', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"the world of streaming events. 🏞️Using Stream Events\\u200bEvent Streaming is a beta API. This API may change a bit based on\\nfeedback.noteIntroduced in langchain-core 0.1.14.import langchain_corelangchain_core.__version__'0.1.18'For the astream_events API to work properly:Use async throughout the code to the extent possible (e.g., async\\ntools etc)Propagate callbacks if defining custom functions / runnablesWhenever using runnables without LCEL, make sure to call\\n.astream() on LLMs rather than .ainvoke to force the LLM to\\nstream tokens.Let us know if anything doesn’t work as expected! :)Event Reference\\u200bBelow is a reference table that shows some events that might be emitted\\nby the various Runnable objects.noteWhen streaming is implemented properly, the inputs to a runnable will\\nnot be known until after the input stream has been entirely consumed.\\nThis means that inputs will often be included only for end events\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='and rather than for start events.eventnamechunkinputoutputon_chat_model_start[model name]{“messages”: [[SystemMessage, HumanMessage]]}on_chat_model_stream[model name]AIMessageChunk(content=“hello”)on_chat_model_end[model name]{“messages”: [[SystemMessage, HumanMessage]]}{“generations”: […], “llm_output”: None, …}on_llm_start[model name]{‘input’: ‘hello’}on_llm_stream[model name]‘Hello’on_llm_end[model name]‘Hello human!’on_chain_startformat_docson_chain_streamformat_docs“hello world!, goodbye world!”on_chain_endformat_docs[Document(…)]“hello world!, goodbye world!”on_tool_startsome_tool{“x”: 1, “y”: “2”}on_tool_streamsome_tool{“x”: 1, “y”: “2”}on_tool_endsome_tool{“x”: 1, “y”: “2”}on_retriever_start[retriever name]{“query”: “hello”}on_retriever_chunk[retriever name]{documents: […]}on_retriever_end[retriever name]{“query”: “hello”}{documents: […]}on_prompt_start[template_name]{“question”: “hello”}on_prompt_end[template_name]{“question”: “hello”}ChatPromptValue(messages: [SystemMessage,', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='“hello”}on_prompt_end[template_name]{“question”: “hello”}ChatPromptValue(messages: [SystemMessage, …])Chat Model\\u200bLet’s start off by looking at the events produced by a chat model.events = []async for event in model.astream_events(\"hello\", version=\"v1\"):    events.append(event)/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.  warn_beta(noteHey what’s that funny version=“v1” parameter in the API?! 😾This is a beta API, and we’re almost certainly going to make some', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='changes to it.This version parameter will allow us to minimize such breaking changes\\nto your code.In short, we are annoying you now, so we don’t have to annoy you later.Let’s take a look at the few of the start event and a few of the end', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"events.events[:3][{'event': 'on_chat_model_start',  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'name': 'ChatAnthropic',  'tags': [],  'metadata': {},  'data': {'input': 'hello'}}, {'event': 'on_chat_model_stream',  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': [],  'metadata': {},  'name': 'ChatAnthropic',  'data': {'chunk': AIMessageChunk(content=' Hello')}}, {'event': 'on_chat_model_stream',  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': [],  'metadata': {},  'name': 'ChatAnthropic',  'data': {'chunk': AIMessageChunk(content='!')}}]events[-2:][{'event': 'on_chat_model_stream',  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': [],  'metadata': {},  'name': 'ChatAnthropic',  'data': {'chunk': AIMessageChunk(content='')}}, {'event': 'on_chat_model_end',  'name': 'ChatAnthropic',  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': [],  'metadata': {},  'data': {'output': AIMessageChunk(content=' Hello!')}}]Chain\\u200bLet’s revisit the\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"'metadata': {},  'data': {'output': AIMessageChunk(content=' Hello!')}}]Chain\\u200bLet’s revisit the example chain that parsed streaming JSON to explore\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='the streaming events API.chain = (    model | JsonOutputParser())  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some modelsevents = [    event    async for event in chain.astream_events(        \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',        version=\"v1\",    )]If you examine at the first few events, you’ll notice that there are', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='3 different start events rather than 2 start events.The three start events correspond to:The chain (model + parser)The modelThe parserevents[:3][{\\'event\\': \\'on_chain_start\\',  \\'run_id\\': \\'b1074bff-2a17-458b-9e7b-625211710df4\\',  \\'name\\': \\'RunnableSequence\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'data\\': {\\'input\\': \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'}}, {\\'event\\': \\'on_chat_model_start\\',  \\'name\\': \\'ChatAnthropic\\',  \\'run_id\\': \\'6072be59-1f43-4f1c-9470-3b92e8406a99\\',  \\'tags\\': [\\'seq:step:1\\'],  \\'metadata\\': {},  \\'data\\': {\\'input\\': {\\'messages\\': [[HumanMessage(content=\\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\')]]}}}, {\\'event\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"a list of countries. Each country should have the key `name` and `population`')]]}}}, {'event': 'on_parser_start',  'name': 'JsonOutputParser',  'run_id': 'bf978194-0eda-4494-ad15-3a5bfe69cd59',  'tags': ['seq:step:2'],  'metadata': {},  'data': {}}]What do you think you’d see if you looked at the last 3 events? what\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='about the middle?Let’s use this API to take output the stream events from the model and\\nthe parser. We’re ignoring start events, end events and events from the', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='chain.num_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(            f\"Chat model chunk: {repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        )    if kind == \"on_parser_stream\":        print(f\"Parser chunk: {event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if num_events > 30:        # Truncate the output        print(\"...\")        breakChat model chunk: \\' Here\\'Chat model chunk: \\' is\\'Chat model chunk: \\' the\\'Chat model chunk: \\' JSON\\'Chat model chunk: \\' with\\'Chat model chunk: \\' the\\'Chat model chunk: \\' requested\\'Chat model chunk: \\' countries\\'Chat model chunk: \\' and\\'Chat model chunk: \\' their\\'Chat model chunk: \\' populations\\'Chat', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='\\' countries\\'Chat model chunk: \\' and\\'Chat model chunk: \\' their\\'Chat model chunk: \\' populations\\'Chat model chunk: \\':\\'Chat model chunk: \\'\\\\n\\\\n```\\'Chat model chunk: \\'json\\'Parser chunk: {}Chat model chunk: \\'\\\\n{\\'Chat model chunk: \\'\\\\n \\'Chat model chunk: \\' \"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: {\\'countries\\': []}Chat model chunk: \\' [\\'Chat model chunk: \\'\\\\n   \\'Parser chunk: {\\'countries\\': [{}]}Chat model chunk: \\' {\\'...Because both the model and the parser support streaming, we see sreaming', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='events from both components in real time! Kind of cool isn’t it? 🦜Filtering Events\\u200bBecause this API produces so many events, it is useful to be able to\\nfilter on events.You can filter by either component name, component tags or component', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='type.By Name\\u200bchain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(    {\"run_name\": \"my_parser\"})max_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",    include_names=[\"my_parser\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_parser_start\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\',', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': ''}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67}]}}}{'event': 'on_parser_stream', 'name': 'my_parser',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='[{\\'name\\': \\'France\\', \\'population\\': 67}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 6739}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 673915}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {}]}}}...By Type\\u200bchain = model.with_config({\"run_name\": \"model\"}) |', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='\\'population\\': 67391582}, {}]}}}...By Type\\u200bchain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(    {\"run_name\": \"my_parser\"})max_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",    include_types=[\"chat_model\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_chat_model_start\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'input\\': {\\'messages\\': [[HumanMessage(content=\\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\')]]}}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' Here\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' is\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' the\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' JSON\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"AIMessageChunk(content=' JSON')}}{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' with')}}{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' requested')}}{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' countries')}}{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='\\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' and\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' their\\')}}...By Tags\\u200bcautionTags are inherited by child components of a given runnable.If you’re using tags to filter, make sure that this is what you want.chain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})max_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",    include_tags=[\"my_chain\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_chain_start\\', \\'run_id\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_chain_start\\', \\'run_id\\': \\'190875f3-3fb7-49ad-9b6e-f49da22f3e49\\', \\'name\\': \\'RunnableSequence\\', \\'tags\\': [\\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'}}{\\'event\\': \\'on_chat_model_start\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'input\\': {\\'messages\\': [[HumanMessage(content=\\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\')]]}}}{\\'event\\': \\'on_parser_start\\', \\'name\\': \\'JsonOutputParser\\', \\'run_id\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"`name` and `population`')]]}}}{'event': 'on_parser_start', 'name': 'JsonOutputParser', 'run_id': '3b5e4ca1-40fe-4a02-9a19-ba2a43a6115c', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' Here')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' is')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {},\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' JSON')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' with')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' requested')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' countries')}}...Non-streaming components\\u200bRemember how some components don’t stream well because they don’t\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='operate on input streams?While such components can break streaming of the final output when using\\nastream, astream_events will still yield streaming events from\\nintermediate steps that support streaming!# Function that does not support streaming.# It operates on the finalizes inputs rather than# operating on the input stream.def _extract_country_names(inputs):    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if \"countries\" not in inputs:        return \"\"    countries = inputs[\"countries\"]    if not isinstance(countries, list):        return \"\"    country_names = [        country.get(\"name\") for country in countries if isinstance(country, dict)    ]    return country_nameschain = (    model | JsonOutputParser() | _extract_country_names)  # This parser only works with OpenAI right nowAs expected, the astream API doesn’t work correctly because', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='_extract_country_names doesn’t operate on streams.async for chunk in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',):    print(chunk, flush=True)[\\'France\\', \\'Spain\\', \\'Japan\\']Now, let’s confirm that with astream_events we’re still seeing streaming', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='output from the model and the parser.num_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(            f\"Chat model chunk: {repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        )    if kind == \"on_parser_stream\":        print(f\"Parser chunk: {event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if num_events > 30:        # Truncate the output        print(\"...\")        breakChat model chunk: \\' Here\\'Chat model chunk: \\' is\\'Chat model chunk: \\' the\\'Chat model chunk: \\' JSON\\'Chat model chunk: \\' with\\'Chat model chunk: \\' the\\'Chat model chunk: \\' requested\\'Chat model chunk: \\' countries\\'Chat model chunk: \\' and\\'Chat model chunk: \\' their\\'Chat model', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='model chunk: \\' countries\\'Chat model chunk: \\' and\\'Chat model chunk: \\' their\\'Chat model chunk: \\' populations\\'Chat model chunk: \\':\\'Chat model chunk: \\'\\\\n\\\\n```\\'Chat model chunk: \\'json\\'Parser chunk: {}Chat model chunk: \\'\\\\n{\\'Chat model chunk: \\'\\\\n \\'Chat model chunk: \\' \"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: {\\'countries\\': []}Chat model chunk: \\' [\\'Chat model chunk: \\'\\\\n   \\'Parser chunk: {\\'countries\\': [{}]}Chat model chunk: \\' {\\'Chat model chunk: \\'\\\\n     \\'Chat model chunk: \\' \"\\'...Propagating Callbacks\\u200bcautionIf you’re using invoking runnables inside your tools, you need to', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='propagate callbacks to the runnable; otherwise, no stream events will be\\ngenerated.noteWhen using RunnableLambdas or @chain decorator, callbacks are propagated', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='automatically behind the scenes.from langchain_core.runnables import RunnableLambdafrom langchain_core.tools import tooldef reverse_word(word: str):    return word[::-1]reverse_word = RunnableLambda(reverse_word)@tooldef bad_tool(word: str):    \"\"\"Custom tool that doesn\\'t propagate callbacks.\"\"\"    return reverse_word.invoke(word)async for event in bad_tool.astream_events(\"hello\", version=\"v1\"):    print(event){\\'event\\': \\'on_tool_start\\', \\'run_id\\': \\'ae7690f8-ebc9-4886-9bbe-cb336ff274f2\\', \\'name\\': \\'bad_tool\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\'}}{\\'event\\': \\'on_tool_stream\\', \\'run_id\\': \\'ae7690f8-ebc9-4886-9bbe-cb336ff274f2\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'bad_tool\\', \\'data\\': {\\'chunk\\': \\'olleh\\'}}{\\'event\\': \\'on_tool_end\\', \\'name\\': \\'bad_tool\\', \\'run_id\\': \\'ae7690f8-ebc9-4886-9bbe-cb336ff274f2\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'olleh\\'}}Here’s a re-implementation that does propagate callbacks correctly.', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='You’ll notice that now we’re getting events from the reverse_word', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='runnable as well.@tooldef correct_tool(word: str, callbacks):    \"\"\"A tool that correctly propagates callbacks.\"\"\"    return reverse_word.invoke(word, {\"callbacks\": callbacks})async for event in correct_tool.astream_events(\"hello\", version=\"v1\"):    print(event){\\'event\\': \\'on_tool_start\\', \\'run_id\\': \\'384f1710-612e-4022-a6d4-8a7bb0cc757e\\', \\'name\\': \\'correct_tool\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\'}}{\\'event\\': \\'on_chain_start\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'c4882303-8867-4dff-b031-7d9499b39dda\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'c4882303-8867-4dff-b031-7d9499b39dda\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\', \\'output\\': \\'olleh\\'}}{\\'event\\': \\'on_tool_stream\\', \\'run_id\\': \\'384f1710-612e-4022-a6d4-8a7bb0cc757e\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'correct_tool\\', \\'data\\': {\\'chunk\\': \\'olleh\\'}}{\\'event\\': \\'on_tool_end\\', \\'name\\': \\'correct_tool\\', \\'run_id\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"'data': {'chunk': 'olleh'}}{'event': 'on_tool_end', 'name': 'correct_tool', 'run_id': '384f1710-612e-4022-a6d4-8a7bb0cc757e', 'tags': [], 'metadata': {}, 'data': {'output': 'olleh'}}If you’re invoking runnables from within Runnable Lambdas or @chains,\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='then callbacks will be passed automatically on your behalf.from langchain_core.runnables import RunnableLambdaasync def reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * 2reverse_and_double = RunnableLambda(reverse_and_double)await reverse_and_double.ainvoke(\"1234\")async for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):    print(event){\\'event\\': \\'on_chain_start\\', \\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'name\\': \\'reverse_and_double\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_start\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'335fe781-8944-4464-8d2e-81f61d1f85f5\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'335fe781-8944-4464-8d2e-81f61d1f85f5\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\', \\'output\\': \\'4321\\'}}{\\'event\\': \\'on_chain_stream\\', \\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='\\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'reverse_and_double\\', \\'data\\': {\\'chunk\\': \\'43214321\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_and_double\\', \\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'43214321\\'}}And with the @chain decorator:from langchain_core.runnables import chain@chainasync def reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * 2await reverse_and_double.ainvoke(\"1234\")async for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):    print(event){\\'event\\': \\'on_chain_start\\', \\'run_id\\': \\'7485eedb-1854-429c-a2f8-03d01452daef\\', \\'name\\': \\'reverse_and_double\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_start\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'e7cddab2-9b95-4e80-abaf-4b2429117835\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_word\\', \\'run_id\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content=\"{}, 'data': {'input': '1234'}}{'event': 'on_chain_end', 'name': 'reverse_word', 'run_id': 'e7cddab2-9b95-4e80-abaf-4b2429117835', 'tags': [], 'metadata': {}, 'data': {'input': '1234', 'output': '4321'}}{'event': 'on_chain_stream', 'run_id': '7485eedb-1854-429c-a2f8-03d01452daef', 'tags': [], 'metadata': {}, 'name': 'reverse_and_double', 'data': {'chunk': '43214321'}}{'event': 'on_chain_end', 'name': 'reverse_and_double', 'run_id': '7485eedb-1854-429c-a2f8-03d01452daef', 'tags': [], 'metadata': {}, 'data': {'output': '43214321'}}Help us out by providing feedback on this documentation page:PreviousInterfaceNextHow toUsing StreamLLMs and Chat ModelsChainsWorking with Input StreamsNon-streaming componentsUsing Stream EventsEvent ReferenceChat ModelChainFiltering EventsNon-streaming componentsPropagating CallbacksCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming', 'title': 'Streaming | 🦜️🔗 Langchain', 'description': 'streaming-with-langchain}', 'language': 'en'}), Document(page_content='Interface | 🦜️🔗 Langchain', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTube🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageInterfaceOn this pageInterfaceTo make it as easy as possible to create custom chains, we’ve\\nimplemented a\\n“Runnable”\\nprotocol. The Runnable protocol is implemented for most components.\\nThis is a standard interface, which makes it easy to define custom\\nchains as well as invoke them in a standard way. The standard interface', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='chains as well as invoke them in a standard way. The standard interface\\nincludes:stream: stream back chunks of the responseinvoke: call the chain on an inputbatch: call the chain on a list of inputsThese also have corresponding async methods:astream: stream back chunks of the response asyncainvoke: call the chain on an input asyncabatch: call the chain on a list of inputs asyncastream_log: stream back\\nintermediate steps as they happen, in addition to the final responseastream_events: beta stream events as', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='they happen in the chain (introduced in langchain-core 0.1.14)The input type and output type varies by component:ComponentInput TypeOutput TypePromptDictionaryPromptValueChatModelSingle string, list of chat messages or a PromptValueChatMessageLLMSingle string, list of chat messages or a PromptValueStringOutputParserThe output of an LLM or ChatModelDepends on the parserRetrieverSingle stringList of DocumentsToolSingle string or dictionary, depending on the toolDepends on the toolAll runnables expose input and output schemas to inspect the inputs\\nand outputs: - input_schema: an input Pydantic model\\nauto-generated from the structure of the Runnable -\\noutput_schema: an output Pydantic model\\nauto-generated from the structure of the RunnableLet’s take a look at these methods. To do so, we’ll create a super\\nsimple PromptTemplate + ChatModel chain.%pip install –upgrade –quiet langchain-core langchain-community', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='langchain-openaifrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelInput Schema\\u200bA description of the inputs accepted by a Runnable. This is a Pydantic\\nmodel dynamically generated from the structure of any Runnable. You can', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"call .schema() on it to obtain a JSONSchema representation.# The input schema of the chain is the input schema of its first part, the prompt.chain.input_schema.schema(){'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}prompt.input_schema.schema(){'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}model.input_schema.schema(){'title': 'ChatOpenAIInput', 'anyOf': [{'type': 'string'},  {'$ref': '#/definitions/StringPromptValue'},  {'$ref': '#/definitions/ChatPromptValueConcrete'},  {'type': 'array',   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},     {'$ref': '#/definitions/HumanMessage'},     {'$ref': '#/definitions/ChatMessage'},     {'$ref': '#/definitions/SystemMessage'},     {'$ref': '#/definitions/FunctionMessage'},     {'$ref': '#/definitions/ToolMessage'}]}}], 'definitions': {'StringPromptValue': {'title': 'StringPromptValue',   'description': 'String prompt\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'definitions': {'StringPromptValue': {'title': 'StringPromptValue',   'description': 'String prompt value.',   'type': 'object',   'properties': {'text': {'title': 'Text', 'type': 'string'},    'type': {'title': 'Type',     'default': 'StringPromptValue',     'enum': ['StringPromptValue'],     'type': 'string'}},   'required': ['text']},  'AIMessage': {'title': 'AIMessage',   'description': 'A Message from an AI.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'ai',     'enum': ['ai'],     'type': 'string'},    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},   'required': ['content']},  'HumanMessage': {'title': 'HumanMessage',   'description': 'A Message from a human.',   'type': 'object',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"{'title': 'HumanMessage',   'description': 'A Message from a human.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'human',     'enum': ['human'],     'type': 'string'},    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},   'required': ['content']},  'ChatMessage': {'title': 'ChatMessage',   'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'chat',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'chat',     'enum': ['chat'],     'type': 'string'},    'role': {'title': 'Role', 'type': 'string'}},   'required': ['content', 'role']},  'SystemMessage': {'title': 'SystemMessage',   'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\\\nof input messages.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'system',     'enum': ['system'],     'type': 'string'}},   'required': ['content']},  'FunctionMessage': {'title': 'FunctionMessage',   'description': 'A Message for passing the result of executing a function back to a model.',   'type': 'object',   'properties': {'content': {'title': 'Content',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"a function back to a model.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'function',     'enum': ['function'],     'type': 'string'},    'name': {'title': 'Name', 'type': 'string'}},   'required': ['content', 'name']},  'ToolMessage': {'title': 'ToolMessage',   'description': 'A Message for passing the result of executing a tool back to a model.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'tool',     'enum': ['tool'],     'type': 'string'},\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'type': {'title': 'Type',     'default': 'tool',     'enum': ['tool'],     'type': 'string'},    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},   'required': ['content', 'tool_call_id']},  'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',   'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\\\nFor use in external schemas.',   'type': 'object',   'properties': {'messages': {'title': 'Messages',     'type': 'array',     'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},       {'$ref': '#/definitions/HumanMessage'},       {'$ref': '#/definitions/ChatMessage'},       {'$ref': '#/definitions/SystemMessage'},       {'$ref': '#/definitions/FunctionMessage'},       {'$ref': '#/definitions/ToolMessage'}]}},    'type': {'title': 'Type',     'default': 'ChatPromptValueConcrete',     'enum': ['ChatPromptValueConcrete'],     'type': 'string'}},   'required': ['messages']}}}Output Schema\\u200bA description of the outputs\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'type': 'string'}},   'required': ['messages']}}}Output Schema\\u200bA description of the outputs produced by a Runnable. This is a Pydantic\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='model dynamically generated from the structure of any Runnable. You can', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"call .schema() on it to obtain a JSONSchema representation.# The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessagechain.output_schema.schema(){'title': 'ChatOpenAIOutput', 'anyOf': [{'$ref': '#/definitions/AIMessage'},  {'$ref': '#/definitions/HumanMessage'},  {'$ref': '#/definitions/ChatMessage'},  {'$ref': '#/definitions/SystemMessage'},  {'$ref': '#/definitions/FunctionMessage'},  {'$ref': '#/definitions/ToolMessage'}], 'definitions': {'AIMessage': {'title': 'AIMessage',   'description': 'A Message from an AI.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'ai',     'enum': ['ai'],     'type': 'string'},    'example': {'title': 'Example',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'default': 'ai',     'enum': ['ai'],     'type': 'string'},    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},   'required': ['content']},  'HumanMessage': {'title': 'HumanMessage',   'description': 'A Message from a human.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'human',     'enum': ['human'],     'type': 'string'},    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},   'required': ['content']},  'ChatMessage': {'title': 'ChatMessage',   'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items':\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"{'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'chat',     'enum': ['chat'],     'type': 'string'},    'role': {'title': 'Role', 'type': 'string'}},   'required': ['content', 'role']},  'SystemMessage': {'title': 'SystemMessage',   'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\\\nof input messages.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'system',     'enum': ['system'],     'type': 'string'}},   'required': ['content']},  'FunctionMessage':\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'enum': ['system'],     'type': 'string'}},   'required': ['content']},  'FunctionMessage': {'title': 'FunctionMessage',   'description': 'A Message for passing the result of executing a function back to a model.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': 'Type',     'default': 'function',     'enum': ['function'],     'type': 'string'},    'name': {'title': 'Name', 'type': 'string'}},   'required': ['content', 'name']},  'ToolMessage': {'title': 'ToolMessage',   'description': 'A Message for passing the result of executing a tool back to a model.',   'type': 'object',   'properties': {'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type':\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='[{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'tool\\',     \\'enum\\': [\\'tool\\'],     \\'type\\': \\'string\\'},    \\'tool_call_id\\': {\\'title\\': \\'Tool Call Id\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'tool_call_id\\']}}}Stream\\u200bfor s in chain.stream({\"topic\": \"bears\"}):    print(s.content, end=\"\", flush=True)Sure, here\\'s a bear-themed joke for you:Why don\\'t bears wear shoes?Because they already have bear feet!Invoke\\u200bchain.invoke({\"topic\": \"bears\"})AIMessage(content=\"Why don\\'t bears wear shoes? \\\\n\\\\nBecause they have bear feet!\")Batch\\u200bchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])[AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\"), AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]You can set the number', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]You can set the number of concurrent requests by using the', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='max_concurrency parameterchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why don\\'t cats play poker in the wild? Too many cheetahs!\")]Async Stream\\u200basync for s in chain.astream({\"topic\": \"bears\"}):    print(s.content, end=\"\", flush=True)Why don\\'t bears wear shoes?Because they have bear feet!Async Invoke\\u200bawait chain.ainvoke({\"topic\": \"bears\"})AIMessage(content=\"Why don\\'t bears ever wear shoes?\\\\n\\\\nBecause they already have bear feet!\")Async Batch\\u200bawait chain.abatch([{\"topic\": \"bears\"}])[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")]Async Stream Events (beta)\\u200bEvent Streaming is a beta API, and may change a bit based on\\nfeedback.Note: Introduced in langchain-core 0.2.0For now, when using the astream_events API, for everything to work', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='properly please:Use async throughout the code (including async tools etc)Propagate callbacks if defining custom functions / runnables.Whenever using runnables without LCEL, make sure to call\\n.astream() on LLMs rather than .ainvoke to force the LLM to\\nstream tokens.Event Reference\\u200bHere is a reference table that shows some events that might be emitted\\nby the various Runnable objects. Definitions for some of the Runnable\\nare included after the table.⚠️ When streaming the inputs for the runnable will not be available\\nuntil the input stream has been entirely consumed This means that the\\ninputs will be available at for the corresponding end hook rather than', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='start event.eventnamechunkinputoutputon_chat_model_start[model name]{“messages”: [[SystemMessage, HumanMessage]]}on_chat_model_stream[model name]AIMessageChunk(content=“hello”)on_chat_model_end[model name]{“messages”: [[SystemMessage, HumanMessage]]}{“generations”: […], “llm_output”: None, …}on_llm_start[model name]{‘input’: ‘hello’}on_llm_stream[model name]‘Hello’on_llm_end[model name]‘Hello human!’on_chain_startformat_docson_chain_streamformat_docs“hello world!, goodbye world!”on_chain_endformat_docs[Document(…)]“hello world!, goodbye world!”on_tool_startsome_tool{“x”: 1, “y”: “2”}on_tool_streamsome_tool{“x”: 1, “y”: “2”}on_tool_endsome_tool{“x”: 1, “y”: “2”}on_retriever_start[retriever name]{“query”: “hello”}on_retriever_chunk[retriever name]{documents: […]}on_retriever_end[retriever name]{“query”: “hello”}{documents: […]}on_prompt_start[template_name]{“question”: “hello”}on_prompt_end[template_name]{“question”: “hello”}ChatPromptValue(messages: [SystemMessage, …])Here are', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='“hello”}ChatPromptValue(messages: [SystemMessage, …])Here are declarations associated with the events shown above:format_docs:def format_docs(docs: List[Document]) -> str:    \\'\\'\\'Format the docs.\\'\\'\\'    return \", \".join([doc.page_content for doc in docs])format_docs = RunnableLambda(format_docs)some_tool:@tooldef some_tool(x: int, y: str) -> dict:    \\'\\'\\'Some_tool.\\'\\'\\'    return {\"x\": x, \"y\": y}prompt:template = ChatPromptTemplate.from_messages(    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})Let’s define a new chain to make it more interesting to show off the', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='astream_events interface (and later the astream_log interface).from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()retrieval_chain = (    {        \"context\": retriever.with_config(run_name=\"Docs\"),        \"question\": RunnablePassthrough(),    }    | prompt    | model.with_config(run_name=\"my_llm\")    | StrOutputParser())Now let’s use astream_events to get events from the retriever and the', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='LLM.async for event in retrieval_chain.astream_events(    \"where did harrison work?\", version=\"v1\", include_names=[\"Docs\", \"my_llm\"]):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(event[\"data\"][\"chunk\"].content, end=\"|\")    elif kind in {\"on_chat_model_start\"}:        print()        print(\"Streaming LLM:\")    elif kind in {\"on_chat_model_end\"}:        print()        print(\"Done streaming LLM.\")    elif kind == \"on_retriever_end\":        print(\"--\")        print(\"Retrieved the following documents:\")        print(event[\"data\"][\"output\"][\"documents\"])    elif kind == \"on_tool_end\":        print(f\"Ended tool: {event[\\'name\\']}\")    else:        pass/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.  warn_beta(--Retrieved the following documents:[Document(page_content=\\'harrison worked at kensho\\')]Streaming LLM:|H|arrison| worked| at| Kens|ho|.||Done streaming', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"worked at kensho')]Streaming LLM:|H|arrison| worked| at| Kens|ho|.||Done streaming LLM.Async Stream Intermediate Steps\\u200bAll runnables also have a method .astream_log() which is used to\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='stream (as they happen) all or part of the intermediate steps of your\\nchain/sequence.This is useful to show progress to the user, to use intermediate\\nresults, or to debug your chain.You can stream all steps (default) or include/exclude steps by name,\\ntags or metadata.This method yields JSONPatch ops that when', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='applied in the same order as received build up the RunState.class LogEntry(TypedDict):    id: str    \"\"\"ID of the sub-run.\"\"\"    name: str    \"\"\"Name of the object being run.\"\"\"    type: str    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"    tags: List[str]    \"\"\"List of tags for the run.\"\"\"    metadata: Dict[str, Any]    \"\"\"Key-value pairs of metadata for the run.\"\"\"    start_time: str    \"\"\"ISO-8601 timestamp of when the run started.\"\"\"    streamed_output_str: List[str]    \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"    final_output: Optional[Any]    \"\"\"Final output of this run.    Only available after the run has finished successfully.\"\"\"    end_time: Optional[str]    \"\"\"ISO-8601 timestamp of when the run ended.    Only available after the run has finished.\"\"\"class RunState(TypedDict):    id: str    \"\"\"ID of the run.\"\"\"    streamed_output: List[Any]    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"    final_output: Optional[Any]', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='\"\"\"List of output chunks streamed by Runnable.stream()\"\"\"    final_output: Optional[Any]    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.    Only available after the run has finished successfully.\"\"\"    logs: Dict[str, LogEntry]    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will    contain only the runs that matched the filters.\"\"\"Streaming JSONPatch chunks\\u200bThis is useful eg. to stream the JSONPatch in an HTTP server, and then', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='apply the ops on the client to rebuild the run state there. See\\nLangServe for tooling to', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='make it easier to build a webserver from any Runnable.async for chunk in retrieval_chain.astream_log(    \"where did harrison work?\", include_names=[\"Docs\"]):    print(\"-\" * 40)    print(chunk)----------------------------------------RunLogPatch({\\'op\\': \\'replace\\',  \\'path\\': \\'\\',  \\'value\\': {\\'final_output\\': None,            \\'id\\': \\'82e9b4b1-3dd6-4732-8db9-90e79c4da48c\\',            \\'logs\\': {},            \\'name\\': \\'RunnableSequence\\',            \\'streamed_output\\': [],            \\'type\\': \\'chain\\'}})----------------------------------------RunLogPatch({\\'op\\': \\'add\\',  \\'path\\': \\'/logs/Docs\\',  \\'value\\': {\\'end_time\\': None,            \\'final_output\\': None,            \\'id\\': \\'9206e94a-57bd-48ee-8c5e-fdd1c52a6da2\\',            \\'metadata\\': {},            \\'name\\': \\'Docs\\',            \\'start_time\\': \\'2024-01-19T22:33:55.902+00:00\\',            \\'streamed_output\\': [],            \\'streamed_output_str\\': [],            \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],            \\'type\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"[],            'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],            'type': 'retriever'}})----------------------------------------RunLogPatch({'op': 'add',  'path': '/logs/Docs/final_output',  'value': {'documents': [Document(page_content='harrison worked at kensho')]}}, {'op': 'add',  'path': '/logs/Docs/end_time',  'value': '2024-01-19T22:33:56.064+00:00'})----------------------------------------RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}, {'op': 'replace', 'path': '/final_output', 'value': ''})----------------------------------------RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'H'}, {'op': 'replace', 'path': '/final_output', 'value': 'H'})----------------------------------------RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'arrison'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison'})----------------------------------------RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'add', 'path': '/streamed_output/-', 'value': ' worked'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked'})----------------------------------------RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' at'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at'})----------------------------------------RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Kens'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at Kens'})----------------------------------------RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ho'}, {'op': 'replace',  'path': '/final_output',  'value': 'Harrison worked at Kensho'})----------------------------------------RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}, {'op': 'replace',  'path': '/final_output',  'value': 'Harrison worked at Kensho.'})----------------------------------------RunLogPatch({'op': 'add', 'path': '/streamed_output/-',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'add', 'path': '/streamed_output/-', 'value': ''})Streaming the incremental RunState\\u200bYou can simply pass diff=False to get incremental values of\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='RunState. You get more verbose output with more repetitive parts.async for chunk in retrieval_chain.astream_log(    \"where did harrison work?\", include_names=[\"Docs\"], diff=False):    print(\"-\" * 70)    print(chunk)----------------------------------------------------------------------RunLog({\\'final_output\\': None, \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': None, \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': None,                   \\'final_output\\': None,                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\',', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"[],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [],\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': '', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [''], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': 'H', 'id':\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'H', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H'], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': 'Harrison', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison'], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': 'Harrison worked', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked'], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': 'Harrison worked at', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at'], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': 'Harrison worked at Kens', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens'], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': 'Harrison worked at Kensho', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho'], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': 'Harrison worked at Kensho.', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags':\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '.'], 'type': 'chain'})----------------------------------------------------------------------RunLog({'final_output': 'Harrison worked at Kensho.', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': {},                   'name': 'Docs',                   'start_time': '2024-01-19T22:33:56.939+00:00',                   'streamed_output': [],                   'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS',\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content=\"'streamed_output_str': [],                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['',                     'H',                     'arrison',                     ' worked',                     ' at',                     ' Kens',                     'ho',                     '.',                     ''], 'type': 'chain'})Parallelism\\u200bLet’s take a look at how LangChain Expression Language supports parallel\", metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='requests. For example, when using a RunnableParallel (often written as', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='a dictionary) it executes each element in parallel.from langchain_core.runnables import RunnableParallelchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | modelchain2 = (    ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")    | model)combined = RunnableParallel(joke=chain1, poem=chain2)%%timechain1.invoke({\"topic\": \"bears\"})CPU times: user 18 ms, sys: 1.27 ms, total: 19.3 msWall time: 692 msAIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")%%timechain2.invoke({\"topic\": \"bears\"})CPU times: user 10.5 ms, sys: 166 µs, total: 10.7 msWall time: 579 msAIMessage(content=\"In forest\\'s embrace,\\\\nMajestic bears pace.\")%%timecombined.invoke({\"topic\": \"bears\"})CPU times: user 32 ms, sys: 2.59 ms, total: 34.6 msWall time: 816 ms{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear-related joke for you:\\\\n\\\\nWhy did the bear bring a ladder to the bar?\\\\n\\\\nBecause he heard the drinks were on the house!\"), \\'poem\\':', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='the bear bring a ladder to the bar?\\\\n\\\\nBecause he heard the drinks were on the house!\"), \\'poem\\': AIMessage(content=\"In wilderness they roam,\\\\nMajestic strength, nature\\'s throne.\")}Parallelism on batches\\u200bParallelism can be combined with other runnables. Let’s try to use', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='parallelism with batches.%%timechain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU times: user 17.3 ms, sys: 4.84 ms, total: 22.2 msWall time: 628 ms[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]%%timechain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU times: user 15.8 ms, sys: 3.83 ms, total: 19.7 msWall time: 718 ms[AIMessage(content=\\'In the wild, bears roam,\\\\nMajestic guardians of ancient home.\\'), AIMessage(content=\\'Whiskers grace, eyes gleam,\\\\nCats dance through the moonbeam.\\')]%%timecombined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU times: user 44.8 ms, sys: 3.17 ms, total: 48 msWall time: 721 ms[{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"),  \\'poem\\': AIMessage(content=\"Majestic bears roam,\\\\nNature\\'s strength, beauty shown.\")}, {\\'joke\\': AIMessage(content=\"Why don\\'t', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='bears roam,\\\\nNature\\'s strength, beauty shown.\")}, {\\'joke\\': AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\"),  \\'poem\\': AIMessage(content=\"Whiskers dance, eyes aglow,\\\\nCats embrace the night\\'s gentle flow.\")}]Help us out by providing feedback on this documentation page:PreviousWhy use LCELNextStreamingInput SchemaOutput SchemaStreamInvokeBatchAsync StreamAsync InvokeAsync BatchAsync Stream Events (beta)Event ReferenceAsync Stream Intermediate StepsStreaming JSONPatch chunksStreaming the incremental RunStateParallelismParallelism on batchesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface', 'title': 'Interface | 🦜️🔗 Langchain', 'description': 'To make it as easy as possible to create custom chains, we’ve', 'language': 'en'}), Document(page_content='Cookbook | 🦜️🔗 Langchain', metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/', 'title': 'Cookbook | 🦜️🔗 Langchain', 'description': \"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.\", 'language': 'en'}), Document(page_content=\"Skip to main contentDocsUse casesIntegrationsGuidesAPIMorePeopleVersioningChangelogContributingTemplatesCookbooksTutorialsYouTube🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookPrompt + LLMRAGMultiple chainsQuerying a SQL DBAgentsCode writingRouting by semantic similarityAdding memoryAdding moderationManaging prompt sizeUsing toolsLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageCookbookCookbookExample code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.📄️ Prompt + LLMThe most\", metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/', 'title': 'Cookbook | 🦜️🔗 Langchain', 'description': \"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.\", 'language': 'en'}), Document(page_content='acquainted with LCEL, the Prompt + LLM page is a good place to start.📄️ Prompt + LLMThe most common and valuable composition is taking:📄️ RAGLet’s look at adding in a retrieval step to a prompt and LLM, which adds📄️ Multiple chainsRunnables can easily be used to string together multiple Chains📄️ Querying a SQL DBWe can replicate our SQLDatabaseChain with Runnables.📄️ AgentsYou can pass a Runnable into an agent. Make sure you have langchainhub📄️ Code writingExample of how to use LCEL to write Python code.📄️ Routing by semantic similarityWith LCEL you can easily add [custom routing📄️ Adding memoryThis shows how to add memory to an arbitrary chain. Right now, you can📄️ Adding moderationThis shows how to add in moderation (or other safeguards) around your📄️ Managing prompt sizeAgents dynamically call tools. The results of those tool calls are added📄️ Using toolsYou can use any Tools with Runnables easily.Help us out by providing feedback on this documentation page:PreviousAdd message', metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/', 'title': 'Cookbook | 🦜️🔗 Langchain', 'description': \"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.\", 'language': 'en'}), Document(page_content='Runnables easily.Help us out by providing feedback on this documentation page:PreviousAdd message history (memory)NextPrompt + LLMCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/', 'title': 'Cookbook | 🦜️🔗 Langchain', 'description': \"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.\", 'language': 'en'})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://python.langchain.com/docs/expression_language'\n",
    "# pprint(j_data)\n",
    "web_data = load_url(url)\n",
    "print(web_data)\n",
    "\n",
    "query = 'what is LCEL?'\n",
    "# model = Chat_with_ai(web_data,query)\n",
    "# print(model.chat())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
