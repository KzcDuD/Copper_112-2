[
  {
    "title": "Using tools | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/tools",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookUsing tools\nUsing tools\n\nYou can use any Tools with Runnables easily.\n\n%pip install --upgrade --quiet  langchain langchain-openai duckduckgo-search\n\nfrom langchain.tools import DuckDuckGoSearchRun\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nsearch = DuckDuckGoSearchRun()\n\ntemplate = \"\"\"turn the following user input into a search query for a search engine:\n\n{input}\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nmodel = ChatOpenAI()\n\nchain = prompt | model | StrOutputParser() | search\n\nchain.invoke({\"input\": \"I'd like to figure out what games are tonight\"})\n\n'What sports games are on TV today & tonight? Watch and stream live sports on TV today, tonight, tomorrow. Today\\'s 2023 sports TV schedule includes football, basketball, baseball, hockey, motorsports, soccer and more. Watch on TV or stream online on ESPN, FOX, FS1, CBS, NBC, ABC, Peacock, Paramount+, fuboTV, local channels and many other networks. MLB Games Tonight: How to Watch on TV, Streaming & Odds - Thursday, September 7. Seattle Mariners\\' Julio Rodriguez greets teammates in the dugout after scoring against the Oakland Athletics in a ... Circle - Country Music and Lifestyle. Live coverage of all the MLB action today is available to you, with the information provided below. The Brewers will look to pick up a road win at PNC Park against the Pirates on Wednesday at 12:35 PM ET. Check out the latest odds and with BetMGM Sportsbook. Use bonus code \"GNPLAY\" for special offers! MLB Games Tonight: How to Watch on TV, Streaming & Odds - Tuesday, September 5. Houston Astros\\' Kyle Tucker runs after hitting a double during the fourth inning of a baseball game against the Los Angeles Angels, Sunday, Aug. 13, 2023, in Houston. (AP Photo/Eric Christian Smith) (APMedia) The Houston Astros versus the Texas Rangers is one of ... The second half of tonight\\'s college football schedule still has some good games remaining to watch on your television.. We\\'ve already seen an exciting one when Colorado upset TCU. And we saw some ...'\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nManaging prompt size\nNext\nLangChain Expression Language (LCEL)\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Managing prompt size | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/prompt_size",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookManaging prompt size\nManaging prompt size\n\nAgents dynamically call tools. The results of those tool calls are added back to the prompt, so that the agent can plan the next action. Depending on what tools are being used and how they’re being called, the agent prompt can easily grow larger than the model context window.\n\nWith LCEL, it’s easy to add custom functionality for managing the size of prompts within your chain or agent. Let’s look at simple agent example that can search Wikipedia for information.\n\n%pip install --upgrade --quiet  langchain langchain-openai wikipedia\n\nfrom operator import itemgetter\n\nfrom langchain.agents import AgentExecutor, load_tools\nfrom langchain.agents.format_scratchpad import format_to_openai_function_messages\nfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\nfrom langchain.tools import WikipediaQueryRun\nfrom langchain_community.utilities import WikipediaAPIWrapper\nfrom langchain_core.prompt_values import ChatPromptValue\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\n\nwiki = WikipediaQueryRun(\n    api_wrapper=WikipediaAPIWrapper(top_k_results=5, doc_content_chars_max=10_000)\n)\ntools = [wiki]\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful assistant\"),\n        (\"user\", \"{input}\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ]\n)\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n\nLet’s try a many-step question without any prompt size handling:\n\nagent = (\n    {\n        \"input\": itemgetter(\"input\"),\n        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n            x[\"intermediate_steps\"]\n        ),\n    }\n    | prompt\n    | llm.bind_functions(tools)\n    | OpenAIFunctionsAgentOutputParser()\n)\n\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\nagent_executor.invoke(\n    {\n        \"input\": \"Who is the current US president? What's their home state? What's their home state's bird? What's that bird's scientific name?\"\n    }\n)\n\n\n\n> Entering new AgentExecutor chain...\n\nInvoking: `Wikipedia` with `List of presidents of the United States`\n\n\nPage: List of presidents of the United States\nSummary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of individuals who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party.\n\nPage: List of presidents of the United States by age\nSummary: In this list of presidents of the United States by age, the first table charts the age of each president of the United States at the time of presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated up to January 25, 2024.\n\nPage: List of vice presidents of the United States\nSummary: There have been 49 vice presidents of the United States since the office was created in 1789. Originally, the vice president was the person who received the second-most votes for president in the Electoral College. But after the election of 1800 produced a tie between Thomas Jefferson and Aaron Burr, requiring the House of Representatives to choose between them, lawmakers acted to prevent such a situation from recurring. The Twelfth Amendment was added to the Constitution in 1804, creating the current system where electors cast a separate ballot for the vice presidency.The vice president is the first person in the presidential line of succession—that is, they assume the presidency if the president dies, resigns, or is impeached and removed from office. Nine vice presidents have ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, Harry S. Truman, and Lyndon B. Johnson) through the president's death and one (Gerald Ford) through the president's resignation. The vice president also serves as the president of the Senate and may choose to cast a tie-breaking vote on decisions made by the Senate. Vice presidents have exercised this latter power to varying extents over the years.Before adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the office of the vice president could not be filled until the next post-election inauguration. Several such vacancies occurred: seven vice presidents died, one resigned and eight succeeded to the presidency. This amendment allowed for a vacancy to be filled through appointment by the president and confirmation by both chambers of the Congress. Since its ratification, the vice presidency has been vacant twice (both in the context of scandals surrounding the Nixon administration) and was filled both times through this process, namely in 1973 following Spiro Agnew's resignation, and again in 1974 after Gerald Ford succeeded to the presidency. The amendment also established a procedure whereby a vice president may, if the president is unable to discharge the powers and duties of the office, temporarily assume the powers and duties of the office as acting president. Three vice presidents have briefly acted as president under the 25th Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, and on July 21, 2007; and Kamala Harris on November 19, 2021.\nThe persons who have served as vice president were born in or primarily affiliated with 27 states plus the District of Columbia. New York has produced the most of any state as eight have been born there and three others considered it their home state. Most vice presidents have been in their 50s or 60s and had political experience before assuming the office. Two vice presidents—George Clinton and John C. Calhoun—served under more than one president. Ill with tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. King, by an Act of Congress, was allowed to take the oath outside the United States. He is the only vice president to take his oath of office in a foreign country.\n\nPage: List of presidents of the United States by net worth\nSummary: The list of presidents of the United States by net worth at peak varies greatly. Debt and depreciation often means that presidents' net worth is less than $0 at the time of death. Most presidents before 1845 were extremely wealthy, especially Andrew Jackson and George Washington.    \nPresidents since 1929, when Herbert Hoover took office, have generally been wealthier than presidents of the late nineteenth and early twentieth centuries; with the exception of Harry S. Truman, all presidents since this time have been millionaires. These presidents have often received income from autobiographies and other writing. Except for Franklin D. Roosevelt and John F. Kennedy (both of whom died while in office), all presidents beginning with Calvin Coolidge have written autobiographies. In addition, many presidents—including Bill Clinton—have earned considerable income from public speaking after leaving office.The richest president in history may be Donald Trump. However, his net worth is not precisely known because the Trump Organization is privately held.Truman was among the poorest U.S. presidents, with a net worth considerably less than $1 million. His financial situation contributed to the doubling of the presidential salary to $100,000 in 1949. In addition, the presidential pension was created in 1958 when Truman was again experiencing financial difficulties. Harry and Bess Truman received the first Medicare cards in 1966 via the Social Security Act of 1965.\n\nPage: List of presidents of the United States by home state\nSummary: These lists give the states of primary affiliation and of birth for each president of the United States.\nInvoking: `Wikipedia` with `Joe Biden`\n\n\nPage: Joe Biden\nSummary: Joseph Robinette Biden Jr. (  BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.\nBorn in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. He became the oldest president in U.S. history, and the first to have a female vice president.\nAs president, Biden signed the American Rescue Plan Act in response to the COVID-19 pandemic and subsequent recession. He signed bipartisan bills on infrastructure and manufacturing. He proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that he signed into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme Court. He worked with congressional Republicans to resolve the 2023 United States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the Israel–Hamas war, Biden announced military support for Israel, and condemned the actions of Hamas and other Palestinian militants as terrorism. In April 2023, Biden announced his candidacy for the Democratic nomination in the 2024 presidential election.\n\nPage: Presidency of Joe Biden\nSummary: Joe Biden's tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president for two terms under president Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Biden won the presidency with a popular vote of over 81 million, the highest number of votes cast for a single United States presidential candidate. Upon his inauguration, he became the oldest president in American history, breaking the record set by his predecessor Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump's energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump's border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden alongside Democrats and Republicans in Congress, to invest in American roads, bridges, public transit, ports and broadband access. Biden signed the Juneteenth National Independence Day Act, making Juneteenth a federal holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. Supreme Court—the first Black woman to serve on the court. After The Supreme Court overturned Roe v. Wade, Biden took executive actions, such as the signing of Executive Order 14076, to preserve and protect women's health rights nationwide, against abortion bans in Republican led states. Biden proposed a significant expansion of the U.S. social safety net through the Build Back Better Act, but those efforts, along with voting rights legislation, failed in Congress. However, in August 2022, Biden signed the Inflation Reduction Act of 2022, a domestic appropriations bill that included some of the provisions of the Build Back Better Act after the entire bill failed to pass. It included significant federal investment in climate and domestic clean energy production, tax credits for solar panels, electric cars and other home energy programs as well as a three-year extension of Affordable Care Act subsidies. The administration's economic policies, known as \"Bidenomics\", were inspired and designed by Trickle-up economics. Described as growing the economy from the middle out and bottom up and growing the middle class. Biden signed the CHIPS and Science Act, bolstering the semiconductor and manufacturing industry, the Honoring our PACT Act, expanding health care for US veterans, the Bipartisan Safer Communities Act and the Electoral Count Reform and Presidential Transition Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, which repealed the Defense of Marriage Act and codified same-sex and interracial marriage in the United States. In response to the debt-ceiling crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of 2023, which restrains federal spending for fiscal years 2024 and 2025, implements minor changes to SNAP and TANF, includes energy permitting reform, claws back some IRS funding and unspent money for COVID-19, and suspends the debt ceiling to January 1, 2025. Biden established the American Climate Corps and created the first ever White House Office of Gun Violence Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers picket line during the 2023 United Auto Workers strike, making him the first US president to visit one.\nThe foreign policy goal of the Biden administration is to restore the US to a \"position of trusted leadership\" among global democracies in order to address the challenges posed by Russia and China. In foreign policy, Biden completed the withdrawal of U.S. military forces from Afghanistan, declaring an end to nation-building efforts and shifting U.S. foreign policy toward strategic competition with China and, to a lesser extent, Russia. However, during the withdrawal, the Afghan government collapsed and the Taliban seized control, leading to Biden receiving bipartisan criticism. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia as well as providing Ukraine with over $100 billion in combined military, economic, and humanitarian aid. Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi al-Qurashi, the leader of the Islamic State, and approved a drone strike which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed and created AUKUS, an international security alliance, together with Australia and the United Kingdom. Biden called for the expansion of NATO with the addition of Finland and Sweden, and rallied NATO allies in support of Ukraine. During the 2023 Israel–Hamas war, Biden condemned Hamas and other Palestinian militants as terrorism and announced American military support for Israel; Biden also showed his support and sympathy towards Palestinians affected by the war, sent humanitarian aid, and brokered a four-day temporary pause and hostage exchange.\n\nPage: Family of Joe Biden\nSummary: Joe Biden, the 46th and current president of the United States, has family members who are prominent in law, education, activism and politics. Biden's immediate family became the first family of the United States on his inauguration on January 20, 2021. His immediate family circle was also the second family of the United States from 2009 to 2017, when Biden was vice president. Biden's family is mostly descended from the British Isles, with most of their ancestors coming from Ireland and England, and a smaller number descending from the French.Of Joe Biden's sixteen great-great-grandparents, ten were born in Ireland. He is descended from the Blewitts of County Mayo and the Finnegans of County Louth. One of Biden's great-great-great-grandfathers was born in Sussex, England, and emigrated to Maryland in the United States by 1820.\n\nPage: Inauguration of Joe Biden\nSummary: The inauguration of Joe Biden as the 46th president of the United States took place on Wednesday, January 20, 2021, marking the start of the four-year term of Joe Biden as president and Kamala Harris as vice president. The 59th presidential inauguration took place on the West Front of the United States Capitol in Washington, D.C. Biden took the presidential oath of office, before which Harris took the vice presidential oath of office.\nThe inauguration took place amidst extraordinary political, public health, economic, and national security crises, including the ongoing COVID-19 pandemic; outgoing President Donald Trump's attempts to overturn the 2020 United States presidential election, which provoked an attack on the United States Capitol on January 6; Trump'\nInvoking: `Wikipedia` with `Delaware`\n\n\nPage: Delaware\nSummary: Delaware (  DEL-ə-wair) is a state in the northeast and Mid-Atlantic regions of the United States. It borders Maryland to its south and west, Pennsylvania to its north, New Jersey to its northeast, and the Atlantic Ocean to its east. The state's name derives from the adjacent Delaware Bay, which in turn was named after Thomas West, 3rd Baron De La Warr, an English nobleman and the Colony of Virginia's first colonial-era governor.Delaware occupies the northeastern portion of the Delmarva Peninsula, and some islands and territory within the Delaware River. It is the 2nd smallest and 6th least populous state, but also the 6th most densely populated. Delaware's most populous city is Wilmington, and the state's capital is Dover, the 2nd most populous city in Delaware. The state is divided into three counties, the fewest number of counties of any of the 50 U.S. states; from north to south, the three counties are: New Castle County, Kent County, and Sussex County.\nThe southern two counties, Kent and Sussex counties, historically have been predominantly agrarian economies. New Castle is more urbanized and is considered part of the Delaware Valley metropolitan statistical area that surrounds and includes Philadelphia, the nation's 6th most populous city. Delaware is considered part of the Southern United States by the U.S. Census Bureau, but the state's geography, culture, and history are a hybrid of the Mid-Atlantic and Northeastern regions of the country.Before Delaware coastline was explored and developed by Europeans in the 16th century, the state was inhabited by several Native Americans tribes, including the Lenape in the north and Nanticoke in the south. The state was first colonized by Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in 1631.\nDelaware was one of the Thirteen Colonies that participated in the American Revolution and American Revolutionary War, in which the American Continental Army, led by George Washington, defeated the British, ended British colonization and establishing the United States as a sovereign and independent nation.\nOn December 7, 1787, Delaware was the first state to ratify the Constitution of the United States, earning it the nickname \"The First State\".Since the turn of the 20th century, Delaware has become an onshore corporate haven whose corporate laws are deemed appealing to corporations; over half of all New York Stock Exchange-listed corporations and over three-fifths of the Fortune 500 is legally incorporated in the state.\n\nPage: Delaware City, Delaware\nSummary: Delaware City is a city in New Castle County, Delaware, United States. The population was 1,885 as of 2020. It is a small port town on the eastern terminus of the Chesapeake and Delaware Canal and is the location of the Forts Ferry Crossing to Fort Delaware on Pea Patch Island.\n\nPage: Delaware River\nSummary: The Delaware River is a major river in the Mid-Atlantic region of the United States and is the longest free-flowing (undammed) river in the Eastern United States. From the meeting of its branches in Hancock, New York, the river flows for 282 miles (454 km) along the borders of New York, Pennsylvania, New Jersey, and Delaware, before emptying into Delaware Bay.\nThe river has been recognized by the National Wildlife Federation as one of the country's Great Waters and has been called the \"Lifeblood of the Northeast\" by American Rivers. Its watershed drains an area of 13,539 square miles (35,070 km2) and provides drinking water for 17 million people, including half of New York City via the Delaware Aqueduct.\nThe Delaware River has two branches that rise in the Catskill Mountains of New York: the West Branch at Mount Jefferson in Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware County. The branches merge to form the main Delaware River at Hancock, New York. Flowing south, the river remains relatively undeveloped, with 152 miles (245 km) protected as the Upper, Middle, and Lower Delaware National Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, navigable, and significantly more industrial. This section forms the backbone of the Delaware Valley metropolitan area, serving the port cities of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the bay's outlet to the Atlantic Ocean between Cape May and Cape Henlopen.\nBefore the arrival of European settlers, the river was the homeland of the Lenape native people. They called the river Lenapewihittuk, or Lenape River, and Kithanne, meaning the largest river in this part of the country.In 1609, the river was visited by a Dutch East India Company expedition led by Henry Hudson. Hudson, an English navigator, was hired to find a western route to Cathay (China), but his encounters set the stage for Dutch colonization of North America in the 17th century. Early Dutch and Swedish settlements were established along the lower section of the river and Delaware Bay. Both colonial powers called the river the South River (Zuidrivier), compared to the Hudson River, which was known as the North River. After the English expelled the Dutch and took control of the New Netherland colony in 1664, the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, an English nobleman and the Virginia colony's first royal governor, who defended the colony during the First Anglo-Powhatan War.\n\nPage: University of Delaware\nSummary: The University of Delaware (colloquially known as UD or Delaware) is a privately governed, state-assisted land-grant research university located in Newark, Delaware. UD is the largest university in Delaware. It offers three associate's programs, 148 bachelor's programs, 121 master's programs (with 13 joint degrees), and 55 doctoral programs across its eight colleges. The main campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, and Georgetown. It is considered a large institution with approximately 18,200 undergraduate and 4,200 graduate students. It is a privately governed university which receives public funding for being a land-grant, sea-grant, and space-grant state-supported research institution.UD is classified among \"R1: Doctoral Universities – Very high research activity\". According to the National Science Foundation, UD spent $186 million on research and development in 2018, ranking it 119th in the nation.  It is recognized with the Community Engagement Classification by the Carnegie Foundation for the Advancement of Teaching.UD students, alumni, and sports teams are known as the \"Fightin' Blue Hens\", more commonly shortened to \"Blue Hens\", and the school colors are Delaware blue and gold. UD sponsors 21 men's and women's NCAA Division-I sports teams and have competed in the Colonial Athletic Association (CAA) since 2001.\n\n\n\nPage: Lenape\nSummary: The Lenape (English: , , ; Lenape languages: [lənaːpe]), also called the Lenni Lenape and Delaware people, are an Indigenous people of the Northeastern Woodlands, who live in the United States and Canada.The Lenape's historical territory includes present-day northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, western Long Island, and the lower Hudson Valley in New York state. Today they are based in Oklahoma, Wisconsin, and Ontario.\nDuring the last decades of the 18th century, European settlers and the effects of the American Revolutionary War displaced most Lenape from their homelands and pushed them north and west. In the 1860s, under the Indian removal policy, the U.S. federal government relocated most Lenape remaining in the Eastern United States to the Indian Territory and surrounding regions. Lenape people currently belong to the Delaware Nation and Delaware Tribe of Indians in Oklahoma, the Stockbridge–Munsee Community in Wisconsin, and the Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of Six Nations in Ontario.\n\nBadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 5487 tokens (5419 in the messages, 68 in the functions). Please reduce the length of the messages or functions.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n\n\nLangSmith trace\n\nUnfortunately we run out of space in our model’s context window before we the agent can get to the final answer. Now let’s add some prompt handling logic. To keep things simple, if our messages have too many tokens we’ll start dropping the earliest AI, Function message pairs (this is the model tool invocation message and the subsequent tool output message) in the chat history.\n\ndef condense_prompt(prompt: ChatPromptValue) -> ChatPromptValue:\n    messages = prompt.to_messages()\n    num_tokens = llm.get_num_tokens_from_messages(messages)\n    ai_function_messages = messages[2:]\n    while num_tokens > 4_000:\n        ai_function_messages = ai_function_messages[2:]\n        num_tokens = llm.get_num_tokens_from_messages(\n            messages[:2] + ai_function_messages\n        )\n    messages = messages[:2] + ai_function_messages\n    return ChatPromptValue(messages=messages)\n\n\nagent = (\n    {\n        \"input\": itemgetter(\"input\"),\n        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n            x[\"intermediate_steps\"]\n        ),\n    }\n    | prompt\n    | condense_prompt\n    | llm.bind_functions(tools)\n    | OpenAIFunctionsAgentOutputParser()\n)\n\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\nagent_executor.invoke(\n    {\n        \"input\": \"Who is the current US president? What's their home state? What's their home state's bird? What's that bird's scientific name?\"\n    }\n)\n\n\n\n> Entering new AgentExecutor chain...\n\nInvoking: `Wikipedia` with `List of presidents of the United States`\n\n\nPage: List of presidents of the United States\nSummary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of individuals who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party.\n\nPage: List of presidents of the United States by age\nSummary: In this list of presidents of the United States by age, the first table charts the age of each president of the United States at the time of presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated up to January 25, 2024.\n\nPage: List of vice presidents of the United States\nSummary: There have been 49 vice presidents of the United States since the office was created in 1789. Originally, the vice president was the person who received the second-most votes for president in the Electoral College. But after the election of 1800 produced a tie between Thomas Jefferson and Aaron Burr, requiring the House of Representatives to choose between them, lawmakers acted to prevent such a situation from recurring. The Twelfth Amendment was added to the Constitution in 1804, creating the current system where electors cast a separate ballot for the vice presidency.The vice president is the first person in the presidential line of succession—that is, they assume the presidency if the president dies, resigns, or is impeached and removed from office. Nine vice presidents have ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, Harry S. Truman, and Lyndon B. Johnson) through the president's death and one (Gerald Ford) through the president's resignation. The vice president also serves as the president of the Senate and may choose to cast a tie-breaking vote on decisions made by the Senate. Vice presidents have exercised this latter power to varying extents over the years.Before adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the office of the vice president could not be filled until the next post-election inauguration. Several such vacancies occurred: seven vice presidents died, one resigned and eight succeeded to the presidency. This amendment allowed for a vacancy to be filled through appointment by the president and confirmation by both chambers of the Congress. Since its ratification, the vice presidency has been vacant twice (both in the context of scandals surrounding the Nixon administration) and was filled both times through this process, namely in 1973 following Spiro Agnew's resignation, and again in 1974 after Gerald Ford succeeded to the presidency. The amendment also established a procedure whereby a vice president may, if the president is unable to discharge the powers and duties of the office, temporarily assume the powers and duties of the office as acting president. Three vice presidents have briefly acted as president under the 25th Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, and on July 21, 2007; and Kamala Harris on November 19, 2021.\nThe persons who have served as vice president were born in or primarily affiliated with 27 states plus the District of Columbia. New York has produced the most of any state as eight have been born there and three others considered it their home state. Most vice presidents have been in their 50s or 60s and had political experience before assuming the office. Two vice presidents—George Clinton and John C. Calhoun—served under more than one president. Ill with tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. King, by an Act of Congress, was allowed to take the oath outside the United States. He is the only vice president to take his oath of office in a foreign country.\n\nPage: List of presidents of the United States by net worth\nSummary: The list of presidents of the United States by net worth at peak varies greatly. Debt and depreciation often means that presidents' net worth is less than $0 at the time of death. Most presidents before 1845 were extremely wealthy, especially Andrew Jackson and George Washington.    \nPresidents since 1929, when Herbert Hoover took office, have generally been wealthier than presidents of the late nineteenth and early twentieth centuries; with the exception of Harry S. Truman, all presidents since this time have been millionaires. These presidents have often received income from autobiographies and other writing. Except for Franklin D. Roosevelt and John F. Kennedy (both of whom died while in office), all presidents beginning with Calvin Coolidge have written autobiographies. In addition, many presidents—including Bill Clinton—have earned considerable income from public speaking after leaving office.The richest president in history may be Donald Trump. However, his net worth is not precisely known because the Trump Organization is privately held.Truman was among the poorest U.S. presidents, with a net worth considerably less than $1 million. His financial situation contributed to the doubling of the presidential salary to $100,000 in 1949. In addition, the presidential pension was created in 1958 when Truman was again experiencing financial difficulties. Harry and Bess Truman received the first Medicare cards in 1966 via the Social Security Act of 1965.\n\nPage: List of presidents of the United States by home state\nSummary: These lists give the states of primary affiliation and of birth for each president of the United States.\nInvoking: `Wikipedia` with `Joe Biden`\n\n\nPage: Joe Biden\nSummary: Joseph Robinette Biden Jr. (  BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.\nBorn in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. He became the oldest president in U.S. history, and the first to have a female vice president.\nAs president, Biden signed the American Rescue Plan Act in response to the COVID-19 pandemic and subsequent recession. He signed bipartisan bills on infrastructure and manufacturing. He proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that he signed into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme Court. He worked with congressional Republicans to resolve the 2023 United States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the Israel–Hamas war, Biden announced military support for Israel, and condemned the actions of Hamas and other Palestinian militants as terrorism. In April 2023, Biden announced his candidacy for the Democratic nomination in the 2024 presidential election.\n\nPage: Presidency of Joe Biden\nSummary: Joe Biden's tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president for two terms under president Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Biden won the presidency with a popular vote of over 81 million, the highest number of votes cast for a single United States presidential candidate. Upon his inauguration, he became the oldest president in American history, breaking the record set by his predecessor Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump's energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump's border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden alongside Democrats and Republicans in Congress, to invest in American roads, bridges, public transit, ports and broadband access. Biden signed the Juneteenth National Independence Day Act, making Juneteenth a federal holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. Supreme Court—the first Black woman to serve on the court. After The Supreme Court overturned Roe v. Wade, Biden took executive actions, such as the signing of Executive Order 14076, to preserve and protect women's health rights nationwide, against abortion bans in Republican led states. Biden proposed a significant expansion of the U.S. social safety net through the Build Back Better Act, but those efforts, along with voting rights legislation, failed in Congress. However, in August 2022, Biden signed the Inflation Reduction Act of 2022, a domestic appropriations bill that included some of the provisions of the Build Back Better Act after the entire bill failed to pass. It included significant federal investment in climate and domestic clean energy production, tax credits for solar panels, electric cars and other home energy programs as well as a three-year extension of Affordable Care Act subsidies. The administration's economic policies, known as \"Bidenomics\", were inspired and designed by Trickle-up economics. Described as growing the economy from the middle out and bottom up and growing the middle class. Biden signed the CHIPS and Science Act, bolstering the semiconductor and manufacturing industry, the Honoring our PACT Act, expanding health care for US veterans, the Bipartisan Safer Communities Act and the Electoral Count Reform and Presidential Transition Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, which repealed the Defense of Marriage Act and codified same-sex and interracial marriage in the United States. In response to the debt-ceiling crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of 2023, which restrains federal spending for fiscal years 2024 and 2025, implements minor changes to SNAP and TANF, includes energy permitting reform, claws back some IRS funding and unspent money for COVID-19, and suspends the debt ceiling to January 1, 2025. Biden established the American Climate Corps and created the first ever White House Office of Gun Violence Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers picket line during the 2023 United Auto Workers strike, making him the first US president to visit one.\nThe foreign policy goal of the Biden administration is to restore the US to a \"position of trusted leadership\" among global democracies in order to address the challenges posed by Russia and China. In foreign policy, Biden completed the withdrawal of U.S. military forces from Afghanistan, declaring an end to nation-building efforts and shifting U.S. foreign policy toward strategic competition with China and, to a lesser extent, Russia. However, during the withdrawal, the Afghan government collapsed and the Taliban seized control, leading to Biden receiving bipartisan criticism. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia as well as providing Ukraine with over $100 billion in combined military, economic, and humanitarian aid. Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi al-Qurashi, the leader of the Islamic State, and approved a drone strike which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed and created AUKUS, an international security alliance, together with Australia and the United Kingdom. Biden called for the expansion of NATO with the addition of Finland and Sweden, and rallied NATO allies in support of Ukraine. During the 2023 Israel–Hamas war, Biden condemned Hamas and other Palestinian militants as terrorism and announced American military support for Israel; Biden also showed his support and sympathy towards Palestinians affected by the war, sent humanitarian aid, and brokered a four-day temporary pause and hostage exchange.\n\nPage: Family of Joe Biden\nSummary: Joe Biden, the 46th and current president of the United States, has family members who are prominent in law, education, activism and politics. Biden's immediate family became the first family of the United States on his inauguration on January 20, 2021. His immediate family circle was also the second family of the United States from 2009 to 2017, when Biden was vice president. Biden's family is mostly descended from the British Isles, with most of their ancestors coming from Ireland and England, and a smaller number descending from the French.Of Joe Biden's sixteen great-great-grandparents, ten were born in Ireland. He is descended from the Blewitts of County Mayo and the Finnegans of County Louth. One of Biden's great-great-great-grandfathers was born in Sussex, England, and emigrated to Maryland in the United States by 1820.\n\nPage: Inauguration of Joe Biden\nSummary: The inauguration of Joe Biden as the 46th president of the United States took place on Wednesday, January 20, 2021, marking the start of the four-year term of Joe Biden as president and Kamala Harris as vice president. The 59th presidential inauguration took place on the West Front of the United States Capitol in Washington, D.C. Biden took the presidential oath of office, before which Harris took the vice presidential oath of office.\nThe inauguration took place amidst extraordinary political, public health, economic, and national security crises, including the ongoing COVID-19 pandemic; outgoing President Donald Trump's attempts to overturn the 2020 United States presidential election, which provoked an attack on the United States Capitol on January 6; Trump'\nInvoking: `Wikipedia` with `Delaware`\n\n\nPage: Delaware\nSummary: Delaware (  DEL-ə-wair) is a state in the northeast and Mid-Atlantic regions of the United States. It borders Maryland to its south and west, Pennsylvania to its north, New Jersey to its northeast, and the Atlantic Ocean to its east. The state's name derives from the adjacent Delaware Bay, which in turn was named after Thomas West, 3rd Baron De La Warr, an English nobleman and the Colony of Virginia's first colonial-era governor.Delaware occupies the northeastern portion of the Delmarva Peninsula, and some islands and territory within the Delaware River. It is the 2nd smallest and 6th least populous state, but also the 6th most densely populated. Delaware's most populous city is Wilmington, and the state's capital is Dover, the 2nd most populous city in Delaware. The state is divided into three counties, the fewest number of counties of any of the 50 U.S. states; from north to south, the three counties are: New Castle County, Kent County, and Sussex County.\nThe southern two counties, Kent and Sussex counties, historically have been predominantly agrarian economies. New Castle is more urbanized and is considered part of the Delaware Valley metropolitan statistical area that surrounds and includes Philadelphia, the nation's 6th most populous city. Delaware is considered part of the Southern United States by the U.S. Census Bureau, but the state's geography, culture, and history are a hybrid of the Mid-Atlantic and Northeastern regions of the country.Before Delaware coastline was explored and developed by Europeans in the 16th century, the state was inhabited by several Native Americans tribes, including the Lenape in the north and Nanticoke in the south. The state was first colonized by Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in 1631.\nDelaware was one of the Thirteen Colonies that participated in the American Revolution and American Revolutionary War, in which the American Continental Army, led by George Washington, defeated the British, ended British colonization and establishing the United States as a sovereign and independent nation.\nOn December 7, 1787, Delaware was the first state to ratify the Constitution of the United States, earning it the nickname \"The First State\".Since the turn of the 20th century, Delaware has become an onshore corporate haven whose corporate laws are deemed appealing to corporations; over half of all New York Stock Exchange-listed corporations and over three-fifths of the Fortune 500 is legally incorporated in the state.\n\nPage: Delaware City, Delaware\nSummary: Delaware City is a city in New Castle County, Delaware, United States. The population was 1,885 as of 2020. It is a small port town on the eastern terminus of the Chesapeake and Delaware Canal and is the location of the Forts Ferry Crossing to Fort Delaware on Pea Patch Island.\n\nPage: Delaware River\nSummary: The Delaware River is a major river in the Mid-Atlantic region of the United States and is the longest free-flowing (undammed) river in the Eastern United States. From the meeting of its branches in Hancock, New York, the river flows for 282 miles (454 km) along the borders of New York, Pennsylvania, New Jersey, and Delaware, before emptying into Delaware Bay.\nThe river has been recognized by the National Wildlife Federation as one of the country's Great Waters and has been called the \"Lifeblood of the Northeast\" by American Rivers. Its watershed drains an area of 13,539 square miles (35,070 km2) and provides drinking water for 17 million people, including half of New York City via the Delaware Aqueduct.\nThe Delaware River has two branches that rise in the Catskill Mountains of New York: the West Branch at Mount Jefferson in Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware County. The branches merge to form the main Delaware River at Hancock, New York. Flowing south, the river remains relatively undeveloped, with 152 miles (245 km) protected as the Upper, Middle, and Lower Delaware National Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, navigable, and significantly more industrial. This section forms the backbone of the Delaware Valley metropolitan area, serving the port cities of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the bay's outlet to the Atlantic Ocean between Cape May and Cape Henlopen.\nBefore the arrival of European settlers, the river was the homeland of the Lenape native people. They called the river Lenapewihittuk, or Lenape River, and Kithanne, meaning the largest river in this part of the country.In 1609, the river was visited by a Dutch East India Company expedition led by Henry Hudson. Hudson, an English navigator, was hired to find a western route to Cathay (China), but his encounters set the stage for Dutch colonization of North America in the 17th century. Early Dutch and Swedish settlements were established along the lower section of the river and Delaware Bay. Both colonial powers called the river the South River (Zuidrivier), compared to the Hudson River, which was known as the North River. After the English expelled the Dutch and took control of the New Netherland colony in 1664, the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, an English nobleman and the Virginia colony's first royal governor, who defended the colony during the First Anglo-Powhatan War.\n\nPage: University of Delaware\nSummary: The University of Delaware (colloquially known as UD or Delaware) is a privately governed, state-assisted land-grant research university located in Newark, Delaware. UD is the largest university in Delaware. It offers three associate's programs, 148 bachelor's programs, 121 master's programs (with 13 joint degrees), and 55 doctoral programs across its eight colleges. The main campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, and Georgetown. It is considered a large institution with approximately 18,200 undergraduate and 4,200 graduate students. It is a privately governed university which receives public funding for being a land-grant, sea-grant, and space-grant state-supported research institution.UD is classified among \"R1: Doctoral Universities – Very high research activity\". According to the National Science Foundation, UD spent $186 million on research and development in 2018, ranking it 119th in the nation.  It is recognized with the Community Engagement Classification by the Carnegie Foundation for the Advancement of Teaching.UD students, alumni, and sports teams are known as the \"Fightin' Blue Hens\", more commonly shortened to \"Blue Hens\", and the school colors are Delaware blue and gold. UD sponsors 21 men's and women's NCAA Division-I sports teams and have competed in the Colonial Athletic Association (CAA) since 2001.\n\n\n\nPage: Lenape\nSummary: The Lenape (English: , , ; Lenape languages: [lənaːpe]), also called the Lenni Lenape and Delaware people, are an Indigenous people of the Northeastern Woodlands, who live in the United States and Canada.The Lenape's historical territory includes present-day northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, western Long Island, and the lower Hudson Valley in New York state. Today they are based in Oklahoma, Wisconsin, and Ontario.\nDuring the last decades of the 18th century, European settlers and the effects of the American Revolutionary War displaced most Lenape from their homelands and pushed them north and west. In the 1860s, under the Indian removal policy, the U.S. federal government relocated most Lenape remaining in the Eastern United States to the Indian Territory and surrounding regions. Lenape people currently belong to the Delaware Nation and Delaware Tribe of Indians in Oklahoma, the Stockbridge–Munsee Community in Wisconsin, and the Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of Six Nations in Ontario.\n\n\nInvoking: `Wikipedia` with `Blue hen chicken`\n\n\nPage: Delaware Blue Hen\nSummary: The Delaware Blue Hen or Blue Hen of Delaware is a blue strain of American gamecock. Under the name Blue Hen Chicken it is the official bird of the State of Delaware. It is the emblem or mascot of several institutions in the state, among them the sports teams of the University of Delaware.\n\nPage: Delaware Fightin' Blue Hens\nSummary: The Delaware Fightin' Blue Hens are the athletic teams of the University of Delaware (UD) of Newark, Delaware, in the United States. The Blue Hens compete in the Football Championship Subdivision (FCS) of Division I of the National Collegiate Athletic Association (NCAA) as members of the Coastal Athletic Association and its technically separate football league, CAA Football.\nOn November 28, 2023, UD and Conference USA (CUSA) jointly announced that UD would start a transition to the Division I Football Bowl Subdivision (FBS) in 2024 and join CUSA in 2025. UD will continue to compete in both sides of the CAA in 2024–25; it will be ineligible for the FCS playoffs due to NCAA rules for transitioning programs, but will be eligible for all non-football CAA championships. Upon joining CUSA, UD will be eligible for all conference championship events except the football championship game; it will become eligible for that event upon completing the FBS transition in 2026. At the same time, UD also announced it would add one women's sport due to Title IX considerations, and would also be seeking conference homes for the seven sports that UD sponsors but CUSA does not. The new women's sport would later be announced as ice hockey; UD will join College Hockey America for its first season of varsity play in 2025–26.\n\nPage: Brahma chicken\nSummary: The Brahma is an American breed of chicken. It was bred in the United States from birds imported from the Chinese port of Shanghai,: 78  and was the principal American meat breed from the 1850s until about 1930.\n\nPage: Silkie\nSummary: The Silkie (also known as the Silky or Chinese silk chicken) is a breed of chicken named for its atypically fluffy plumage, which is said to feel like silk and satin. The breed has several other unusual qualities, such as black skin and bones, blue earlobes, and five toes on each foot, whereas most chickens have only four. They are often exhibited in poultry shows, and also appear in various colors. In addition to their distinctive physical characteristics, Silkies are well known for their calm and friendly temperament. It is among the most docile of poultry. Hens are also exceptionally broody, and care for young well. Although they are fair layers themselves, laying only about three eggs a week, they are commonly used to hatch eggs from other breeds and bird species due to their broody nature. Silkie chickens have been bred to have a wide variety of colors which include but are not limited to: Black, Blue, Buff, Partridge, Splash, White, Lavender, Paint and Porcelain.\n\nPage: Silverudd Blue\nSummary: The Silverudd Blue, Swedish: Silverudds Blå, is a Swedish breed of chicken. It was developed by Martin Silverudd in Småland, in southern Sweden. Hens lay blue/green eggs, weighing 50–65 grams. The flock-book for the breed is kept by the Svenska Kulturhönsföreningen – the Swedish Cultural Hen Association. It was initially known by various names including Isbar, Blue Isbar and Svensk Grönvärpare, or \"Swedish green egg layer\"; in 2016 it was renamed to 'Silverudd Blue' after its creator.The current US president is Joe Biden. His home state is Delaware. The home state bird of Delaware is the Delaware Blue Hen. The scientific name of the Delaware Blue Hen is Gallus gallus domesticus.\n\n> Finished chain.\n\n{'input': \"Who is the current US president? What's their home state? What's their home state's bird? What's that bird's scientific name?\",\n 'output': 'The current US president is Joe Biden. His home state is Delaware. The home state bird of Delaware is the Delaware Blue Hen. The scientific name of the Delaware Blue Hen is Gallus gallus domesticus.'}\n\n\nLangSmith trace\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nAdding moderation\nNext\nUsing tools\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Adding moderation | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/moderation",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookAdding moderation\nAdding moderation\n\nThis shows how to add in moderation (or other safeguards) around your LLM application.\n\n%pip install --upgrade --quiet  langchain langchain-openai\n\nfrom langchain.chains import OpenAIModerationChain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import OpenAI\n\nmoderate = OpenAIModerationChain()\n\nmodel = OpenAI()\nprompt = ChatPromptTemplate.from_messages([(\"system\", \"repeat after me: {input}\")])\n\nchain = prompt | model\n\nchain.invoke({\"input\": \"you are stupid\"})\n\n'\\n\\nYou are stupid.'\n\nmoderated_chain = chain | moderate\n\nmoderated_chain.invoke({\"input\": \"you are stupid\"})\n\n{'input': '\\n\\nYou are stupid',\n 'output': \"Text was found that violates OpenAI's content policy.\"}\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nAdding memory\nNext\nManaging prompt size\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Adding memory | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/memory",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookAdding memory\nAdding memory\n\nThis shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook it up manually\n\n%pip install --upgrade --quiet  langchain langchain-openai\n\nfrom operator import itemgetter\n\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI()\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful chatbot\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\n\nmemory = ConversationBufferMemory(return_messages=True)\n\nmemory.load_memory_variables({})\n\n{'history': []}\n\nchain = (\n    RunnablePassthrough.assign(\n        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n    )\n    | prompt\n    | model\n)\n\ninputs = {\"input\": \"hi im bob\"}\nresponse = chain.invoke(inputs)\nresponse\n\nAIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, example=False)\n\nmemory.save_context(inputs, {\"output\": response.content})\n\nmemory.load_memory_variables({})\n\n{'history': [HumanMessage(content='hi im bob', additional_kwargs={}, example=False),\n  AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, example=False)]}\n\ninputs = {\"input\": \"whats my name\"}\nresponse = chain.invoke(inputs)\nresponse\n\nAIMessage(content='Your name is Bob.', additional_kwargs={}, example=False)\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nRouting by semantic similarity\nNext\nAdding moderation\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Routing by semantic similarity | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/embedding_router",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookRouting by semantic similarity\nRouting by semantic similarity\n\nWith LCEL you can easily add custom routing logic to your chain to dynamically determine the chain logic based on user input. All you need to do is define a function that given an input returns a Runnable.\n\nOne especially useful technique is to use embeddings to route a query to the most relevant prompt. Here’s a very simple example.\n\n%pip install --upgrade --quiet  langchain-core langchain langchain-openai\n\nfrom langchain.utils.math import cosine_similarity\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nphysics_template = \"\"\"You are a very smart physics professor. \\\nYou are great at answering questions about physics in a concise and easy to understand manner. \\\nWhen you don't know the answer to a question you admit that you don't know.\n\nHere is a question:\n{query}\"\"\"\n\nmath_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\nYou are so good because you are able to break down hard problems into their component parts, \\\nanswer the component parts, and then put them together to answer the broader question.\n\nHere is a question:\n{query}\"\"\"\n\nembeddings = OpenAIEmbeddings()\nprompt_templates = [physics_template, math_template]\nprompt_embeddings = embeddings.embed_documents(prompt_templates)\n\n\ndef prompt_router(input):\n    query_embedding = embeddings.embed_query(input[\"query\"])\n    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n    most_similar = prompt_templates[similarity.argmax()]\n    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n    return PromptTemplate.from_template(most_similar)\n\n\nchain = (\n    {\"query\": RunnablePassthrough()}\n    | RunnableLambda(prompt_router)\n    | ChatOpenAI()\n    | StrOutputParser()\n)\n\nprint(chain.invoke(\"What's a black hole\"))\n\nUsing PHYSICS\nA black hole is a region in space where gravity is extremely strong, so strong that nothing, not even light, can escape its gravitational pull. It is formed when a massive star collapses under its own gravity during a supernova explosion. The collapse causes an incredibly dense mass to be concentrated in a small volume, creating a gravitational field that is so intense that it warps space and time. Black holes have a boundary called the event horizon, which marks the point of no return for anything that gets too close. Beyond the event horizon, the gravitational pull is so strong that even light cannot escape, hence the name \"black hole.\" While we have a good understanding of black holes, there is still much to learn, especially about what happens inside them.\n\nprint(chain.invoke(\"What's a path integral\"))\n\nUsing MATH\nThank you for your kind words! I will do my best to break down the concept of a path integral for you.\n\nIn mathematics and physics, a path integral is a mathematical tool used to calculate the probability amplitude or wave function of a particle or system of particles. It was introduced by Richard Feynman and is an integral over all possible paths that a particle can take to go from an initial state to a final state.\n\nTo understand the concept better, let's consider an example. Suppose we have a particle moving from point A to point B in space. Classically, we would describe this particle's motion using a definite trajectory, but in quantum mechanics, particles can simultaneously take multiple paths from A to B.\n\nThe path integral formalism considers all possible paths that the particle could take and assigns a probability amplitude to each path. These probability amplitudes are then added up, taking into account the interference effects between different paths.\n\nTo calculate a path integral, we need to define an action, which is a mathematical function that describes the behavior of the system. The action is usually expressed in terms of the particle's position, velocity, and time.\n\nOnce we have the action, we can write down the path integral as an integral over all possible paths. Each path is weighted by a factor determined by the action and the principle of least action, which states that a particle takes a path that minimizes the action.\n\nMathematically, the path integral is expressed as:\n\n∫ e^(iS/ħ) D[x(t)]\n\nHere, S is the action, ħ is the reduced Planck's constant, and D[x(t)] represents the integration over all possible paths x(t) of the particle.\n\nBy evaluating this integral, we can obtain the probability amplitude for the particle to go from the initial state to the final state. The absolute square of this amplitude gives us the probability of finding the particle in a particular state.\n\nPath integrals have proven to be a powerful tool in various areas of physics, including quantum mechanics, quantum field theory, and statistical mechanics. They allow us to study complex systems and calculate probabilities that are difficult to obtain using other methods.\n\nI hope this explanation helps you understand the concept of a path integral. If you have any further questions, feel free to ask!\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nCode writing\nNext\nAdding memory\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Agents | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/agent",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookAgents\nAgents\n\nYou can pass a Runnable into an agent. Make sure you have langchainhub installed: pip install langchainhub\n\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, tool\nfrom langchain.agents.output_parsers import XMLAgentOutputParser\nfrom langchain_community.chat_models import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-2\")\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search things about current events.\"\"\"\n    return \"32 degrees\"\n\ntool_list = [search]\n\n# Get the prompt to use - you can modify this!\nprompt = hub.pull(\"hwchase17/xml-agent-convo\")\n\n# Logic for going from intermediate steps to a string to pass into model\n# This is pretty tied to the prompt\ndef convert_intermediate_steps(intermediate_steps):\n    log = \"\"\n    for action, observation in intermediate_steps:\n        log += (\n            f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\n            f\"</tool_input><observation>{observation}</observation>\"\n        )\n    return log\n\n\n# Logic for converting tools to string to go in prompt\ndef convert_tools(tools):\n    return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n\n\nBuilding an agent from a runnable usually involves a few things:\n\nData processing for the intermediate steps. These need to be represented in a way that the language model can recognize them. This should be pretty tightly coupled to the instructions in the prompt\n\nThe prompt itself\n\nThe model, complete with stop tokens if needed\n\nThe output parser - should be in sync with how the prompt specifies things to be formatted.\n\nagent = (\n    {\n        \"input\": lambda x: x[\"input\"],\n        \"agent_scratchpad\": lambda x: convert_intermediate_steps(\n            x[\"intermediate_steps\"]\n        ),\n    }\n    | prompt.partial(tools=convert_tools(tool_list))\n    | model.bind(stop=[\"</tool_input>\", \"</final_answer>\"])\n    | XMLAgentOutputParser()\n)\n\nagent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True)\n\nagent_executor.invoke({\"input\": \"whats the weather in New york?\"})\n\n\n\n> Entering new AgentExecutor chain...\n <tool>search</tool><tool_input>weather in New York32 degrees <tool>search</tool>\n<tool_input>weather in New York32 degrees <final_answer>The weather in New York is 32 degrees\n\n> Finished chain.\n\n{'input': 'whats the weather in New york?',\n 'output': 'The weather in New York is 32 degrees'}\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nQuerying a SQL DB\nNext\nCode writing\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Code writing | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/code_writing",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookCode writing\nCode writing\n\nExample of how to use LCEL to write Python code.\n\n%pip install --upgrade --quiet  langchain-core langchain-experimental langchain-openai\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import (\n    ChatPromptTemplate,\n)\nfrom langchain_experimental.utilities import PythonREPL\nfrom langchain_openai import ChatOpenAI\n\ntemplate = \"\"\"Write some python code to solve the user's problem. \n\nReturn only python code in Markdown format, e.g.:\n\n```python\n....\n```\"\"\"\nprompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n\nmodel = ChatOpenAI()\n\ndef _sanitize_output(text: str):\n    _, after = text.split(\"```python\")\n    return after.split(\"```\")[0]\n\nchain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run\n\nchain.invoke({\"input\": \"whats 2 plus 2\"})\n\nPython REPL can execute arbitrary code. Use with caution.\n\n'4\\n'\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nAgents\nNext\nRouting by semantic similarity\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Querying a SQL DB | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/sql_db",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookQuerying a SQL DB\nQuerying a SQL DB\n\nWe can replicate our SQLDatabaseChain with Runnables.\n\n%pip install –upgrade –quiet langchain langchain-openai\n\nfrom langchain_core.prompts import ChatPromptTemplate\n\ntemplate = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n{schema}\n\nQuestion: {question}\nSQL Query:\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nfrom langchain_community.utilities import SQLDatabase\n\n\nWe’ll need the Chinook sample DB for this example. There’s many places to download it from, e.g. https://database.guide/2-sample-databases-sqlite/\n\ndb = SQLDatabase.from_uri(\"sqlite:///./Chinook.db\")\n\ndef get_schema(_):\n    return db.get_table_info()\n\ndef run_query(query):\n    return db.run(query)\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI()\n\nsql_response = (\n    RunnablePassthrough.assign(schema=get_schema)\n    | prompt\n    | model.bind(stop=[\"\\nSQLResult:\"])\n    | StrOutputParser()\n)\n\nsql_response.invoke({\"question\": \"How many employees are there?\"})\n\n'SELECT COUNT(*) FROM Employee'\n\ntemplate = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n{schema}\n\nQuestion: {question}\nSQL Query: {query}\nSQL Response: {response}\"\"\"\nprompt_response = ChatPromptTemplate.from_template(template)\n\nfull_chain = (\n    RunnablePassthrough.assign(query=sql_response).assign(\n        schema=get_schema,\n        response=lambda x: db.run(x[\"query\"]),\n    )\n    | prompt_response\n    | model\n)\n\nfull_chain.invoke({\"question\": \"How many employees are there?\"})\n\nAIMessage(content='There are 8 employees.', additional_kwargs={}, example=False)\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nMultiple chains\nNext\nAgents\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Multiple chains | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/multiple_chains",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookMultiple chains\nMultiple chains\n\nRunnables can easily be used to string together multiple Chains\n\n%pip install –upgrade –quiet langchain langchain-openai\n\nfrom operator import itemgetter\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\nprompt2 = ChatPromptTemplate.from_template(\n    \"what country is the city {city} in? respond in {language}\"\n)\n\nmodel = ChatOpenAI()\n\nchain1 = prompt1 | model | StrOutputParser()\n\nchain2 = (\n    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n    | prompt2\n    | model\n    | StrOutputParser()\n)\n\nchain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})\n\n'El país donde se encuentra la ciudad de Honolulu, donde nació Barack Obama, el 44º Presidente de los Estados Unidos, es Estados Unidos. Honolulu se encuentra en la isla de Oahu, en el estado de Hawái.'\n\nfrom langchain_core.runnables import RunnablePassthrough\n\nprompt1 = ChatPromptTemplate.from_template(\n    \"generate a {attribute} color. Return the name of the color and nothing else:\"\n)\nprompt2 = ChatPromptTemplate.from_template(\n    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\"\n)\nprompt3 = ChatPromptTemplate.from_template(\n    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n)\nprompt4 = ChatPromptTemplate.from_template(\n    \"What is the color of {fruit} and the flag of {country}?\"\n)\n\nmodel_parser = model | StrOutputParser()\n\ncolor_generator = (\n    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n)\ncolor_to_fruit = prompt2 | model_parser\ncolor_to_country = prompt3 | model_parser\nquestion_generator = (\n    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n)\n\nquestion_generator.invoke(\"warm\")\n\nChatPromptValue(messages=[HumanMessage(content='What is the color of strawberry and the flag of China?', additional_kwargs={}, example=False)])\n\nprompt = question_generator.invoke(\"warm\")\nmodel.invoke(prompt)\n\nAIMessage(content='The color of an apple is typically red or green. The flag of China is predominantly red with a large yellow star in the upper left corner and four smaller yellow stars surrounding it.', additional_kwargs={}, example=False)\n\nBranching and Merging​\n\nYou may want the output of one component to be processed by 2 or more other components. RunnableParallels let you split or fork the chain so multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:\n\n     Input\n      / \\\n     /   \\\n Branch1 Branch2\n     \\   /\n      \\ /\n      Combine\n\nplanner = (\n    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n    | ChatOpenAI()\n    | StrOutputParser()\n    | {\"base_response\": RunnablePassthrough()}\n)\n\narguments_for = (\n    ChatPromptTemplate.from_template(\n        \"List the pros or positive aspects of {base_response}\"\n    )\n    | ChatOpenAI()\n    | StrOutputParser()\n)\narguments_against = (\n    ChatPromptTemplate.from_template(\n        \"List the cons or negative aspects of {base_response}\"\n    )\n    | ChatOpenAI()\n    | StrOutputParser()\n)\n\nfinal_responder = (\n    ChatPromptTemplate.from_messages(\n        [\n            (\"ai\", \"{original_response}\"),\n            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n            (\"system\", \"Generate a final response given the critique\"),\n        ]\n    )\n    | ChatOpenAI()\n    | StrOutputParser()\n)\n\nchain = (\n    planner\n    | {\n        \"results_1\": arguments_for,\n        \"results_2\": arguments_against,\n        \"original_response\": itemgetter(\"base_response\"),\n    }\n    | final_responder\n)\n\nchain.invoke({\"input\": \"scrum\"})\n\n'While Scrum has its potential cons and challenges, many organizations have successfully embraced and implemented this project management framework to great effect. The cons mentioned above can be mitigated or overcome with proper training, support, and a commitment to continuous improvement. It is also important to note that not all cons may be applicable to every organization or project.\\n\\nFor example, while Scrum may be complex initially, with proper training and guidance, teams can quickly grasp the concepts and practices. The lack of predictability can be mitigated by implementing techniques such as velocity tracking and release planning. The limited documentation can be addressed by maintaining a balance between lightweight documentation and clear communication among team members. The dependency on team collaboration can be improved through effective communication channels and regular team-building activities.\\n\\nScrum can be scaled and adapted to larger projects by using frameworks like Scrum of Scrums or LeSS (Large Scale Scrum). Concerns about speed versus quality can be addressed by incorporating quality assurance practices, such as continuous integration and automated testing, into the Scrum process. Scope creep can be managed by having a well-defined and prioritized product backlog, and a strong product owner can be developed through training and mentorship.\\n\\nResistance to change can be overcome by providing proper education and communication to stakeholders and involving them in the decision-making process. Ultimately, the cons of Scrum can be seen as opportunities for growth and improvement, and with the right mindset and support, they can be effectively managed.\\n\\nIn conclusion, while Scrum may have its challenges and potential cons, the benefits and advantages it offers in terms of collaboration, flexibility, adaptability, transparency, and customer satisfaction make it a widely adopted and successful project management framework. With proper implementation and continuous improvement, organizations can leverage Scrum to drive innovation, efficiency, and project success.'\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nRAG\nNext\nQuerying a SQL DB\nBranching and Merging\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Prompt + LLM | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookPrompt + LLM\nPrompt + LLM\n\nThe most common and valuable composition is taking:\n\nPromptTemplate / ChatPromptTemplate -> LLM / ChatModel -> OutputParser\n\nAlmost any other chains you build will use this building block.\n\nPromptTemplate + LLM​\n\nThe simplest composition is just combining a prompt and model to create a chain that takes user input, adds it to a prompt, passes it to a model, and returns the raw model output.\n\nNote, you can mix and match PromptTemplate/ChatPromptTemplates and LLMs/ChatModels as you like here.\n\n%pip install –upgrade –quiet langchain langchain-openai\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")\nmodel = ChatOpenAI()\nchain = prompt | model\n\nchain.invoke({\"foo\": \"bears\"})\n\nAIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)\n\n\nOften times we want to attach kwargs that’ll be passed to each model call. Here are a few examples of that:\n\nAttaching Stop Sequences​\nchain = prompt | model.bind(stop=[\"\\n\"])\n\nchain.invoke({\"foo\": \"bears\"})\n\nAIMessage(content='Why did the bear never wear shoes?', additional_kwargs={}, example=False)\n\nAttaching Function Call information​\nfunctions = [\n    {\n        \"name\": \"joke\",\n        \"description\": \"A joke\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"setup\": {\"type\": \"string\", \"description\": \"The setup for the joke\"},\n                \"punchline\": {\n                    \"type\": \"string\",\n                    \"description\": \"The punchline for the joke\",\n                },\n            },\n            \"required\": [\"setup\", \"punchline\"],\n        },\n    }\n]\nchain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n\nchain.invoke({\"foo\": \"bears\"}, config={})\n\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'joke', 'arguments': '{\\n  \"setup\": \"Why don\\'t bears wear shoes?\",\\n  \"punchline\": \"Because they have bear feet!\"\\n}'}}, example=False)\n\nPromptTemplate + LLM + OutputParser​\n\nWe can also add in an output parser to easily transform the raw LLM/ChatModel output into a more workable format\n\nfrom langchain_core.output_parsers import StrOutputParser\n\nchain = prompt | model | StrOutputParser()\n\n\nNotice that this now returns a string - a much more workable format for downstream tasks\n\nchain.invoke({\"foo\": \"bears\"})\n\n\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"\n\nFunctions Output Parser​\n\nWhen you specify the function to return, you may just want to parse that directly\n\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n\nchain = (\n    prompt\n    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n    | JsonOutputFunctionsParser()\n)\n\nchain.invoke({\"foo\": \"bears\"})\n\n{'setup': \"Why don't bears like fast food?\",\n 'punchline': \"Because they can't catch it!\"}\n\nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n\nchain = (\n    prompt\n    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n)\n\nchain.invoke({\"foo\": \"bears\"})\n\n\"Why don't bears wear shoes?\"\n\nSimplifying input​\n\nTo make invocation even simpler, we can add a RunnableParallel to take care of creating the prompt input dict for us:\n\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\n\nmap_ = RunnableParallel(foo=RunnablePassthrough())\nchain = (\n    map_\n    | prompt\n    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n)\n\nchain.invoke(\"bears\")\n\n\"Why don't bears wear shoes?\"\n\n\nSince we’re composing our map with another Runnable, we can even use some syntactic sugar and just use a dict:\n\nchain = (\n    {\"foo\": RunnablePassthrough()}\n    | prompt\n    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n)\n\nchain.invoke(\"bears\")\n\n\"Why don't bears like fast food?\"\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nCookbook\nNext\nRAG\nPromptTemplate + LLM\nAttaching Stop Sequences\nAttaching Function Call information\nPromptTemplate + LLM + OutputParser\nFunctions Output Parser\nSimplifying input\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Add message history (memory) | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/message_history",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toAdd message history (memory)\nAdd message history (memory)\n\nThe RunnableWithMessageHistory lets us add message history to certain types of chains. It wraps another Runnable and manages the chat message history for it.\n\nSpecifically, it can be used for any Runnable that takes as input one of\n\na sequence of BaseMessage\na dict with a key that takes a sequence of BaseMessage\na dict with a key that takes the latest message(s) as a string or sequence of BaseMessage, and a separate key that takes historical messages\n\nAnd returns as output one of\n\na string that can be treated as the contents of an AIMessage\na sequence of BaseMessage\na dict with a key that contains a sequence of BaseMessage\n\nLet’s take a look at some examples to see how it works. First we construct a runnable (which here accepts a dict as input and returns a message as output):\n\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai.chat_models import ChatOpenAI\n\nmodel = ChatOpenAI()\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n        ),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\nrunnable = prompt | model\n\n\nTo manage the message history, we will need: 1. This runnable; 2. A callable that returns an instance of BaseChatMessageHistory.\n\nCheck out the memory integrations page for implementations of chat message histories using Redis and other providers. Here we demonstrate using an in-memory ChatMessageHistory as well as more persistent storage using RedisChatMessageHistory.\n\nIn-memory​\n\nBelow we show a simple example in which the chat history lives in memory, in this case via a global Python dict.\n\nWe construct a callable get_session_history that references this dict to return an instance of ChatMessageHistory. The arguments to the callable can be specified by passing a configuration to the RunnableWithMessageHistory at runtime. By default, the configuration parameter is expected to be a single string session_id. This can be adjusted via the history_factory_config kwarg.\n\nUsing the single-parameter default:\n\nfrom langchain_community.chat_message_histories import ChatMessageHistory\nfrom langchain_core.chat_history import BaseChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n\nstore = {}\n\n\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n\n\nwith_message_history = RunnableWithMessageHistory(\n    runnable,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\",\n)\n\n\nNote that we’ve specified input_messages_key (the key to be treated as the latest input message) and history_messages_key (the key to add historical messages to).\n\nWhen invoking this new runnable, we specify the corresponding chat history via a configuration parameter:\n\nwith_message_history.invoke(\n    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n    config={\"configurable\": {\"session_id\": \"abc123\"}},\n)\n\nAIMessage(content='Cosine is a trigonometric function that calculates the ratio of the adjacent side to the hypotenuse of a right triangle.')\n\n# Remembers\nwith_message_history.invoke(\n    {\"ability\": \"math\", \"input\": \"What?\"},\n    config={\"configurable\": {\"session_id\": \"abc123\"}},\n)\n\nAIMessage(content='Cosine is a mathematical function used to calculate the length of a side in a right triangle.')\n\n# New session_id --> does not remember.\nwith_message_history.invoke(\n    {\"ability\": \"math\", \"input\": \"What?\"},\n    config={\"configurable\": {\"session_id\": \"def234\"}},\n)\n\nAIMessage(content='I can help with math problems. What do you need assistance with?')\n\n\nThe configuration parameters by which we track message histories can be customized by passing in a list of ConfigurableFieldSpec objects to the history_factory_config parameter. Below, we use two parameters: a user_id and conversation_id.\n\nfrom langchain_core.runnables import ConfigurableFieldSpec\n\nstore = {}\n\n\ndef get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n    if (user_id, conversation_id) not in store:\n        store[(user_id, conversation_id)] = ChatMessageHistory()\n    return store[(user_id, conversation_id)]\n\n\nwith_message_history = RunnableWithMessageHistory(\n    runnable,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\",\n    history_factory_config=[\n        ConfigurableFieldSpec(\n            id=\"user_id\",\n            annotation=str,\n            name=\"User ID\",\n            description=\"Unique identifier for the user.\",\n            default=\"\",\n            is_shared=True,\n        ),\n        ConfigurableFieldSpec(\n            id=\"conversation_id\",\n            annotation=str,\n            name=\"Conversation ID\",\n            description=\"Unique identifier for the conversation.\",\n            default=\"\",\n            is_shared=True,\n        ),\n    ],\n)\n\nwith_message_history.invoke(\n    {\"ability\": \"math\", \"input\": \"Hello\"},\n    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},\n)\n\nExamples with runnables of different signatures​\n\nThe above runnable takes a dict as input and returns a BaseMessage. Below we show some alternatives.\n\nMessages input, dict output​\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.runnables import RunnableParallel\n\nchain = RunnableParallel({\"output_message\": ChatOpenAI()})\n\n\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n\n\nwith_message_history = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    output_messages_key=\"output_message\",\n)\n\nwith_message_history.invoke(\n    [HumanMessage(content=\"What did Simone de Beauvoir believe about free will\")],\n    config={\"configurable\": {\"session_id\": \"baz\"}},\n)\n\n{'output_message': AIMessage(content=\"Simone de Beauvoir believed in the existence of free will. She argued that individuals have the ability to make choices and determine their own actions, even in the face of social and cultural constraints. She rejected the idea that individuals are purely products of their environment or predetermined by biology or destiny. Instead, she emphasized the importance of personal responsibility and the need for individuals to actively engage in creating their own lives and defining their own existence. De Beauvoir believed that freedom and agency come from recognizing one's own freedom and actively exercising it in the pursuit of personal and collective liberation.\")}\n\nwith_message_history.invoke(\n    [HumanMessage(content=\"How did this compare to Sartre\")],\n    config={\"configurable\": {\"session_id\": \"baz\"}},\n)\n\n{'output_message': AIMessage(content='Simone de Beauvoir\\'s views on free will were closely aligned with those of her contemporary and partner Jean-Paul Sartre. Both de Beauvoir and Sartre were existentialist philosophers who emphasized the importance of individual freedom and the rejection of determinism. They believed that human beings have the capacity to transcend their circumstances and create their own meaning and values.\\n\\nSartre, in his famous work \"Being and Nothingness,\" argued that human beings are condemned to be free, meaning that we are burdened with the responsibility of making choices and defining ourselves in a world that lacks inherent meaning. Like de Beauvoir, Sartre believed that individuals have the ability to exercise their freedom and make choices in the face of external and internal constraints.\\n\\nWhile there may be some nuanced differences in their philosophical writings, overall, de Beauvoir and Sartre shared a similar belief in the existence of free will and the importance of individual agency in shaping one\\'s own life.')}\n\nMessages input, messages output​\nRunnableWithMessageHistory(\n    ChatOpenAI(),\n    get_session_history,\n)\n\nDict with single key for all messages input, messages output​\nfrom operator import itemgetter\n\nRunnableWithMessageHistory(\n    itemgetter(\"input_messages\") | ChatOpenAI(),\n    get_session_history,\n    input_messages_key=\"input_messages\",\n)\n\nPersistent storage​\n\nIn many cases it is preferable to persist conversation histories. RunnableWithMessageHistory is agnostic as to how the get_session_history callable retrieves its chat message histories. See here for an example using a local filesystem. Below we demonstrate how one could use Redis. Check out the memory integrations page for implementations of chat message histories using other providers.\n\nSetup​\n\nWe’ll need to install Redis if it’s not installed already:\n\n%pip install --upgrade --quiet redis\n\n\nStart a local Redis Stack server if we don’t have an existing Redis deployment to connect to:\n\ndocker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n\nREDIS_URL = \"redis://localhost:6379/0\"\n\nLangSmith​\n\nLangSmith is especially useful for something like message history injection, where it can be hard to otherwise understand what the inputs are to various parts of the chain.\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to uncoment the below and set your environment variables to start logging traces:\n\n# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n\n\nUpdating the message history implementation just requires us to define a new callable, this time returning an instance of RedisChatMessageHistory:\n\nfrom langchain_community.chat_message_histories import RedisChatMessageHistory\n\n\ndef get_message_history(session_id: str) -> RedisChatMessageHistory:\n    return RedisChatMessageHistory(session_id, url=REDIS_URL)\n\n\nwith_message_history = RunnableWithMessageHistory(\n    runnable,\n    get_message_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\",\n)\n\n\nWe can invoke as before:\n\nwith_message_history.invoke(\n    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n    config={\"configurable\": {\"session_id\": \"foobar\"}},\n)\n\nAIMessage(content='Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.')\n\nwith_message_history.invoke(\n    {\"ability\": \"math\", \"input\": \"What's its inverse\"},\n    config={\"configurable\": {\"session_id\": \"foobar\"}},\n)\n\nAIMessage(content='The inverse of cosine is the arccosine function, denoted as acos or cos^-1, which gives the angle corresponding to a given cosine value.')\n\n\nLangsmith trace\n\nLooking at the Langsmith trace for the second call, we can see that when constructing the prompt, a “history” variable has been injected which is a list of two messages (our first input and first output).\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nInspect your runnables\nNext\nCookbook\nIn-memory\nExamples with runnables of different signatures\nPersistent storage\nSetup\nLangSmith\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Inspect your runnables | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/inspect",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toInspect your runnables\nInspect your runnables\n\nOnce you create a runnable with LCEL, you may often want to inspect it to get a better sense for what is going on. This notebook covers some methods for doing so.\n\nFirst, let’s create an example LCEL. We will create one that does retrieval\n\n%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken\n\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nmodel = ChatOpenAI()\n\nchain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nGet a graph​\n\nYou can get a graph of the runnable\n\nchain.get_graph()\n\nPrint a graph​\n\nWhile that is not super legible, you can print it to get a display that’s easier to understand\n\nchain.get_graph().print_ascii()\n\n           +---------------------------------+         \n           | Parallel<context,question>Input |         \n           +---------------------------------+         \n                    **               **                \n                 ***                   ***             \n               **                         **           \n+----------------------+              +-------------+  \n| VectorStoreRetriever |              | Passthrough |  \n+----------------------+              +-------------+  \n                    **               **                \n                      ***         ***                  \n                         **     **                     \n           +----------------------------------+        \n           | Parallel<context,question>Output |        \n           +----------------------------------+        \n                             *                         \n                             *                         \n                             *                         \n                  +--------------------+               \n                  | ChatPromptTemplate |               \n                  +--------------------+               \n                             *                         \n                             *                         \n                             *                         \n                      +------------+                   \n                      | ChatOpenAI |                   \n                      +------------+                   \n                             *                         \n                             *                         \n                             *                         \n                   +-----------------+                 \n                   | StrOutputParser |                 \n                   +-----------------+                 \n                             *                         \n                             *                         \n                             *                         \n                +-----------------------+              \n                | StrOutputParserOutput |              \n                +-----------------------+              \n\nGet the prompts​\n\nAn important part of every chain is the prompts that are used. You can get the prompts present in the chain:\n\nchain.get_prompts()\n\n[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])]\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nStream custom generator functions\nNext\nAdd message history (memory)\nGet a graph\nPrint a graph\nGet the prompts\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Stream custom generator functions | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/generators",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toStream custom generator functions\nStream custom generator functions\n\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a LCEL pipeline.\n\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\n\nThese are useful for: - implementing a custom output parser - modifying the output of a previous step, while preserving streaming capabilities\n\nLet’s implement a custom output parser for comma-separated lists.\n\nSync version​\n%pip install --upgrade --quiet  langchain langchain-openai\n\nfrom typing import Iterator, List\n\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_template(\n    \"Write a comma-separated list of 5 animals similar to: {animal}\"\n)\nmodel = ChatOpenAI(temperature=0.0)\n\nstr_chain = prompt | model | StrOutputParser()\n\nfor chunk in str_chain.stream({\"animal\": \"bear\"}):\n    print(chunk, end=\"\", flush=True)\n\nlion, tiger, wolf, gorilla, panda\n\nstr_chain.invoke({\"animal\": \"bear\"})\n\n'lion, tiger, wolf, gorilla, panda'\n\n# This is a custom parser that splits an iterator of llm tokens\n# into a list of strings separated by commas\ndef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n    # hold partial input until we get a comma\n    buffer = \"\"\n    for chunk in input:\n        # add current chunk to buffer\n        buffer += chunk\n        # while there are commas in the buffer\n        while \",\" in buffer:\n            # split buffer on comma\n            comma_index = buffer.index(\",\")\n            # yield everything before the comma\n            yield [buffer[:comma_index].strip()]\n            # save the rest for the next iteration\n            buffer = buffer[comma_index + 1 :]\n    # yield the last chunk\n    yield [buffer.strip()]\n\nlist_chain = str_chain | split_into_list\n\nfor chunk in list_chain.stream({\"animal\": \"bear\"}):\n    print(chunk, flush=True)\n\n['lion']\n['tiger']\n['wolf']\n['gorilla']\n['panda']\n\nlist_chain.invoke({\"animal\": \"bear\"})\n\n['lion', 'tiger', 'wolf', 'gorilla', 'panda']\n\nAsync version​\nfrom typing import AsyncIterator\n\n\nasync def asplit_into_list(\n    input: AsyncIterator[str],\n) -> AsyncIterator[List[str]]:  # async def\n    buffer = \"\"\n    async for (\n        chunk\n    ) in input:  # `input` is a `async_generator` object, so use `async for`\n        buffer += chunk\n        while \",\" in buffer:\n            comma_index = buffer.index(\",\")\n            yield [buffer[:comma_index].strip()]\n            buffer = buffer[comma_index + 1 :]\n    yield [buffer.strip()]\n\n\nlist_chain = str_chain | asplit_into_list\n\nasync for chunk in list_chain.astream({\"animal\": \"bear\"}):\n    print(chunk, flush=True)\n\n['lion']\n['tiger']\n['wolf']\n['gorilla']\n['panda']\n\nawait list_chain.ainvoke({\"animal\": \"bear\"})\n\n['lion', 'tiger', 'wolf', 'gorilla', 'panda']\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nAdd fallbacks\nNext\nInspect your runnables\nSync version\nAsync version\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Add fallbacks | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/fallbacks",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toAdd fallbacks\nAdd fallbacks\n\nThere are many possible points of failure in an LLM application, whether that be issues with LLM API’s, poor model outputs, issues with other integrations, etc. Fallbacks help you gracefully handle and isolate these issues.\n\nCrucially, fallbacks can be applied not only on the LLM level but on the whole runnable level.\n\nHandling LLM API Errors​\n\nThis is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things.\n\nIMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing.\n\n%pip install --upgrade --quiet  langchain langchain-openai\n\nfrom langchain_community.chat_models import ChatAnthropic\nfrom langchain_openai import ChatOpenAI\n\n\nFirst, let’s mock out what happens if we hit a RateLimitError from OpenAI\n\nfrom unittest.mock import patch\n\nimport httpx\nfrom openai import RateLimitError\n\nrequest = httpx.Request(\"GET\", \"/\")\nresponse = httpx.Response(200, request=request)\nerror = RateLimitError(\"rate limit\", response=response, body=\"\")\n\n# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc\nopenai_llm = ChatOpenAI(max_retries=0)\nanthropic_llm = ChatAnthropic()\nllm = openai_llm.with_fallbacks([anthropic_llm])\n\n# Let's use just the OpenAI LLm first, to show that we run into an error\nwith patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n    try:\n        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n    except RateLimitError:\n        print(\"Hit error\")\n\nHit error\n\n# Now let's try with fallbacks to Anthropic\nwith patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n    try:\n        print(llm.invoke(\"Why did the chicken cross the road?\"))\n    except RateLimitError:\n        print(\"Hit error\")\n\ncontent=' I don\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\' convention.\\n\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False\n\n\nWe can use our “LLM with Fallbacks” as we would a normal LLM.\n\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You're a nice assistant who always includes a compliment in your response\",\n        ),\n        (\"human\", \"Why did the {animal} cross the road\"),\n    ]\n)\nchain = prompt | llm\nwith patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n    try:\n        print(chain.invoke({\"animal\": \"kangaroo\"}))\n    except RateLimitError:\n        print(\"Hit error\")\n\ncontent=\" I don't actually know why the kangaroo crossed the road, but I'm happy to take a guess! Maybe the kangaroo was trying to get to the other side to find some tasty grass to eat. Or maybe it was trying to get away from a predator or other danger. Kangaroos do need to cross roads and other open areas sometimes as part of their normal activities. Whatever the reason, I'm sure the kangaroo looked both ways before hopping across!\" additional_kwargs={} example=False\n\nSpecifying errors to handle​\n\nWe can also specify the errors to handle if we want to be more specific about when the fallback is invoked:\n\nllm = openai_llm.with_fallbacks(\n    [anthropic_llm], exceptions_to_handle=(KeyboardInterrupt,)\n)\n\nchain = prompt | llm\nwith patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n    try:\n        print(chain.invoke({\"animal\": \"kangaroo\"}))\n    except RateLimitError:\n        print(\"Hit error\")\n\nHit error\n\nFallbacks for Sequences​\n\nWe can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt.\n\n# First let's create a chain with a ChatModel\n# We add in a string output parser here so the outputs between the two are the same type\nfrom langchain_core.output_parsers import StrOutputParser\n\nchat_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You're a nice assistant who always includes a compliment in your response\",\n        ),\n        (\"human\", \"Why did the {animal} cross the road\"),\n    ]\n)\n# Here we're going to use a bad model name to easily create a chain that will error\nchat_model = ChatOpenAI(model_name=\"gpt-fake\")\nbad_chain = chat_prompt | chat_model | StrOutputParser()\n\n# Now lets create a chain with the normal OpenAI model\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\n\nprompt_template = \"\"\"Instructions: You should always include a compliment in your response.\n\nQuestion: Why did the {animal} cross the road?\"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\nllm = OpenAI()\ngood_chain = prompt | llm\n\n# We can now create a final chain which combines the two\nchain = bad_chain.with_fallbacks([good_chain])\nchain.invoke({\"animal\": \"turtle\"})\n\n'\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.'\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nCreate a runnable with the `@chain` decorator\nNext\nStream custom generator functions\nHandling LLM API Errors\nSpecifying errors to handle\nFallbacks for Sequences\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Create a runnable with the `@chain` decorator | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/decorator",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toCreate a runnable with the `@chain` decorator\nCreate a runnable with the `@chain` decorator\n\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping in a RunnableLambda.\n\nThis will have the benefit of improved observability by tracing your chain correctly. Any calls to runnables inside this function will be traced as nested childen.\n\nIt will also allow you to use this as any other runnable, compose it in chain, etc.\n\nLet’s take a look at this in action!\n\n%pip install --upgrade --quiet  langchain langchain-openai\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import chain\nfrom langchain_openai import ChatOpenAI\n\nprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\nprompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n\n@chain\ndef custom_chain(text):\n    prompt_val1 = prompt1.invoke({\"topic\": text})\n    output1 = ChatOpenAI().invoke(prompt_val1)\n    parsed_output1 = StrOutputParser().invoke(output1)\n    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()\n    return chain2.invoke({\"joke\": parsed_output1})\n\n\ncustom_chain is now a runnable, meaning you will need to use invoke\n\ncustom_chain.invoke(\"bears\")\n\n'The subject of this joke is bears.'\n\n\nIf you check out your LangSmith traces, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nConfigure chain internals at runtime\nNext\nAdd fallbacks\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Configure chain internals at runtime | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/configure",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toConfigure chain internals at runtime\nConfigure chain internals at runtime\n\nOftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things. In order to make this experience as easy as possible, we have defined two methods.\n\nFirst, a configurable_fields method. This lets you configure particular fields of a runnable.\n\nSecond, a configurable_alternatives method. With this method, you can list out alternatives for any particular runnable that can be set during runtime.\n\nConfiguration Fields​\nWith LLMs​\n\nWith LLMs we can configure things like temperature\n\n%pip install --upgrade --quiet  langchain langchain-openai\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.runnables import ConfigurableField\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(temperature=0).configurable_fields(\n    temperature=ConfigurableField(\n        id=\"llm_temperature\",\n        name=\"LLM Temperature\",\n        description=\"The temperature of the LLM\",\n    )\n)\n\nmodel.invoke(\"pick a random number\")\n\nAIMessage(content='7')\n\nmodel.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"pick a random number\")\n\nAIMessage(content='34')\n\n\nWe can also do this when its used as part of a chain\n\nprompt = PromptTemplate.from_template(\"Pick a random number above {x}\")\nchain = prompt | model\n\nchain.invoke({\"x\": 0})\n\nAIMessage(content='57')\n\nchain.with_config(configurable={\"llm_temperature\": 0.9}).invoke({\"x\": 0})\n\nAIMessage(content='6')\n\nWith HubRunnables​\n\nThis is useful to allow for switching of prompts\n\nfrom langchain.runnables.hub import HubRunnable\n\nprompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(\n    owner_repo_commit=ConfigurableField(\n        id=\"hub_commit\",\n        name=\"Hub Commit\",\n        description=\"The Hub commit to pull from\",\n    )\n)\n\nprompt.invoke({\"question\": \"foo\", \"context\": \"bar\"})\n\nChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: foo \\nContext: bar \\nAnswer:\")])\n\nprompt.with_config(configurable={\"hub_commit\": \"rlm/rag-prompt-llama\"}).invoke(\n    {\"question\": \"foo\", \"context\": \"bar\"}\n)\n\nChatPromptValue(messages=[HumanMessage(content=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: foo \\nContext: bar \\nAnswer: [/INST]\")])\n\nConfigurable Alternatives​\nWith LLMs​\n\nLet’s take a look at doing this with LLMs\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.chat_models import ChatAnthropic\nfrom langchain_core.runnables import ConfigurableField\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatAnthropic(temperature=0).configurable_alternatives(\n    # This gives this field an id\n    # When configuring the end runnable, we can then use this id to configure this field\n    ConfigurableField(id=\"llm\"),\n    # This sets a default_key.\n    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n    default_key=\"anthropic\",\n    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`\n    openai=ChatOpenAI(),\n    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`\n    gpt4=ChatOpenAI(model=\"gpt-4\"),\n    # You can add more configuration options here\n)\nprompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\nchain = prompt | llm\n\n# By default it will call Anthropic\nchain.invoke({\"topic\": \"bears\"})\n\nAIMessage(content=\" Here's a silly joke about bears:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!\")\n\n# We can use `.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to use\nchain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})\n\nAIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they already have bear feet!\")\n\n# If we use the `default_key` then it uses the default\nchain.with_config(configurable={\"llm\": \"anthropic\"}).invoke({\"topic\": \"bears\"})\n\nAIMessage(content=\" Here's a silly joke about bears:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!\")\n\nWith Prompts​\n\nWe can do a similar thing, but alternate between prompts\n\nllm = ChatAnthropic(temperature=0)\nprompt = PromptTemplate.from_template(\n    \"Tell me a joke about {topic}\"\n).configurable_alternatives(\n    # This gives this field an id\n    # When configuring the end runnable, we can then use this id to configure this field\n    ConfigurableField(id=\"prompt\"),\n    # This sets a default_key.\n    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n    default_key=\"joke\",\n    # This adds a new option, with name `poem`\n    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),\n    # You can add more configuration options here\n)\nchain = prompt | llm\n\n# By default it will write a joke\nchain.invoke({\"topic\": \"bears\"})\n\nAIMessage(content=\" Here's a silly joke about bears:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!\")\n\n# We can configure it write a poem\nchain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"bears\"})\n\nAIMessage(content=' Here is a short poem about bears:\\n\\nThe bears awaken from their sleep\\nAnd lumber out into the deep\\nForests filled with trees so tall\\nForaging for food before nightfall \\nTheir furry coats and claws so sharp\\nSniffing for berries and fish to nab\\nLumbering about without a care\\nThe mighty grizzly and black bear\\nProud creatures, wild and free\\nRuling their domain majestically\\nWandering the woods they call their own\\nBefore returning to their dens alone')\n\nWith Prompts and LLMs​\n\nWe can also have multiple things configurable! Here’s an example doing that with both prompts and LLMs.\n\nllm = ChatAnthropic(temperature=0).configurable_alternatives(\n    # This gives this field an id\n    # When configuring the end runnable, we can then use this id to configure this field\n    ConfigurableField(id=\"llm\"),\n    # This sets a default_key.\n    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n    default_key=\"anthropic\",\n    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`\n    openai=ChatOpenAI(),\n    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`\n    gpt4=ChatOpenAI(model=\"gpt-4\"),\n    # You can add more configuration options here\n)\nprompt = PromptTemplate.from_template(\n    \"Tell me a joke about {topic}\"\n).configurable_alternatives(\n    # This gives this field an id\n    # When configuring the end runnable, we can then use this id to configure this field\n    ConfigurableField(id=\"prompt\"),\n    # This sets a default_key.\n    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n    default_key=\"joke\",\n    # This adds a new option, with name `poem`\n    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),\n    # You can add more configuration options here\n)\nchain = prompt | llm\n\n# We can configure it write a poem with OpenAI\nchain.with_config(configurable={\"prompt\": \"poem\", \"llm\": \"openai\"}).invoke(\n    {\"topic\": \"bears\"}\n)\n\nAIMessage(content=\"In the forest, where tall trees sway,\\nA creature roams, both fierce and gray.\\nWith mighty paws and piercing eyes,\\nThe bear, a symbol of strength, defies.\\n\\nThrough snow-kissed mountains, it does roam,\\nA guardian of its woodland home.\\nWith fur so thick, a shield of might,\\nIt braves the coldest winter night.\\n\\nA gentle giant, yet wild and free,\\nThe bear commands respect, you see.\\nWith every step, it leaves a trace,\\nOf untamed power and ancient grace.\\n\\nFrom honeyed feast to salmon's leap,\\nIt takes its place, in nature's keep.\\nA symbol of untamed delight,\\nThe bear, a wonder, day and night.\\n\\nSo let us honor this noble beast,\\nIn forests where its soul finds peace.\\nFor in its presence, we come to know,\\nThe untamed spirit that in us also flows.\")\n\n# We can always just configure only one if we want\nchain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})\n\nAIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet!\")\n\nSaving configurations​\n\nWe can also easily save configured chains as their own objects\n\nopenai_poem = chain.with_config(configurable={\"llm\": \"openai\"})\n\nopenai_poem.invoke({\"topic\": \"bears\"})\n\nAIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nBind runtime args\nNext\nCreate a runnable with the `@chain` decorator\nConfiguration Fields\nWith LLMs\nWith HubRunnables\nConfigurable Alternatives\nWith LLMs\nWith Prompts\nWith Prompts and LLMs\nSaving configurations\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Bind runtime args | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/binding",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toBind runtime args\nBind runtime args\n\nSometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to easily pass these arguments in.\n\nSuppose we have a simple prompt + model sequence:\n\n%pip install --upgrade --quiet  langchain langchain-openai\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n        ),\n        (\"human\", \"{equation_statement}\"),\n    ]\n)\nmodel = ChatOpenAI(temperature=0)\nrunnable = (\n    {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n)\n\nprint(runnable.invoke(\"x raised to the third plus seven equals 12\"))\n\nEQUATION: x^3 + 7 = 12\n\nSOLUTION:\nSubtracting 7 from both sides of the equation, we get:\nx^3 = 12 - 7\nx^3 = 5\n\nTaking the cube root of both sides, we get:\nx = ∛5\n\nTherefore, the solution to the equation x^3 + 7 = 12 is x = ∛5.\n\n\nand want to call the model with certain stop words:\n\nrunnable = (\n    {\"equation_statement\": RunnablePassthrough()}\n    | prompt\n    | model.bind(stop=\"SOLUTION\")\n    | StrOutputParser()\n)\nprint(runnable.invoke(\"x raised to the third plus seven equals 12\"))\n\nEQUATION: x^3 + 7 = 12\n\n\nAttaching OpenAI functions​\n\nOne particularly useful application of binding is to attach OpenAI functions to a compatible OpenAI model:\n\nfunction = {\n    \"name\": \"solver\",\n    \"description\": \"Formulates and solves an equation\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"equation\": {\n                \"type\": \"string\",\n                \"description\": \"The algebraic expression of the equation\",\n            },\n            \"solution\": {\n                \"type\": \"string\",\n                \"description\": \"The solution to the equation\",\n            },\n        },\n        \"required\": [\"equation\", \"solution\"],\n    },\n}\n\n# Need gpt-4 to solve this one correctly\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"Write out the following equation using algebraic symbols then solve it.\",\n        ),\n        (\"human\", \"{equation_statement}\"),\n    ]\n)\nmodel = ChatOpenAI(model=\"gpt-4\", temperature=0).bind(\n    function_call={\"name\": \"solver\"}, functions=[function]\n)\nrunnable = {\"equation_statement\": RunnablePassthrough()} | prompt | model\nrunnable.invoke(\"x raised to the third plus seven equals 12\")\n\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'solver', 'arguments': '{\\n\"equation\": \"x^3 + 7 = 12\",\\n\"solution\": \"x = ∛5\"\\n}'}}, example=False)\n\nAttaching OpenAI tools​\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools)\nmodel.invoke(\"What's the weather in SF, NYC and LA?\")\n\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_zHN0ZHwrxM7nZDdqTp6dkPko', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_aqdMm9HBSlFW9c9rqxTa7eQv', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_cx8E567zcLzYV2WSWVgO63f1', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]})\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nRunnableBranch: Dynamically route logic based on input\nNext\nConfigure chain internals at runtime\nAttaching OpenAI functions\nAttaching OpenAI tools\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "RunnableBranch: Dynamically route logic based on input | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/routing",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toRunnableBranch: Dynamically route logic based on input\nDynamically route logic based on input\n\nThis notebook covers how to do routing in the LangChain Expression Language.\n\nRouting allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs.\n\nThere are two ways to perform routing:\n\nConditionally return runnables from a RunnableLambda (recommended)\nUsing a RunnableBranch.\n\nWe’ll illustrate both methods using a two step sequence where the first step classifies an input question as being about LangChain, Anthropic, or Other, then routes to a corresponding prompt chain.\n\nExample Setup​\n\nFirst, let’s create a chain that will identify incoming questions as being about LangChain, Anthropic, or Other:\n\nfrom langchain_community.chat_models import ChatAnthropic\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\nchain = (\n    PromptTemplate.from_template(\n        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.\n\nDo not respond with more than one word.\n\n<question>\n{question}\n</question>\n\nClassification:\"\"\"\n    )\n    | ChatAnthropic()\n    | StrOutputParser()\n)\n\nchain.invoke({\"question\": \"how do I call Anthropic?\"})\n\n' Anthropic'\n\n\nNow, let’s create three sub chains:\n\nlangchain_chain = (\n    PromptTemplate.from_template(\n        \"\"\"You are an expert in langchain. \\\nAlways answer questions starting with \"As Harrison Chase told me\". \\\nRespond to the following question:\n\nQuestion: {question}\nAnswer:\"\"\"\n    )\n    | ChatAnthropic()\n)\nanthropic_chain = (\n    PromptTemplate.from_template(\n        \"\"\"You are an expert in anthropic. \\\nAlways answer questions starting with \"As Dario Amodei told me\". \\\nRespond to the following question:\n\nQuestion: {question}\nAnswer:\"\"\"\n    )\n    | ChatAnthropic()\n)\ngeneral_chain = (\n    PromptTemplate.from_template(\n        \"\"\"Respond to the following question:\n\nQuestion: {question}\nAnswer:\"\"\"\n    )\n    | ChatAnthropic()\n)\n\nUsing a custom function (Recommended)​\n\nYou can also use a custom function to route between different outputs. Here’s an example:\n\ndef route(info):\n    if \"anthropic\" in info[\"topic\"].lower():\n        return anthropic_chain\n    elif \"langchain\" in info[\"topic\"].lower():\n        return langchain_chain\n    else:\n        return general_chain\n\nfrom langchain_core.runnables import RunnableLambda\n\nfull_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n    route\n)\n\nfull_chain.invoke({\"question\": \"how do I use Anthropic?\"})\n\nAIMessage(content=' As Dario Amodei told me, to use Anthropic IPC you first need to import it:\\n\\n```python\\nfrom anthroipc import ic\\n```\\n\\nThen you can create a client and connect to the server:\\n\\n```python \\nclient = ic.connect()\\n```\\n\\nAfter that, you can call methods on the client and get responses:\\n\\n```python\\nresponse = client.ask(\"What is the meaning of life?\")\\nprint(response)\\n```\\n\\nYou can also register callbacks to handle events: \\n\\n```python\\ndef on_poke(event):\\n  print(\"Got poked!\")\\n\\nclient.on(\\'poke\\', on_poke)\\n```\\n\\nAnd that\\'s the basics of using the Anthropic IPC client library for Python! Let me know if you have any other questions!', additional_kwargs={}, example=False)\n\nfull_chain.invoke({\"question\": \"how do I use LangChain?\"})\n\nAIMessage(content=' As Harrison Chase told me, to use LangChain you first need to sign up for an API key at platform.langchain.com. Once you have your API key, you can install the Python library and write a simple Python script to call the LangChain API. Here is some sample code to get started:\\n\\n```python\\nimport langchain\\n\\napi_key = \"YOUR_API_KEY\"\\n\\nlangchain.set_key(api_key)\\n\\nresponse = langchain.ask(\"What is the capital of France?\")\\n\\nprint(response.response)\\n```\\n\\nThis will send the question \"What is the capital of France?\" to the LangChain API and print the response. You can customize the request by providing parameters like max_tokens, temperature, etc. The LangChain Python library documentation has more details on the available options. The key things are getting an API key and calling langchain.ask() with your question text. Let me know if you have any other questions!', additional_kwargs={}, example=False)\n\nfull_chain.invoke({\"question\": \"whats 2 + 2\"})\n\nAIMessage(content=' 4', additional_kwargs={}, example=False)\n\nUsing a RunnableBranch​\n\nA RunnableBranch is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does not offer anything that you can’t achieve in a custom function as described above, so we recommend using a custom function instead.\n\nA RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it’s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n\nIf no provided conditions match, it runs the default runnable.\n\nHere’s an example of what it looks like in action:\n\nfrom langchain_core.runnables import RunnableBranch\n\nbranch = RunnableBranch(\n    (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),\n    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n    general_chain,\n)\nfull_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\nfull_chain.invoke({\"question\": \"how do I use Anthropic?\"})\n\nAIMessage(content=\" As Dario Amodei told me, here are some ways to use Anthropic:\\n\\n- Sign up for an account on Anthropic's website to access tools like Claude, Constitutional AI, and Writer. \\n\\n- Use Claude for tasks like email generation, customer service chat, and QA. Claude can understand natural language prompts and provide helpful responses.\\n\\n- Use Constitutional AI if you need an AI assistant that is harmless, honest, and helpful. It is designed to be safe and aligned with human values.\\n\\n- Use Writer to generate natural language content for things like marketing copy, stories, reports, and more. Give it a topic and prompt and it will create high-quality written content.\\n\\n- Check out Anthropic's documentation and blog for tips, tutorials, examples, and announcements about new capabilities as they continue to develop their AI technology.\\n\\n- Follow Anthropic on social media or subscribe to their newsletter to stay up to date on new features and releases.\\n\\n- For most people, the easiest way to leverage Anthropic's technology is through their website - just create an account to get started!\", additional_kwargs={}, example=False)\n\nfull_chain.invoke({\"question\": \"how do I use LangChain?\"})\n\nAIMessage(content=' As Harrison Chase told me, here is how you use LangChain:\\n\\nLangChain is an AI assistant that can have conversations, answer questions, and generate text. To use LangChain, you simply type or speak your input and LangChain will respond. \\n\\nYou can ask LangChain questions, have discussions, get summaries or explanations about topics, and request it to generate text on a subject. Some examples of interactions:\\n\\n- Ask general knowledge questions and LangChain will try to answer factually. For example \"What is the capital of France?\"\\n\\n- Have conversations on topics by taking turns speaking. You can prompt the start of a conversation by saying something like \"Let\\'s discuss machine learning\"\\n\\n- Ask for summaries or high-level explanations on subjects. For example \"Can you summarize the main themes in Shakespeare\\'s Hamlet?\" \\n\\n- Give creative writing prompts or requests to have LangChain generate text in different styles. For example \"Write a short children\\'s story about a mouse\" or \"Generate a poem in the style of Robert Frost about nature\"\\n\\n- Correct LangChain if it makes an inaccurate statement and provide the right information. This helps train it.\\n\\nThe key is interacting naturally and giving it clear prompts and requests', additional_kwargs={}, example=False)\n\nfull_chain.invoke({\"question\": \"whats 2 + 2\"})\n\nAIMessage(content=' 2 + 2 = 4', additional_kwargs={}, example=False)\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nRunnableLambda: Run Custom Functions\nNext\nBind runtime args\nExample Setup\nUsing a custom function (Recommended)\nUsing a RunnableBranch\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "RunnableLambda: Run Custom Functions | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/functions",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toRunnableLambda: Run Custom Functions\nRun custom functions\n\nYou can use arbitrary functions in the pipeline.\n\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single input and unpacks it into multiple argument.\n\n%pip install –upgrade –quiet langchain langchain-openai\n\nfrom operator import itemgetter\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_openai import ChatOpenAI\n\n\ndef length_function(text):\n    return len(text)\n\n\ndef _multiple_length_function(text1, text2):\n    return len(text1) * len(text2)\n\n\ndef multiple_length_function(_dict):\n    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n\n\nprompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\nmodel = ChatOpenAI()\n\nchain1 = prompt | model\n\nchain = (\n    {\n        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n        | RunnableLambda(multiple_length_function),\n    }\n    | prompt\n    | model\n)\n\nchain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})\n\nAIMessage(content='3 + 9 equals 12.')\n\nAccepting a Runnable Config​\n\nRunnable lambdas can optionally accept a RunnableConfig, which they can use to pass callbacks, tags, and other configuration information to nested runs.\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableConfig\n\nimport json\n\n\ndef parse_or_fix(text: str, config: RunnableConfig):\n    fixing_chain = (\n        ChatPromptTemplate.from_template(\n            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n            \" Don't narrate, just respond with the fixed data.\"\n        )\n        | ChatOpenAI()\n        | StrOutputParser()\n    )\n    for _ in range(3):\n        try:\n            return json.loads(text)\n        except Exception as e:\n            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n    return \"Failed to parse\"\n\nfrom langchain.callbacks import get_openai_callback\n\nwith get_openai_callback() as cb:\n    output = RunnableLambda(parse_or_fix).invoke(\n        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n    )\n    print(output)\n    print(cb)\n\n{'foo': 'bar'}\nTokens Used: 65\n    Prompt Tokens: 56\n    Completion Tokens: 9\nSuccessful Requests: 1\nTotal Cost (USD): $0.00010200000000000001\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nRunnablePassthrough: Passing data through\nNext\nRunnableBranch: Dynamically route logic based on input\nAccepting a Runnable Config\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "RunnablePassthrough: Passing data through | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/passthrough",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toRunnablePassthrough: Passing data through\nPassing data through\n\nRunnablePassthrough allows to pass inputs unchanged or with the addition of extra keys. This typically is used in conjuction with RunnableParallel to assign data to a new key in the map.\n\nRunnablePassthrough() called on it’s own, will simply take the input and pass it through.\n\nRunnablePassthrough called with assign (RunnablePassthrough.assign(...)) will take the input, and will add the extra arguments passed to the assign function.\n\nSee the example below:\n\n%pip install --upgrade --quiet  langchain langchain-openai\n\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\n\nrunnable = RunnableParallel(\n    passed=RunnablePassthrough(),\n    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n    modified=lambda x: x[\"num\"] + 1,\n)\n\nrunnable.invoke({\"num\": 1})\n\n{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}\n\n\nAs seen above, passed key was called with RunnablePassthrough() and so it simply passed on {'num': 1}.\n\nIn the second line, we used RunnablePastshrough.assign with a lambda that multiplies the numerical value by 3. In this cased, extra was set with {'num': 1, 'mult': 3} which is the original value with the mult key added.\n\nFinally, we also set a third key in the map with modified which uses a lambda to set a single value adding 1 to the num, which resulted in modified key with the value of 2.\n\nRetrieval Example​\n\nIn the example below, we see a use case where we use RunnablePassthrough along with RunnableMap.\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nmodel = ChatOpenAI()\n\nretrieval_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nretrieval_chain.invoke(\"where did harrison work?\")\n\n'Harrison worked at Kensho.'\n\n\nHere the input to prompt is expected to be a map with keys “context” and “question”. The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the “question” key. In this case, the RunnablePassthrough allows us to pass on the user’s question to the prompt and model.\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nRunnableParallel: Manipulating data\nNext\nRunnableLambda: Run Custom Functions\nRetrieval Example\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "RunnableParallel: Manipulating data | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/map",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow toRunnableParallel: Manipulating data\nManipulating inputs & output\n\nRunnableParallel can be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.\n\nHere the input to prompt is expected to be a map with keys “context” and “question”. The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the “question” key.\n\n%pip install --upgrade --quiet  langchain langchain-openai\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nmodel = ChatOpenAI()\n\nretrieval_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nretrieval_chain.invoke(\"where did harrison work?\")\n\n'Harrison worked at Kensho.'\n\nTIP\n\nNote that when composing a RunnableParallel with another Runnable we don’t even need to wrap our dictionary in the RunnableParallel class — the type conversion is handled for us. In the context of a chain, these are equivalent:\n\n{\"context\": retriever, \"question\": RunnablePassthrough()}\n\nRunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n\nRunnableParallel(context=retriever, question=RunnablePassthrough())\n\nUsing itemgetter as shorthand​\n\nNote that you can use Python’s itemgetter as shorthand to extract data from the map when combining with RunnableParallel. You can find more information about itemgetter in the Python Documentation.\n\nIn the example below, we use itemgetter to extract specific keys from the map:\n\nfrom operator import itemgetter\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\nAnswer in the following language: {language}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nchain = (\n    {\n        \"context\": itemgetter(\"question\") | retriever,\n        \"question\": itemgetter(\"question\"),\n        \"language\": itemgetter(\"language\"),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nchain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\n\n'Harrison ha lavorato a Kensho.'\n\nParallelize steps​\n\nRunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableParallel\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI()\njoke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\npoem_chain = (\n    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n)\n\nmap_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n\nmap_chain.invoke({\"topic\": \"bear\"})\n\n{'joke': AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"),\n 'poem': AIMessage(content=\"In the wild's embrace, bear roams free,\\nStrength and grace, a majestic decree.\")}\n\nParallelism​\n\nRunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two.\n\n%%timeit\n\njoke_chain.invoke({\"topic\": \"bear\"})\n\n958 ms ± 402 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n%%timeit\n\npoem_chain.invoke({\"topic\": \"bear\"})\n\n1.22 s ± 508 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n%%timeit\n\nmap_chain.invoke({\"topic\": \"bear\"})\n\n1.15 s ± 119 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nHow to\nNext\nRunnablePassthrough: Passing data through\nUsing itemgetter as shorthand\nParallelize steps\nParallelism\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "RAG | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/retrieval",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbookRAG\nRAG\n\nLet’s look at adding in a retrieval step to a prompt and LLM, which adds up to a “retrieval-augmented generation” chain\n\n%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken\n\nfrom operator import itemgetter\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nmodel = ChatOpenAI()\n\nchain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nchain.invoke(\"where did harrison work?\")\n\n'Harrison worked at Kensho.'\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\nAnswer in the following language: {language}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nchain = (\n    {\n        \"context\": itemgetter(\"question\") | retriever,\n        \"question\": itemgetter(\"question\"),\n        \"language\": itemgetter(\"language\"),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nchain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\n\n'Harrison ha lavorato a Kensho.'\n\nConversational Retrieval Chain​\n\nWe can easily add in conversation history. This primarily means adding in chat_message_history\n\nfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\nfrom langchain_core.prompts import format_document\nfrom langchain_core.runnables import RunnableParallel\n\nfrom langchain.prompts.prompt import PromptTemplate\n\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:\"\"\"\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n\n\ndef _combine_documents(\n    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n):\n    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n    return document_separator.join(doc_strings)\n\n_inputs = RunnableParallel(\n    standalone_question=RunnablePassthrough.assign(\n        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n    )\n    | CONDENSE_QUESTION_PROMPT\n    | ChatOpenAI(temperature=0)\n    | StrOutputParser(),\n)\n_context = {\n    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n    \"question\": lambda x: x[\"standalone_question\"],\n}\nconversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\n\nconversational_qa_chain.invoke(\n    {\n        \"question\": \"where did harrison work?\",\n        \"chat_history\": [],\n    }\n)\n\nAIMessage(content='Harrison was employed at Kensho.')\n\nconversational_qa_chain.invoke(\n    {\n        \"question\": \"where did he work?\",\n        \"chat_history\": [\n            HumanMessage(content=\"Who wrote this notebook?\"),\n            AIMessage(content=\"Harrison\"),\n        ],\n    }\n)\n\nAIMessage(content='Harrison worked at Kensho.')\n\nWith Memory and returning source documents​\n\nThis shows how to use memory with the above. For memory, we need to manage that outside at the memory. For returning the retrieved documents, we just need to pass them through all the way.\n\nfrom operator import itemgetter\n\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(\n    return_messages=True, output_key=\"answer\", input_key=\"question\"\n)\n\n# First we add a step to load memory\n# This adds a \"memory\" key to the input object\nloaded_memory = RunnablePassthrough.assign(\n    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n)\n# Now we calculate the standalone question\nstandalone_question = {\n    \"standalone_question\": {\n        \"question\": lambda x: x[\"question\"],\n        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n    }\n    | CONDENSE_QUESTION_PROMPT\n    | ChatOpenAI(temperature=0)\n    | StrOutputParser(),\n}\n# Now we retrieve the documents\nretrieved_documents = {\n    \"docs\": itemgetter(\"standalone_question\") | retriever,\n    \"question\": lambda x: x[\"standalone_question\"],\n}\n# Now we construct the inputs for the final prompt\nfinal_inputs = {\n    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n    \"question\": itemgetter(\"question\"),\n}\n# And finally, we do the part that returns the answers\nanswer = {\n    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n    \"docs\": itemgetter(\"docs\"),\n}\n# And now we put it all together!\nfinal_chain = loaded_memory | standalone_question | retrieved_documents | answer\n\ninputs = {\"question\": \"where did harrison work?\"}\nresult = final_chain.invoke(inputs)\nresult\n\n{'answer': AIMessage(content='Harrison was employed at Kensho.'),\n 'docs': [Document(page_content='harrison worked at kensho')]}\n\n# Note that the memory does not save automatically\n# This will be improved in the future\n# For now you need to save it yourself\nmemory.save_context(inputs, {\"answer\": result[\"answer\"].content})\n\nmemory.load_memory_variables({})\n\n{'history': [HumanMessage(content='where did harrison work?'),\n  AIMessage(content='Harrison was employed at Kensho.')]}\n\ninputs = {\"question\": \"but where did he really work?\"}\nresult = final_chain.invoke(inputs)\nresult\n\n{'answer': AIMessage(content='Harrison actually worked at Kensho.'),\n 'docs': [Document(page_content='harrison worked at kensho')]}\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nPrompt + LLM\nNext\nMultiple chains\nConversational Retrieval Chain\nWith Memory and returning source documents\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Cookbook | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/cookbook/",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nPrompt + LLM\nRAG\nMultiple chains\nQuerying a SQL DB\nAgents\nCode writing\nRouting by semantic similarity\nAdding memory\nAdding moderation\nManaging prompt size\nUsing tools\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageCookbook\nCookbook\n\nExample code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.\n\n📄️ Prompt + LLM\n\nThe most common and valuable composition is taking:\n\n📄️ RAG\n\nLet’s look at adding in a retrieval step to a prompt and LLM, which adds\n\n📄️ Multiple chains\n\nRunnables can easily be used to string together multiple Chains\n\n📄️ Querying a SQL DB\n\nWe can replicate our SQLDatabaseChain with Runnables.\n\n📄️ Agents\n\nYou can pass a Runnable into an agent. Make sure you have langchainhub\n\n📄️ Code writing\n\nExample of how to use LCEL to write Python code.\n\n📄️ Routing by semantic similarity\n\nWith LCEL you can easily add [custom routing\n\n📄️ Adding memory\n\nThis shows how to add memory to an arbitrary chain. Right now, you can\n\n📄️ Adding moderation\n\nThis shows how to add in moderation (or other safeguards) around your\n\n📄️ Managing prompt size\n\nAgents dynamically call tools. The results of those tool calls are added\n\n📄️ Using tools\n\nYou can use any Tools with Runnables easily.\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nAdd message history (memory)\nNext\nPrompt + LLM\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "How to | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/how_to/",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nRunnableParallel: Manipulating data\nRunnablePassthrough: Passing data through\nRunnableLambda: Run Custom Functions\nRunnableBranch: Dynamically route logic based on input\nBind runtime args\nConfigure chain internals at runtime\nCreate a runnable with the `@chain` decorator\nAdd fallbacks\nStream custom generator functions\nInspect your runnables\nAdd message history (memory)\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageHow to\nHow to\n📄️ RunnableParallel: Manipulating data\n\nmanipulating-inputs-output}\n\n📄️ RunnablePassthrough: Passing data through\n\npassing-data-through}\n\n📄️ RunnableLambda: Run Custom Functions\n\nrun-custom-functions}\n\n📄️ RunnableBranch: Dynamically route logic based on input\n\ndynamically-route-logic-based-on-input}\n\n📄️ Bind runtime args\n\nSometimes we want to invoke a Runnable within a Runnable sequence with\n\n📄️ Configure chain internals at runtime\n\nOftentimes you may want to experiment with, or even expose to the end\n\n📄️ Create a runnable with the `@chain` decorator\n\nYou can also turn an arbitrary function into a chain by adding a\n\n📄️ Add fallbacks\n\nThere are many possible points of failure in an LLM application, whether\n\n📄️ Stream custom generator functions\n\nYou can use generator functions (ie. functions that use the yield\n\n📄️ Inspect your runnables\n\nOnce you create a runnable with LCEL, you may often want to inspect it\n\n📄️ Add message history (memory)\n\nThe RunnableWithMessageHistory lets us add message history to certain\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nStreaming\nNext\nRunnableParallel: Manipulating data\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Interface | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/interface",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageInterface\nInterface\n\nTo make it as easy as possible to create custom chains, we’ve implemented a “Runnable” protocol. The Runnable protocol is implemented for most components. This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way. The standard interface includes:\n\nstream: stream back chunks of the response\ninvoke: call the chain on an input\nbatch: call the chain on a list of inputs\n\nThese also have corresponding async methods:\n\nastream: stream back chunks of the response async\nainvoke: call the chain on an input async\nabatch: call the chain on a list of inputs async\nastream_log: stream back intermediate steps as they happen, in addition to the final response\nastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)\n\nThe input type and output type varies by component:\n\nComponent\tInput Type\tOutput Type\nPrompt\tDictionary\tPromptValue\nChatModel\tSingle string, list of chat messages or a PromptValue\tChatMessage\nLLM\tSingle string, list of chat messages or a PromptValue\tString\nOutputParser\tThe output of an LLM or ChatModel\tDepends on the parser\nRetriever\tSingle string\tList of Documents\nTool\tSingle string or dictionary, depending on the tool\tDepends on the tool\n\nAll runnables expose input and output schemas to inspect the inputs and outputs: - input_schema: an input Pydantic model auto-generated from the structure of the Runnable - output_schema: an output Pydantic model auto-generated from the structure of the Runnable\n\nLet’s take a look at these methods. To do so, we’ll create a super simple PromptTemplate + ChatModel chain.\n\n%pip install –upgrade –quiet langchain-core langchain-community langchain-openai\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI()\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\nchain = prompt | model\n\nInput Schema​\n\nA description of the inputs accepted by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation.\n\n# The input schema of the chain is the input schema of its first part, the prompt.\nchain.input_schema.schema()\n\n{'title': 'PromptInput',\n 'type': 'object',\n 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}\n\nprompt.input_schema.schema()\n\n{'title': 'PromptInput',\n 'type': 'object',\n 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}\n\nmodel.input_schema.schema()\n\n{'title': 'ChatOpenAIInput',\n 'anyOf': [{'type': 'string'},\n  {'$ref': '#/definitions/StringPromptValue'},\n  {'$ref': '#/definitions/ChatPromptValueConcrete'},\n  {'type': 'array',\n   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n     {'$ref': '#/definitions/HumanMessage'},\n     {'$ref': '#/definitions/ChatMessage'},\n     {'$ref': '#/definitions/SystemMessage'},\n     {'$ref': '#/definitions/FunctionMessage'},\n     {'$ref': '#/definitions/ToolMessage'}]}}],\n 'definitions': {'StringPromptValue': {'title': 'StringPromptValue',\n   'description': 'String prompt value.',\n   'type': 'object',\n   'properties': {'text': {'title': 'Text', 'type': 'string'},\n    'type': {'title': 'Type',\n     'default': 'StringPromptValue',\n     'enum': ['StringPromptValue'],\n     'type': 'string'}},\n   'required': ['text']},\n  'AIMessage': {'title': 'AIMessage',\n   'description': 'A Message from an AI.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'ai',\n     'enum': ['ai'],\n     'type': 'string'},\n    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n   'required': ['content']},\n  'HumanMessage': {'title': 'HumanMessage',\n   'description': 'A Message from a human.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'human',\n     'enum': ['human'],\n     'type': 'string'},\n    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n   'required': ['content']},\n  'ChatMessage': {'title': 'ChatMessage',\n   'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'chat',\n     'enum': ['chat'],\n     'type': 'string'},\n    'role': {'title': 'Role', 'type': 'string'}},\n   'required': ['content', 'role']},\n  'SystemMessage': {'title': 'SystemMessage',\n   'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'system',\n     'enum': ['system'],\n     'type': 'string'}},\n   'required': ['content']},\n  'FunctionMessage': {'title': 'FunctionMessage',\n   'description': 'A Message for passing the result of executing a function back to a model.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'function',\n     'enum': ['function'],\n     'type': 'string'},\n    'name': {'title': 'Name', 'type': 'string'}},\n   'required': ['content', 'name']},\n  'ToolMessage': {'title': 'ToolMessage',\n   'description': 'A Message for passing the result of executing a tool back to a model.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'tool',\n     'enum': ['tool'],\n     'type': 'string'},\n    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n   'required': ['content', 'tool_call_id']},\n  'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',\n   'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n   'type': 'object',\n   'properties': {'messages': {'title': 'Messages',\n     'type': 'array',\n     'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n       {'$ref': '#/definitions/HumanMessage'},\n       {'$ref': '#/definitions/ChatMessage'},\n       {'$ref': '#/definitions/SystemMessage'},\n       {'$ref': '#/definitions/FunctionMessage'},\n       {'$ref': '#/definitions/ToolMessage'}]}},\n    'type': {'title': 'Type',\n     'default': 'ChatPromptValueConcrete',\n     'enum': ['ChatPromptValueConcrete'],\n     'type': 'string'}},\n   'required': ['messages']}}}\n\nOutput Schema​\n\nA description of the outputs produced by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation.\n\n# The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage\nchain.output_schema.schema()\n\n{'title': 'ChatOpenAIOutput',\n 'anyOf': [{'$ref': '#/definitions/AIMessage'},\n  {'$ref': '#/definitions/HumanMessage'},\n  {'$ref': '#/definitions/ChatMessage'},\n  {'$ref': '#/definitions/SystemMessage'},\n  {'$ref': '#/definitions/FunctionMessage'},\n  {'$ref': '#/definitions/ToolMessage'}],\n 'definitions': {'AIMessage': {'title': 'AIMessage',\n   'description': 'A Message from an AI.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'ai',\n     'enum': ['ai'],\n     'type': 'string'},\n    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n   'required': ['content']},\n  'HumanMessage': {'title': 'HumanMessage',\n   'description': 'A Message from a human.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'human',\n     'enum': ['human'],\n     'type': 'string'},\n    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n   'required': ['content']},\n  'ChatMessage': {'title': 'ChatMessage',\n   'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'chat',\n     'enum': ['chat'],\n     'type': 'string'},\n    'role': {'title': 'Role', 'type': 'string'}},\n   'required': ['content', 'role']},\n  'SystemMessage': {'title': 'SystemMessage',\n   'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'system',\n     'enum': ['system'],\n     'type': 'string'}},\n   'required': ['content']},\n  'FunctionMessage': {'title': 'FunctionMessage',\n   'description': 'A Message for passing the result of executing a function back to a model.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'function',\n     'enum': ['function'],\n     'type': 'string'},\n    'name': {'title': 'Name', 'type': 'string'}},\n   'required': ['content', 'name']},\n  'ToolMessage': {'title': 'ToolMessage',\n   'description': 'A Message for passing the result of executing a tool back to a model.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content',\n     'anyOf': [{'type': 'string'},\n      {'type': 'array',\n       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'tool',\n     'enum': ['tool'],\n     'type': 'string'},\n    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n   'required': ['content', 'tool_call_id']}}}\n\nStream​\nfor s in chain.stream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n\nSure, here's a bear-themed joke for you:\n\nWhy don't bears wear shoes?\n\nBecause they already have bear feet!\n\nInvoke​\nchain.invoke({\"topic\": \"bears\"})\n\nAIMessage(content=\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\")\n\nBatch​\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n\n[AIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they already have bear feet!\"),\n AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")]\n\n\nYou can set the number of concurrent requests by using the max_concurrency parameter\n\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})\n\n[AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"),\n AIMessage(content=\"Why don't cats play poker in the wild? Too many cheetahs!\")]\n\nAsync Stream​\nasync for s in chain.astream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n\nWhy don't bears wear shoes?\n\nBecause they have bear feet!\n\nAsync Invoke​\nawait chain.ainvoke({\"topic\": \"bears\"})\n\nAIMessage(content=\"Why don't bears ever wear shoes?\\n\\nBecause they already have bear feet!\")\n\nAsync Batch​\nawait chain.abatch([{\"topic\": \"bears\"}])\n\n[AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")]\n\nAsync Stream Events (beta)​\n\nEvent Streaming is a beta API, and may change a bit based on feedback.\n\nNote: Introduced in langchain-core 0.2.0\n\nFor now, when using the astream_events API, for everything to work properly please:\n\nUse async throughout the code (including async tools etc)\nPropagate callbacks if defining custom functions / runnables.\nWhenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens.\nEvent Reference​\n\nHere is a reference table that shows some events that might be emitted by the various Runnable objects. Definitions for some of the Runnable are included after the table.\n\n⚠️ When streaming the inputs for the runnable will not be available until the input stream has been entirely consumed This means that the inputs will be available at for the corresponding end hook rather than start event.\n\nevent\tname\tchunk\tinput\toutput\non_chat_model_start\t[model name]\t\t{“messages”: [[SystemMessage, HumanMessage]]}\t\non_chat_model_stream\t[model name]\tAIMessageChunk(content=“hello”)\t\t\non_chat_model_end\t[model name]\t\t{“messages”: [[SystemMessage, HumanMessage]]}\t{“generations”: […], “llm_output”: None, …}\non_llm_start\t[model name]\t\t{‘input’: ‘hello’}\t\non_llm_stream\t[model name]\t‘Hello’\t\t\non_llm_end\t[model name]\t\t‘Hello human!’\t\non_chain_start\tformat_docs\t\t\t\non_chain_stream\tformat_docs\t“hello world!, goodbye world!”\t\t\non_chain_end\tformat_docs\t\t[Document(…)]\t“hello world!, goodbye world!”\non_tool_start\tsome_tool\t\t{“x”: 1, “y”: “2”}\t\non_tool_stream\tsome_tool\t{“x”: 1, “y”: “2”}\t\t\non_tool_end\tsome_tool\t\t\t{“x”: 1, “y”: “2”}\non_retriever_start\t[retriever name]\t\t{“query”: “hello”}\t\non_retriever_chunk\t[retriever name]\t{documents: […]}\t\t\non_retriever_end\t[retriever name]\t\t{“query”: “hello”}\t{documents: […]}\non_prompt_start\t[template_name]\t\t{“question”: “hello”}\t\non_prompt_end\t[template_name]\t\t{“question”: “hello”}\tChatPromptValue(messages: [SystemMessage, …])\n\nHere are declarations associated with the events shown above:\n\nformat_docs:\n\ndef format_docs(docs: List[Document]) -> str:\n    '''Format the docs.'''\n    return \", \".join([doc.page_content for doc in docs])\n\nformat_docs = RunnableLambda(format_docs)\n\n\nsome_tool:\n\n@tool\ndef some_tool(x: int, y: str) -> dict:\n    '''Some_tool.'''\n    return {\"x\": x, \"y\": y}\n\n\nprompt:\n\ntemplate = ChatPromptTemplate.from_messages(\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n\n\nLet’s define a new chain to make it more interesting to show off the astream_events interface (and later the astream_log interface).\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import OpenAIEmbeddings\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n\nretrieval_chain = (\n    {\n        \"context\": retriever.with_config(run_name=\"Docs\"),\n        \"question\": RunnablePassthrough(),\n    }\n    | prompt\n    | model.with_config(run_name=\"my_llm\")\n    | StrOutputParser()\n)\n\n\nNow let’s use astream_events to get events from the retriever and the LLM.\n\nasync for event in retrieval_chain.astream_events(\n    \"where did harrison work?\", version=\"v1\", include_names=[\"Docs\", \"my_llm\"]\n):\n    kind = event[\"event\"]\n    if kind == \"on_chat_model_stream\":\n        print(event[\"data\"][\"chunk\"].content, end=\"|\")\n    elif kind in {\"on_chat_model_start\"}:\n        print()\n        print(\"Streaming LLM:\")\n    elif kind in {\"on_chat_model_end\"}:\n        print()\n        print(\"Done streaming LLM.\")\n    elif kind == \"on_retriever_end\":\n        print(\"--\")\n        print(\"Retrieved the following documents:\")\n        print(event[\"data\"][\"output\"][\"documents\"])\n    elif kind == \"on_tool_end\":\n        print(f\"Ended tool: {event['name']}\")\n    else:\n        pass\n\n/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.\n  warn_beta(\n\n--\nRetrieved the following documents:\n[Document(page_content='harrison worked at kensho')]\n\nStreaming LLM:\n|H|arrison| worked| at| Kens|ho|.||\nDone streaming LLM.\n\nAsync Stream Intermediate Steps​\n\nAll runnables also have a method .astream_log() which is used to stream (as they happen) all or part of the intermediate steps of your chain/sequence.\n\nThis is useful to show progress to the user, to use intermediate results, or to debug your chain.\n\nYou can stream all steps (default) or include/exclude steps by name, tags or metadata.\n\nThis method yields JSONPatch ops that when applied in the same order as received build up the RunState.\n\nclass LogEntry(TypedDict):\n    id: str\n    \"\"\"ID of the sub-run.\"\"\"\n    name: str\n    \"\"\"Name of the object being run.\"\"\"\n    type: str\n    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"\n    tags: List[str]\n    \"\"\"List of tags for the run.\"\"\"\n    metadata: Dict[str, Any]\n    \"\"\"Key-value pairs of metadata for the run.\"\"\"\n    start_time: str\n    \"\"\"ISO-8601 timestamp of when the run started.\"\"\"\n\n    streamed_output_str: List[str]\n    \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"\n    final_output: Optional[Any]\n    \"\"\"Final output of this run.\n    Only available after the run has finished successfully.\"\"\"\n    end_time: Optional[str]\n    \"\"\"ISO-8601 timestamp of when the run ended.\n    Only available after the run has finished.\"\"\"\n\n\nclass RunState(TypedDict):\n    id: str\n    \"\"\"ID of the run.\"\"\"\n    streamed_output: List[Any]\n    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"\n    final_output: Optional[Any]\n    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.\n    Only available after the run has finished successfully.\"\"\"\n\n    logs: Dict[str, LogEntry]\n    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will\n    contain only the runs that matched the filters.\"\"\"\n\nStreaming JSONPatch chunks​\n\nThis is useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops on the client to rebuild the run state there. See LangServe for tooling to make it easier to build a webserver from any Runnable.\n\nasync for chunk in retrieval_chain.astream_log(\n    \"where did harrison work?\", include_names=[\"Docs\"]\n):\n    print(\"-\" * 40)\n    print(chunk)\n\n----------------------------------------\nRunLogPatch({'op': 'replace',\n  'path': '',\n  'value': {'final_output': None,\n            'id': '82e9b4b1-3dd6-4732-8db9-90e79c4da48c',\n            'logs': {},\n            'name': 'RunnableSequence',\n            'streamed_output': [],\n            'type': 'chain'}})\n----------------------------------------\nRunLogPatch({'op': 'add',\n  'path': '/logs/Docs',\n  'value': {'end_time': None,\n            'final_output': None,\n            'id': '9206e94a-57bd-48ee-8c5e-fdd1c52a6da2',\n            'metadata': {},\n            'name': 'Docs',\n            'start_time': '2024-01-19T22:33:55.902+00:00',\n            'streamed_output': [],\n            'streamed_output_str': [],\n            'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n            'type': 'retriever'}})\n----------------------------------------\nRunLogPatch({'op': 'add',\n  'path': '/logs/Docs/final_output',\n  'value': {'documents': [Document(page_content='harrison worked at kensho')]}},\n {'op': 'add',\n  'path': '/logs/Docs/end_time',\n  'value': '2024-01-19T22:33:56.064+00:00'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''},\n {'op': 'replace', 'path': '/final_output', 'value': ''})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'H'},\n {'op': 'replace', 'path': '/final_output', 'value': 'H'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'arrison'},\n {'op': 'replace', 'path': '/final_output', 'value': 'Harrison'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' worked'},\n {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' at'},\n {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Kens'},\n {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at Kens'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ho'},\n {'op': 'replace',\n  'path': '/final_output',\n  'value': 'Harrison worked at Kensho'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n {'op': 'replace',\n  'path': '/final_output',\n  'value': 'Harrison worked at Kensho.'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})\n\nStreaming the incremental RunState​\n\nYou can simply pass diff=False to get incremental values of RunState. You get more verbose output with more repetitive parts.\n\nasync for chunk in retrieval_chain.astream_log(\n    \"where did harrison work?\", include_names=[\"Docs\"], diff=False\n):\n    print(\"-\" * 70)\n    print(chunk)\n\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {},\n 'name': 'RunnableSequence',\n 'streamed_output': [],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': None,\n                   'final_output': None,\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': [],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': [],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': '',\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': [''],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': 'H',\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': ['', 'H'],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': 'Harrison',\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': ['', 'H', 'arrison'],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': 'Harrison worked',\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': ['', 'H', 'arrison', ' worked'],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': 'Harrison worked at',\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': ['', 'H', 'arrison', ' worked', ' at'],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': 'Harrison worked at Kens',\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens'],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': 'Harrison worked at Kensho',\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho'],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': 'Harrison worked at Kensho.',\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '.'],\n 'type': 'chain'})\n----------------------------------------------------------------------\nRunLog({'final_output': 'Harrison worked at Kensho.',\n 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172',\n 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2024-01-19T22:33:56.939+00:00',\n                   'streamed_output': [],\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n                   'type': 'retriever'}},\n 'name': 'RunnableSequence',\n 'streamed_output': ['',\n                     'H',\n                     'arrison',\n                     ' worked',\n                     ' at',\n                     ' Kens',\n                     'ho',\n                     '.',\n                     ''],\n 'type': 'chain'})\n\nParallelism​\n\nLet’s take a look at how LangChain Expression Language supports parallel requests. For example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel.\n\nfrom langchain_core.runnables import RunnableParallel\n\nchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\nchain2 = (\n    ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")\n    | model\n)\ncombined = RunnableParallel(joke=chain1, poem=chain2)\n\n%%time\nchain1.invoke({\"topic\": \"bears\"})\n\nCPU times: user 18 ms, sys: 1.27 ms, total: 19.3 ms\nWall time: 692 ms\n\nAIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\")\n\n%%time\nchain2.invoke({\"topic\": \"bears\"})\n\nCPU times: user 10.5 ms, sys: 166 µs, total: 10.7 ms\nWall time: 579 ms\n\nAIMessage(content=\"In forest's embrace,\\nMajestic bears pace.\")\n\n%%time\ncombined.invoke({\"topic\": \"bears\"})\n\nCPU times: user 32 ms, sys: 2.59 ms, total: 34.6 ms\nWall time: 816 ms\n\n{'joke': AIMessage(content=\"Sure, here's a bear-related joke for you:\\n\\nWhy did the bear bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house!\"),\n 'poem': AIMessage(content=\"In wilderness they roam,\\nMajestic strength, nature's throne.\")}\n\nParallelism on batches​\n\nParallelism can be combined with other runnables. Let’s try to use parallelism with batches.\n\n%%time\nchain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n\nCPU times: user 17.3 ms, sys: 4.84 ms, total: 22.2 ms\nWall time: 628 ms\n\n[AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"),\n AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")]\n\n%%time\nchain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n\nCPU times: user 15.8 ms, sys: 3.83 ms, total: 19.7 ms\nWall time: 718 ms\n\n[AIMessage(content='In the wild, bears roam,\\nMajestic guardians of ancient home.'),\n AIMessage(content='Whiskers grace, eyes gleam,\\nCats dance through the moonbeam.')]\n\n%%time\ncombined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n\nCPU times: user 44.8 ms, sys: 3.17 ms, total: 48 ms\nWall time: 721 ms\n\n[{'joke': AIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet!\"),\n  'poem': AIMessage(content=\"Majestic bears roam,\\nNature's strength, beauty shown.\")},\n {'joke': AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\"),\n  'poem': AIMessage(content=\"Whiskers dance, eyes aglow,\\nCats embrace the night's gentle flow.\")}]\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nWhy use LCEL\nNext\nStreaming\nInput Schema\nOutput Schema\nStream\nInvoke\nBatch\nAsync Stream\nAsync Invoke\nAsync Batch\nAsync Stream Events (beta)\nEvent Reference\nAsync Stream Intermediate Steps\nStreaming JSONPatch chunks\nStreaming the incremental RunState\nParallelism\nParallelism on batches\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Streaming | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/streaming",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageStreaming\nStreaming With LangChain\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface.\n\nThis interface provides two general approaches to stream content:\n\nsync stream and async astream: a default implementation of streaming that streams the final output from the chain.\nasync astream_events and async astream_log: these provide a way to stream both intermediate steps and final output from the chain.\n\nLet’s take a look at both approaches, and try to understand a how to use them. 🥷\n\nUsing Stream​\n\nAll Runnable objects implement a sync method called stream and an async variant called astream.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an input stream; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps– the LLMs themselves!\n\nLLMs and Chat Models​\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps. 🙊\n\nLarge language models can take several seconds to generate a complete response to a query. This is far slower than the ~200-300 ms threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; e.g., to stream the output from the model token by token.\n\n# Showing the example using anthropic, but you can use\n# your favorite chat model!\nfrom langchain_community.chat_models import ChatAnthropic\n\nmodel = ChatAnthropic()\n\nchunks = []\nasync for chunk in model.astream(\"hello. tell me something about yourself\"):\n    chunks.append(chunk)\n    print(chunk.content, end=\"|\", flush=True)\n\n Hello|!| My| name| is| Claude|.| I|'m| an| AI| assistant| created| by| An|throp|ic| to| be| helpful|,| harmless|,| and| honest|.||\n\n\nLet’s inspect one of the chunks\n\nchunks[0]\n\nAIMessageChunk(content=' Hello')\n\n\nWe got back something called an AIMessageChunk. This chunk represents a part of an AIMessage.\n\nMessage chunks are additive by design – one can simply add them up to get the state of the response so far!\n\nchunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]\n\nAIMessageChunk(content=' Hello! My name is')\n\nChains​\n\nVirtually all LLM applications involve more steps than just a call to a language model.\n\nLet’s build a simple chain using LangChain Expression Language (LCEL) that combines a prompt, model and a parser and verify that streaming works.\n\nWe will use StrOutputParser to parse the output from the model. This is a simple parser that extracts the content field from an AIMessageChunk, giving us the token returned by the model.\n\nTIP\n\nLCEL is a declarative way to specify a “program” by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of stream, and astream allowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\nparser = StrOutputParser()\nchain = prompt | model | parser\n\nasync for chunk in chain.astream({\"topic\": \"parrot\"}):\n    print(chunk, end=\"|\", flush=True)\n\n Here|'s| a| silly| joke| about| a| par|rot|:|\n\nWhat| kind| of| teacher| gives| good| advice|?| An| ap|-|parent| (|app|arent|)| one|!||\n\nNOTE\n\nYou do not have to use the LangChain Expression Language to use LangChain and can instead rely on a standard imperative programming approach by caling invoke, batch or stream on each component individually, assigning the results to variables and then using them downstream as you see fit.\n\nIf that works for your needs, then that’s fine by us 👌!\n\nWorking with Input Streams​\n\nWhat if you wanted to stream JSON from the output as it was being generated?\n\nIf you were to rely on json.loads to parse the partial json, the parsing would fail as the partial json wouldn’t be valid json.\n\nYou’d likely be at a complete loss of what to do and claim that it wasn’t possible to stream JSON.\n\nWell, turns out there is a way to do it – the parser needs to operate on the input stream, and attempt to “auto-complete” the partial json into a valid state.\n\nLet’s see such a parser in action to understand what this means.\n\nfrom langchain_core.output_parsers import JsonOutputParser\n\nchain = (\n    model | JsonOutputParser()\n)  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\nasync for text in chain.astream(\n    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n):\n    print(text, flush=True)\n\n{}\n{'countries': []}\n{'countries': [{}]}\n{'countries': [{'name': ''}]}\n{'countries': [{'name': 'France'}]}\n{'countries': [{'name': 'France', 'population': 67}]}\n{'countries': [{'name': 'France', 'population': 6739}]}\n{'countries': [{'name': 'France', 'population': 673915}]}\n{'countries': [{'name': 'France', 'population': 67391582}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': ''}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Sp'}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain'}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 4675}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 467547}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': ''}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan'}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 12}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 12647}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 1264764}]}\n{'countries': [{'name': 'France', 'population': 67391582}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 126476461}]}\n\n\nNow, let’s break streaming. We’ll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON.\n\nDANGER\n\nAny steps in the chain that operate on finalized inputs rather than on input streams can break streaming functionality via stream or astream.\n\nTIP\n\nLater, we will discuss the astream_events API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on finalized inputs.\n\nfrom langchain_core.output_parsers import (\n    JsonOutputParser,\n)\n\n\n# A function that operates on finalized inputs\n# rather than on an input_stream\ndef _extract_country_names(inputs):\n    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n    if not isinstance(inputs, dict):\n        return \"\"\n\n    if \"countries\" not in inputs:\n        return \"\"\n\n    countries = inputs[\"countries\"]\n\n    if not isinstance(countries, list):\n        return \"\"\n\n    country_names = [\n        country.get(\"name\") for country in countries if isinstance(country, dict)\n    ]\n    return country_names\n\n\nchain = model | JsonOutputParser() | _extract_country_names\n\nasync for text in chain.astream(\n    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n):\n    print(text, end=\"|\", flush=True)\n\n['France', 'Spain', 'Japan']|\n\nGenerator Functions​\n\nLe’ts fix the streaming using a generator function that can operate on the input stream.\n\nTIP\n\nA generator function (a function that uses yield) allows writing code that operators on input streams\n\nfrom langchain_core.output_parsers import JsonOutputParser\n\n\nasync def _extract_country_names_streaming(input_stream):\n    \"\"\"A function that operates on input streams.\"\"\"\n    country_names_so_far = set()\n\n    async for input in input_stream:\n        if not isinstance(input, dict):\n            continue\n\n        if \"countries\" not in input:\n            continue\n\n        countries = input[\"countries\"]\n\n        if not isinstance(countries, list):\n            continue\n\n        for country in countries:\n            name = country.get(\"name\")\n            if not name:\n                continue\n            if name not in country_names_so_far:\n                yield name\n                country_names_so_far.add(name)\n\n\nchain = model | JsonOutputParser() | _extract_country_names_streaming\n\nasync for text in chain.astream(\n    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n):\n    print(text, end=\"|\", flush=True)\n\nFrance|Sp|Spain|Japan|\n\nNOTE\n\nBecause the code above is relying on JSON auto-completion, you may see partial names of countries (e.g., Sp and Spain), which is not what one would want for an extraction result!\n\nWe’re focusing on streaming concepts, not necessarily the results of the chains.\n\nNon-streaming components​\n\nSome built-in components like Retrievers do not offer any streaming. What happens if we try to stream them? 🤨\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import OpenAIEmbeddings\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n\nchunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\nchunks\n\n[[Document(page_content='harrison worked at kensho'),\n  Document(page_content='harrison likes spicy food')]]\n\n\nStream just yielded the final result from that component.\n\nThis is OK 🥹! Not all components have to implement streaming – in some cases streaming is either unnecessary, difficult or just doesn’t make sense.\n\nTIP\n\nAn LCEL chain constructed using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.\n\nretrieval_chain = (\n    {\n        \"context\": retriever.with_config(run_name=\"Docs\"),\n        \"question\": RunnablePassthrough(),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nfor chunk in retrieval_chain.stream(\n    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"\n):\n    print(chunk, end=\"|\", flush=True)\n\n Based| on| the| given| context|,| the| only| information| provided| about| where| Harrison| worked| is| that| he| worked| at| Ken|sh|o|.| Since| there| are| no| other| details| provided| about| Ken|sh|o|,| I| do| not| have| enough| information| to| write| 3| additional| made| up| sentences| about| this| place|.| I| can| only| state| that| Harrison| worked| at| Ken|sh|o|.||\n\n\nNow that we’ve seen how stream and astream work, let’s venture into the world of streaming events. 🏞️\n\nUsing Stream Events​\n\nEvent Streaming is a beta API. This API may change a bit based on feedback.\n\nNOTE\n\nIntroduced in langchain-core 0.1.14.\n\nimport langchain_core\n\nlangchain_core.__version__\n\n'0.1.18'\n\n\nFor the astream_events API to work properly:\n\nUse async throughout the code to the extent possible (e.g., async tools etc)\nPropagate callbacks if defining custom functions / runnables\nWhenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens.\nLet us know if anything doesn’t work as expected! :)\nEvent Reference​\n\nBelow is a reference table that shows some events that might be emitted by the various Runnable objects.\n\nNOTE\n\nWhen streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that inputs will often be included only for end events and rather than for start events.\n\nevent\tname\tchunk\tinput\toutput\non_chat_model_start\t[model name]\t\t{“messages”: [[SystemMessage, HumanMessage]]}\t\non_chat_model_stream\t[model name]\tAIMessageChunk(content=“hello”)\t\t\non_chat_model_end\t[model name]\t\t{“messages”: [[SystemMessage, HumanMessage]]}\t{“generations”: […], “llm_output”: None, …}\non_llm_start\t[model name]\t\t{‘input’: ‘hello’}\t\non_llm_stream\t[model name]\t‘Hello’\t\t\non_llm_end\t[model name]\t\t‘Hello human!’\t\non_chain_start\tformat_docs\t\t\t\non_chain_stream\tformat_docs\t“hello world!, goodbye world!”\t\t\non_chain_end\tformat_docs\t\t[Document(…)]\t“hello world!, goodbye world!”\non_tool_start\tsome_tool\t\t{“x”: 1, “y”: “2”}\t\non_tool_stream\tsome_tool\t{“x”: 1, “y”: “2”}\t\t\non_tool_end\tsome_tool\t\t\t{“x”: 1, “y”: “2”}\non_retriever_start\t[retriever name]\t\t{“query”: “hello”}\t\non_retriever_chunk\t[retriever name]\t{documents: […]}\t\t\non_retriever_end\t[retriever name]\t\t{“query”: “hello”}\t{documents: […]}\non_prompt_start\t[template_name]\t\t{“question”: “hello”}\t\non_prompt_end\t[template_name]\t\t{“question”: “hello”}\tChatPromptValue(messages: [SystemMessage, …])\nChat Model​\n\nLet’s start off by looking at the events produced by a chat model.\n\nevents = []\nasync for event in model.astream_events(\"hello\", version=\"v1\"):\n    events.append(event)\n\n/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.\n  warn_beta(\n\nNOTE\n\nHey what’s that funny version=“v1” parameter in the API?! 😾\n\nThis is a beta API, and we’re almost certainly going to make some changes to it.\n\nThis version parameter will allow us to mimimize such breaking changes to your code.\n\nIn short, we are annoying you now, so we don’t have to annoy you later.\n\nLet’s take a look at the few of the start event and a few of the end events.\n\nevents[:3]\n\n[{'event': 'on_chat_model_start',\n  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',\n  'name': 'ChatAnthropic',\n  'tags': [],\n  'metadata': {},\n  'data': {'input': 'hello'}},\n {'event': 'on_chat_model_stream',\n  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',\n  'tags': [],\n  'metadata': {},\n  'name': 'ChatAnthropic',\n  'data': {'chunk': AIMessageChunk(content=' Hello')}},\n {'event': 'on_chat_model_stream',\n  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',\n  'tags': [],\n  'metadata': {},\n  'name': 'ChatAnthropic',\n  'data': {'chunk': AIMessageChunk(content='!')}}]\n\nevents[-2:]\n\n[{'event': 'on_chat_model_stream',\n  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',\n  'tags': [],\n  'metadata': {},\n  'name': 'ChatAnthropic',\n  'data': {'chunk': AIMessageChunk(content='')}},\n {'event': 'on_chat_model_end',\n  'name': 'ChatAnthropic',\n  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',\n  'tags': [],\n  'metadata': {},\n  'data': {'output': AIMessageChunk(content=' Hello!')}}]\n\nChain​\n\nLet’s revisit the example chain that parsed streaming JSON to explore the streaming events API.\n\nchain = (\n    model | JsonOutputParser()\n)  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n\nevents = [\n    event\n    async for event in chain.astream_events(\n        'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n        version=\"v1\",\n    )\n]\n\n\nIf you examine at the first few events, you’ll notice that there are 3 different start events rather than 2 start events.\n\nThe three start events correspond to:\n\nThe chain (model + parser)\nThe model\nThe parser\nevents[:3]\n\n[{'event': 'on_chain_start',\n  'run_id': 'b1074bff-2a17-458b-9e7b-625211710df4',\n  'name': 'RunnableSequence',\n  'tags': [],\n  'metadata': {},\n  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}},\n {'event': 'on_chat_model_start',\n  'name': 'ChatAnthropic',\n  'run_id': '6072be59-1f43-4f1c-9470-3b92e8406a99',\n  'tags': ['seq:step:1'],\n  'metadata': {},\n  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}}},\n {'event': 'on_parser_start',\n  'name': 'JsonOutputParser',\n  'run_id': 'bf978194-0eda-4494-ad15-3a5bfe69cd59',\n  'tags': ['seq:step:2'],\n  'metadata': {},\n  'data': {}}]\n\n\nWhat do you think you’d see if you looked at the last 3 events? what about the middle?\n\nLet’s use this API to take output the stream events from the model and the parser. We’re ignoring start events, end events and events from the chain.\n\nnum_events = 0\n\nasync for event in chain.astream_events(\n    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n    version=\"v1\",\n):\n    kind = event[\"event\"]\n    if kind == \"on_chat_model_stream\":\n        print(\n            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n            flush=True,\n        )\n    if kind == \"on_parser_stream\":\n        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n    num_events += 1\n    if num_events > 30:\n        # Truncate the output\n        print(\"...\")\n        break\n\nChat model chunk: ' Here'\nChat model chunk: ' is'\nChat model chunk: ' the'\nChat model chunk: ' JSON'\nChat model chunk: ' with'\nChat model chunk: ' the'\nChat model chunk: ' requested'\nChat model chunk: ' countries'\nChat model chunk: ' and'\nChat model chunk: ' their'\nChat model chunk: ' populations'\nChat model chunk: ':'\nChat model chunk: '\\n\\n```'\nChat model chunk: 'json'\nParser chunk: {}\nChat model chunk: '\\n{'\nChat model chunk: '\\n '\nChat model chunk: ' \"'\nChat model chunk: 'countries'\nChat model chunk: '\":'\nParser chunk: {'countries': []}\nChat model chunk: ' ['\nChat model chunk: '\\n   '\nParser chunk: {'countries': [{}]}\nChat model chunk: ' {'\n...\n\n\nBecause both the model and the parser support streaming, we see sreaming events from both components in real time! Kind of cool isn’t it? 🦜\n\nFiltering Events​\n\nBecause this API produces so many events, it is useful to be able to filter on events.\n\nYou can filter by either component name, component tags or component type.\n\nBy Name​\nchain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n    {\"run_name\": \"my_parser\"}\n)\n\nmax_events = 0\nasync for event in chain.astream_events(\n    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n    version=\"v1\",\n    include_names=[\"my_parser\"],\n):\n    print(event)\n    max_events += 1\n    if max_events > 10:\n        # Truncate output\n        print(\"...\")\n        break\n\n{'event': 'on_parser_start', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {}}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{}]}}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': ''}]}}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67}]}}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 6739}]}}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 673915}]}}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67391582}]}}}\n{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67391582}, {}]}}}\n...\n\nBy Type​\nchain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n    {\"run_name\": \"my_parser\"}\n)\n\nmax_events = 0\nasync for event in chain.astream_events(\n    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n    version=\"v1\",\n    include_types=[\"chat_model\"],\n):\n    print(event)\n    max_events += 1\n    if max_events > 10:\n        # Truncate output\n        print(\"...\")\n        break\n\n{'event': 'on_chat_model_start', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' Here')}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' is')}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' JSON')}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' with')}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' requested')}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' countries')}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' and')}}\n{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' their')}}\n...\n\nBy Tags​\nCAUTION\n\nTags are inherited by child components of a given runnable.\n\nIf you’re using tags to filter, make sure that this is what you want.\n\nchain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})\n\nmax_events = 0\nasync for event in chain.astream_events(\n    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n    version=\"v1\",\n    include_tags=[\"my_chain\"],\n):\n    print(event)\n    max_events += 1\n    if max_events > 10:\n        # Truncate output\n        print(\"...\")\n        break\n\n{'event': 'on_chain_start', 'run_id': '190875f3-3fb7-49ad-9b6e-f49da22f3e49', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}}\n{'event': 'on_chat_model_start', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}}}\n{'event': 'on_parser_start', 'name': 'JsonOutputParser', 'run_id': '3b5e4ca1-40fe-4a02-9a19-ba2a43a6115c', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {}}\n{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' Here')}}\n{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' is')}}\n{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}\n{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' JSON')}}\n{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' with')}}\n{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}\n{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' requested')}}\n{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' countries')}}\n...\n\nNon-streaming components​\n\nRemember how some components don’t stream well because they don’t operate on input streams?\n\nWhile such components can break streaming of the final output when using astream, astream_events will still yield streaming events from intermediate steps that support streaming!\n\n# Function that does not support streaming.\n# It operates on the finalizes inputs rather than\n# operating on the input stream.\ndef _extract_country_names(inputs):\n    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n    if not isinstance(inputs, dict):\n        return \"\"\n\n    if \"countries\" not in inputs:\n        return \"\"\n\n    countries = inputs[\"countries\"]\n\n    if not isinstance(countries, list):\n        return \"\"\n\n    country_names = [\n        country.get(\"name\") for country in countries if isinstance(country, dict)\n    ]\n    return country_names\n\n\nchain = (\n    model | JsonOutputParser() | _extract_country_names\n)  # This parser only works with OpenAI right now\n\n\nAs expected, the astream API doesn’t work correctly because _extract_country_names doesn’t operate on streams.\n\nasync for chunk in chain.astream(\n    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n):\n    print(chunk, flush=True)\n\n['France', 'Spain', 'Japan']\n\n\nNow, let’s confirm that with astream_events we’re still seeing streaming output from the model and the parser.\n\nnum_events = 0\n\nasync for event in chain.astream_events(\n    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n    version=\"v1\",\n):\n    kind = event[\"event\"]\n    if kind == \"on_chat_model_stream\":\n        print(\n            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n            flush=True,\n        )\n    if kind == \"on_parser_stream\":\n        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n    num_events += 1\n    if num_events > 30:\n        # Truncate the output\n        print(\"...\")\n        break\n\nChat model chunk: ' Here'\nChat model chunk: ' is'\nChat model chunk: ' the'\nChat model chunk: ' JSON'\nChat model chunk: ' with'\nChat model chunk: ' the'\nChat model chunk: ' requested'\nChat model chunk: ' countries'\nChat model chunk: ' and'\nChat model chunk: ' their'\nChat model chunk: ' populations'\nChat model chunk: ':'\nChat model chunk: '\\n\\n```'\nChat model chunk: 'json'\nParser chunk: {}\nChat model chunk: '\\n{'\nChat model chunk: '\\n '\nChat model chunk: ' \"'\nChat model chunk: 'countries'\nChat model chunk: '\":'\nParser chunk: {'countries': []}\nChat model chunk: ' ['\nChat model chunk: '\\n   '\nParser chunk: {'countries': [{}]}\nChat model chunk: ' {'\nChat model chunk: '\\n     '\nChat model chunk: ' \"'\n...\n\nPropagating Callbacks​\nCAUTION\n\nIf you’re using invoking runnables inside your tools, you need to propagate callbacks to the runnable; otherwise, no stream events will be generated.\n\nNOTE\n\nWhen using RunnableLambdas or @chain decorator, callbacks are propagated automatically behind the scenes.\n\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.tools import tool\n\n\ndef reverse_word(word: str):\n    return word[::-1]\n\n\nreverse_word = RunnableLambda(reverse_word)\n\n\n@tool\ndef bad_tool(word: str):\n    \"\"\"Custom tool that doesn't propagate callbacks.\"\"\"\n    return reverse_word.invoke(word)\n\n\nasync for event in bad_tool.astream_events(\"hello\", version=\"v1\"):\n    print(event)\n\n{'event': 'on_tool_start', 'run_id': 'ae7690f8-ebc9-4886-9bbe-cb336ff274f2', 'name': 'bad_tool', 'tags': [], 'metadata': {}, 'data': {'input': 'hello'}}\n{'event': 'on_tool_stream', 'run_id': 'ae7690f8-ebc9-4886-9bbe-cb336ff274f2', 'tags': [], 'metadata': {}, 'name': 'bad_tool', 'data': {'chunk': 'olleh'}}\n{'event': 'on_tool_end', 'name': 'bad_tool', 'run_id': 'ae7690f8-ebc9-4886-9bbe-cb336ff274f2', 'tags': [], 'metadata': {}, 'data': {'output': 'olleh'}}\n\n\nHere’s a re-implementation that does propagate callbacks correctly. You’ll notice that now we’re getting events from the reverse_word runnable as well.\n\n@tool\ndef correct_tool(word: str, callbacks):\n    \"\"\"A tool that correctly propagates callbacks.\"\"\"\n    return reverse_word.invoke(word, {\"callbacks\": callbacks})\n\n\nasync for event in correct_tool.astream_events(\"hello\", version=\"v1\"):\n    print(event)\n\n{'event': 'on_tool_start', 'run_id': '384f1710-612e-4022-a6d4-8a7bb0cc757e', 'name': 'correct_tool', 'tags': [], 'metadata': {}, 'data': {'input': 'hello'}}\n{'event': 'on_chain_start', 'name': 'reverse_word', 'run_id': 'c4882303-8867-4dff-b031-7d9499b39dda', 'tags': [], 'metadata': {}, 'data': {'input': 'hello'}}\n{'event': 'on_chain_end', 'name': 'reverse_word', 'run_id': 'c4882303-8867-4dff-b031-7d9499b39dda', 'tags': [], 'metadata': {}, 'data': {'input': 'hello', 'output': 'olleh'}}\n{'event': 'on_tool_stream', 'run_id': '384f1710-612e-4022-a6d4-8a7bb0cc757e', 'tags': [], 'metadata': {}, 'name': 'correct_tool', 'data': {'chunk': 'olleh'}}\n{'event': 'on_tool_end', 'name': 'correct_tool', 'run_id': '384f1710-612e-4022-a6d4-8a7bb0cc757e', 'tags': [], 'metadata': {}, 'data': {'output': 'olleh'}}\n\n\nIf you’re invoking runnables from within Runnable Lambdas or @chains, then callbacks will be passed automatically on your behalf.\n\nfrom langchain_core.runnables import RunnableLambda\n\n\nasync def reverse_and_double(word: str):\n    return await reverse_word.ainvoke(word) * 2\n\n\nreverse_and_double = RunnableLambda(reverse_and_double)\n\nawait reverse_and_double.ainvoke(\"1234\")\n\nasync for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):\n    print(event)\n\n{'event': 'on_chain_start', 'run_id': '4fe56c7b-6982-4999-a42d-79ba56151176', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'input': '1234'}}\n{'event': 'on_chain_start', 'name': 'reverse_word', 'run_id': '335fe781-8944-4464-8d2e-81f61d1f85f5', 'tags': [], 'metadata': {}, 'data': {'input': '1234'}}\n{'event': 'on_chain_end', 'name': 'reverse_word', 'run_id': '335fe781-8944-4464-8d2e-81f61d1f85f5', 'tags': [], 'metadata': {}, 'data': {'input': '1234', 'output': '4321'}}\n{'event': 'on_chain_stream', 'run_id': '4fe56c7b-6982-4999-a42d-79ba56151176', 'tags': [], 'metadata': {}, 'name': 'reverse_and_double', 'data': {'chunk': '43214321'}}\n{'event': 'on_chain_end', 'name': 'reverse_and_double', 'run_id': '4fe56c7b-6982-4999-a42d-79ba56151176', 'tags': [], 'metadata': {}, 'data': {'output': '43214321'}}\n\n\nAnd with the @chain decorator:\n\nfrom langchain_core.runnables import chain\n\n\n@chain\nasync def reverse_and_double(word: str):\n    return await reverse_word.ainvoke(word) * 2\n\n\nawait reverse_and_double.ainvoke(\"1234\")\n\nasync for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):\n    print(event)\n\n{'event': 'on_chain_start', 'run_id': '7485eedb-1854-429c-a2f8-03d01452daef', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'input': '1234'}}\n{'event': 'on_chain_start', 'name': 'reverse_word', 'run_id': 'e7cddab2-9b95-4e80-abaf-4b2429117835', 'tags': [], 'metadata': {}, 'data': {'input': '1234'}}\n{'event': 'on_chain_end', 'name': 'reverse_word', 'run_id': 'e7cddab2-9b95-4e80-abaf-4b2429117835', 'tags': [], 'metadata': {}, 'data': {'input': '1234', 'output': '4321'}}\n{'event': 'on_chain_stream', 'run_id': '7485eedb-1854-429c-a2f8-03d01452daef', 'tags': [], 'metadata': {}, 'name': 'reverse_and_double', 'data': {'chunk': '43214321'}}\n{'event': 'on_chain_end', 'name': 'reverse_and_double', 'run_id': '7485eedb-1854-429c-a2f8-03d01452daef', 'tags': [], 'metadata': {}, 'data': {'output': '43214321'}}\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nInterface\nNext\nHow to\nUsing Stream\nLLMs and Chat Models\nChains\nWorking with Input Streams\nNon-streaming components\nUsing Stream Events\nEvent Reference\nChat Model\nChain\nFiltering Events\nNon-streaming components\nPropagating Callbacks\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Why use LCEL | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/why",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageWhy use LCEL\nWhy use LCEL\n\nWe recommend reading the LCEL Get started section first.\n\nLCEL makes it easy to build complex chains from basic components. It does this by providing: 1. A unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, …). This makes it possible for chains of LCEL objects to also automatically support these invocations. That is, every chain of LCEL objects is itself an LCEL object. 2. Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more.\n\nTo better understand the value of LCEL, it’s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough we’ll do just that with our basic example from the get started section. We’ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it.\n\n%pip install –upgrade –quiet langchain-core langchain-openai\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n\nprompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nInvoke​\n\nIn the simplest case, we just want to pass in a topic string and get back a joke string:\n\nWithout LCEL​\nfrom typing import List\n\nimport openai\n\n\nprompt_template = \"Tell me a short joke about {topic}\"\nclient = openai.OpenAI()\n\ndef call_chat_model(messages: List[dict]) -> str:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", \n        messages=messages,\n    )\n    return response.choices[0].message.content\n\ndef invoke_chain(topic: str) -> str:\n    prompt_value = prompt_template.format(topic=topic)\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n    return call_chat_model(messages)\n\ninvoke_chain(\"ice cream\")\n\nLCEL​\nfrom langchain_core.runnables import RunnablePassthrough\n\n\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)\noutput_parser = StrOutputParser()\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")\nchain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt\n    | model\n    | output_parser\n)\n\nchain.invoke(\"ice cream\")\n\nStream​\n\nIf we want to stream results instead, we’ll need to change our function:\n\nWithout LCEL​\nfrom typing import Iterator\n\n\ndef stream_chat_model(messages: List[dict]) -> Iterator[str]:\n    stream = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=messages,\n        stream=True,\n    )\n    for response in stream:\n        content = response.choices[0].delta.content\n        if content is not None:\n            yield content\n\ndef stream_chain(topic: str) -> Iterator[str]:\n    prompt_value = prompt.format(topic=topic)\n    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n\n\nfor chunk in stream_chain(\"ice cream\"):\n    print(chunk, end=\"\", flush=True)\n\nLCEL​\nfor chunk in chain.stream(\"ice cream\"):\n    print(chunk, end=\"\", flush=True)\n\nBatch​\n\nIf we want to run on a batch of inputs in parallel, we’ll again need a new function:\n\nWithout LCEL​\nfrom concurrent.futures import ThreadPoolExecutor\n\n\ndef batch_chain(topics: list) -> list:\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        return list(executor.map(invoke_chain, topics))\n\nbatch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\nLCEL​\nchain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\nAsync​\n\nIf we need an asynchronous version:\n\nWithout LCEL​\nasync_client = openai.AsyncOpenAI()\n\nasync def acall_chat_model(messages: List[dict]) -> str:\n    response = await async_client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", \n        messages=messages,\n    )\n    return response.choices[0].message.content\n\nasync def ainvoke_chain(topic: str) -> str:\n    prompt_value = prompt_template.format(topic=topic)\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n    return await acall_chat_model(messages)\n\nawait ainvoke_chain(\"ice cream\")\n\nLCEL​\nchain.ainvoke(\"ice cream\")\n\nLLM instead of chat model​\n\nIf we want to use a completion endpoint instead of a chat endpoint:\n\nWithout LCEL​\ndef call_llm(prompt_value: str) -> str:\n    response = client.completions.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        prompt=prompt_value,\n    )\n    return response.choices[0].text\n\ndef invoke_llm_chain(topic: str) -> str:\n    prompt_value = prompt_template.format(topic=topic)\n    return call_llm(prompt_value)\n\ninvoke_llm_chain(\"ice cream\")\n\nLCEL​\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nllm_chain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt\n    | llm\n    | output_parser\n)\n\nllm_chain.invoke(\"ice cream\")\n\nDifferent model provider​\n\nIf we want to use Anthropic instead of OpenAI:\n\nWithout LCEL​\nimport anthropic\n\nanthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\nanthropic_client = anthropic.Anthropic()\n\ndef call_anthropic(prompt_value: str) -> str:\n    response = anthropic_client.completions.create(\n        model=\"claude-2\",\n        prompt=prompt_value,\n        max_tokens_to_sample=256,\n    )\n    return response.completion    \n\ndef invoke_anthropic_chain(topic: str) -> str:\n    prompt_value = anthropic_template.format(topic=topic)\n    return call_anthropic(prompt_value)\n\ninvoke_anthropic_chain(\"ice cream\")\n\nLCEL​\nfrom langchain_community.chat_models import ChatAnthropic\n\nanthropic = ChatAnthropic(model=\"claude-2\")\nanthropic_chain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt \n    | anthropic\n    | output_parser\n)\n\nanthropic_chain.invoke(\"ice cream\")\n\nRuntime configurability​\n\nIf we wanted to make the choice of chat model or LLM configurable at runtime:\n\nWithout LCEL​\ndef invoke_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> str:\n    if model == \"chat_openai\":\n        return invoke_chain(topic)\n    elif model == \"openai\":\n        return invoke_llm_chain(topic)\n    elif model == \"anthropic\":\n        return invoke_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef stream_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> Iterator[str]:\n    if model == \"chat_openai\":\n        return stream_chain(topic)\n    elif model == \"openai\":\n        # Note we haven't implemented this yet.\n        return stream_llm_chain(topic)\n    elif model == \"anthropic\":\n        # Note we haven't implemented this yet\n        return stream_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef batch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    # You get the idea\n    ...\n\nasync def abatch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    ...\n\ninvoke_configurable_chain(\"ice cream\", model=\"openai\")\nstream = stream_configurable_chain(\n    \"ice_cream\", \n    model=\"anthropic\"\n)\nfor chunk in stream:\n    print(chunk, end=\"\", flush=True)\n\n# batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\n# await ainvoke_configurable_chain(\"ice cream\")\n\nWith LCEL​\nfrom langchain_core.runnables import ConfigurableField\n\n\nconfigurable_model = model.configurable_alternatives(\n    ConfigurableField(id=\"model\"), \n    default_key=\"chat_openai\", \n    openai=llm,\n    anthropic=anthropic,\n)\nconfigurable_chain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt \n    | configurable_model \n    | output_parser\n)\n\nconfigurable_chain.invoke(\n    \"ice cream\", \n    config={\"model\": \"openai\"}\n)\nstream = configurable_chain.stream(\n    \"ice cream\", \n    config={\"model\": \"anthropic\"}\n)\nfor chunk in stream:\n    print(chunk, end=\"\", flush=True)\n\nconfigurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\n# await configurable_chain.ainvoke(\"ice cream\")\n\nLogging​\n\nIf we want to log our intermediate results:\n\nWithout LCEL​\n\nWe’ll print intermediate steps for illustrative purposes\n\ndef invoke_anthropic_chain_with_logging(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = anthropic_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    output = call_anthropic(prompt_value)\n    print(f\"Output: {output}\")\n    return output\n\ninvoke_anthropic_chain_with_logging(\"ice cream\")\n\nLCEL​\n\nEvery component has built-in integrations with LangSmith. If we set the following two environment variables, all chain traces are logged to LangSmith.\n\nimport os\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n\nanthropic_chain.invoke(\"ice cream\")\n\n\nHere’s what our LangSmith trace looks like: https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/r\n\nFallbacks​\n\nIf we wanted to add fallback logic, in case one model API is down:\n\nWithout LCEL​\ndef invoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return invoke_chain(topic)\n    except Exception:\n        return invoke_anthropic_chain(topic)\n\nasync def ainvoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return await ainvoke_chain(topic)\n    except Exception:\n        # Note: we haven't actually implemented this.\n        return ainvoke_anthropic_chain(topic)\n\nasync def batch_chain_with_fallback(topics: List[str]) -> str:\n    try:\n        return batch_chain(topics)\n    except Exception:\n        # Note: we haven't actually implemented this.\n        return batch_anthropic_chain(topics)\n\ninvoke_chain_with_fallback(\"ice cream\")\n# await ainvoke_chain_with_fallback(\"ice cream\")\nbatch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"]))\n\nLCEL​\nfallback_chain = chain.with_fallbacks([anthropic_chain])\n\nfallback_chain.invoke(\"ice cream\")\n# await fallback_chain.ainvoke(\"ice cream\")\nfallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\nFull code comparison​\n\nEven in this simple case, our LCEL chain succinctly packs in a lot of functionality. As chains become more complex, this becomes especially valuable.\n\nWithout LCEL​\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Iterator, List, Tuple\n\nimport anthropic\nimport openai\n\n\nprompt_template = \"Tell me a short joke about {topic}\"\nanthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\nclient = openai.OpenAI()\nasync_client = openai.AsyncOpenAI()\nanthropic_client = anthropic.Anthropic()\n\ndef call_chat_model(messages: List[dict]) -> str:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", \n        messages=messages,\n    )\n    return response.choices[0].message.content\n\ndef invoke_chain(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = prompt_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n    output = call_chat_model(messages)\n    print(f\"Output: {output}\")\n    return output\n\ndef stream_chat_model(messages: List[dict]) -> Iterator[str]:\n    stream = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=messages,\n        stream=True,\n    )\n    for response in stream:\n        content = response.choices[0].delta.content\n        if content is not None:\n            yield content\n\ndef stream_chain(topic: str) -> Iterator[str]:\n    print(f\"Input: {topic}\")\n    prompt_value = prompt.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n    for chunk in stream:\n        print(f\"Token: {chunk}\", end=\"\")\n        yield chunk\n\ndef batch_chain(topics: list) -> list:\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        return list(executor.map(invoke_chain, topics))\n\ndef call_llm(prompt_value: str) -> str:\n    response = client.completions.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        prompt=prompt_value,\n    )\n    return response.choices[0].text\n\ndef invoke_llm_chain(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = promtp_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    output = call_llm(prompt_value)\n    print(f\"Output: {output}\")\n    return output\n\ndef call_anthropic(prompt_value: str) -> str:\n    response = anthropic_client.completions.create(\n        model=\"claude-2\",\n        prompt=prompt_value,\n        max_tokens_to_sample=256,\n    )\n    return response.completion   \n\ndef invoke_anthropic_chain(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = anthropic_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    output = call_anthropic(prompt_value)\n    print(f\"Output: {output}\")\n    return output\n\nasync def ainvoke_anthropic_chain(topic: str) -> str:\n    ...\n\ndef stream_anthropic_chain(topic: str) -> Iterator[str]:\n    ...\n\ndef batch_anthropic_chain(topics: List[str]) -> List[str]:\n    ...\n\ndef invoke_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> str:\n    if model == \"chat_openai\":\n        return invoke_chain(topic)\n    elif model == \"openai\":\n        return invoke_llm_chain(topic)\n    elif model == \"anthropic\":\n        return invoke_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef stream_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> Iterator[str]:\n    if model == \"chat_openai\":\n        return stream_chain(topic)\n    elif model == \"openai\":\n        # Note we haven't implemented this yet.\n        return stream_llm_chain(topic)\n    elif model == \"anthropic\":\n        # Note we haven't implemented this yet\n        return stream_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef batch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    ...\n\nasync def abatch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    ...\n\ndef invoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return invoke_chain(topic)\n    except Exception:\n        return invoke_anthropic_chain(topic)\n\nasync def ainvoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return await ainvoke_chain(topic)\n    except Exception:\n        return ainvoke_anthropic_chain(topic)\n\nasync def batch_chain_with_fallback(topics: List[str]) -> str:\n    try:\n        return batch_chain(topics)\n    except Exception:\n        return batch_anthropic_chain(topics)\n\nLCEL​\nimport os\n\nfrom langchain_community.chat_models import ChatAnthropic\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai import OpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough, ConfigurableField\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)\nchat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\nopenai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nanthropic = ChatAnthropic(model=\"claude-2\")\nmodel = (\n    chat_openai\n    .with_fallbacks([anthropic])\n    .configurable_alternatives(\n        ConfigurableField(id=\"model\"),\n        default_key=\"chat_openai\",\n        openai=openai,\n        anthropic=anthropic,\n    )\n)\n\nchain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt \n    | model \n    | StrOutputParser()\n)\n\nNext steps​\n\nTo continue learning about LCEL, we recommend: - Reading up on the full LCEL Interface, which we’ve only partially covered here. - Exploring the How-to section to learn about additional composition primitives that LCEL provides. - Looking through the Cookbook section to see LCEL in action for common use cases. A good next use case to look at would be Retrieval-augmented generation.\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nGet started\nNext\nInterface\nInvoke\nStream\nBatch\nAsync\nLLM instead of chat model\nDifferent model provider\nRuntime configurability\nLogging\nFallbacks\nFull code comparison\nNext steps\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "Get started | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/get_started",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression LanguageGet started\nGet started\n\nLCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.\n\nBasic example: prompt + model + output parser​\n\nThe most basic and common use case is chaining a prompt template and a model together. To see how this works, let’s create a chain that takes a topic and generates a joke:\n\n%pip install –upgrade –quiet langchain-core langchain-community langchain-openai\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\nmodel = ChatOpenAI(model=\"gpt-4\")\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nchain.invoke({\"topic\": \"ice cream\"})\n\n\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always drip when things heat up!\"\n\n\nNotice this line of this code, where we piece together then different components into a single chain using LCEL:\n\nchain = prompt | model | output_parser\n\n\nThe | symbol is similar to a unix pipe operator, which chains together the different components feeds the output from one component as input into the next component.\n\nIn this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. Let’s take a look at each component individually to really understand what’s going on.\n\n1. Prompt​\n\nprompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string.\n\nprompt_value = prompt.invoke({\"topic\": \"ice cream\"})\nprompt_value\n\nChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])\n\nprompt_value.to_messages()\n\n[HumanMessage(content='tell me a short joke about ice cream')]\n\nprompt_value.to_string()\n\n'Human: tell me a short joke about ice cream'\n\n2. Model​\n\nThe PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage.\n\nmessage = model.invoke(prompt_value)\nmessage\n\nAIMessage(content=\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always bring a melt down!\")\n\n\nIf our model was an LLM, it would output a string.\n\nfrom langchain_openai.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nllm.invoke(prompt_value)\n\n'\\n\\nRobot: Why did the ice cream truck break down? Because it had a meltdown!'\n\n3. Output parser​\n\nAnd lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The StrOutputParser specifically simple converts any input into a string.\n\noutput_parser.invoke(message)\n\n\"Why did the ice cream go to therapy? \\n\\nBecause it had too many toppings and couldn't find its cone-fidence!\"\n\n4. Entire Pipeline​\n\nTo follow the steps along:\n\nWe pass in user input on the desired topic as {\"topic\": \"ice cream\"}\nThe prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt.\nThe model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object.\nFinally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\nDict\nPromptValue\nChatMessage\nString\nInput: topic=ice cream\nPromptTemplate\nChatModel\nStrOutputParser\nResult\n\nNote that if you’re curious about the output of any components, you can always test out a smaller version of the chain such as prompt or prompt | model to see the intermediate results:\n\ninput = {\"topic\": \"ice cream\"}\n\nprompt.invoke(input)\n# > ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])\n\n(prompt | model).invoke(input)\n# > AIMessage(content=\"Why did the ice cream go to therapy?\\nBecause it had too many toppings and couldn't cone-trol itself!\")\n\nRAG Search Example​\n\nFor our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions.\n\n# Requires:\n# pip install langchain docarray tiktoken\n\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\nvectorstore = DocArrayInMemorySearch.from_texts(\n    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\nchain = setup_and_retrieval | prompt | model | output_parser\n\nchain.invoke(\"where did harrison work?\")\n\n\nIn this case, the composed chain is:\n\nchain = setup_and_retrieval | prompt | model | output_parser\n\n\nTo explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context.\n\nAs a preliminary step, we’ve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:\n\nretriever.invoke(\"where did harrison work?\")\n\n\nWe then use the RunnableParallel to prepare the expected inputs into the prompt by using the entries for the retrieved documents as well as the original user question, using the retriever for document search, and RunnablePassthrough to pass the user’s question:\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\n\n\nTo review, the complete chain is:\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\nchain = setup_and_retrieval | prompt | model | output_parser\n\n\nWith the flow being:\n\nThe first steps create a RunnableParallel object with two entries. The first entry, context will include the document results fetched by the retriever. The second entry, question will contain the user’s original question. To pass on the question, we use RunnablePassthrough to copy this entry.\nFeed the dictionary from the step above to the prompt component. It then takes the user input which is question as well as the retrieved document which is context to construct a prompt and output a PromptValue.\nThe model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object.\nFinally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\nQuestion\nQuestion\ncontext=retrieved docs\nquestion=Question\nPromptValue\nChatMessage\nString\nQuestion\nRunnableParallel\nRetriever\nRunnablePassThrough\nPromptTemplate\nChatModel\nStrOutputParser\nResult\nNext steps​\n\nWe recommend reading our Why use LCEL section next to see a side-by-side comparison of the code needed to produce common functionality with and without LCEL.\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nLangChain Expression Language (LCEL)\nNext\nWhy use LCEL\nBasic example: prompt + model + output parser\n1. Prompt\n2. Model\n3. Output parser\n4. Entire Pipeline\nRAG Search Example\nNext steps\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  },
  {
    "title": "LangChain Expression Language (LCEL) | 🦜️🔗 Langchain",
    "url": "https://python.langchain.com/docs/expression_language/",
    "html": "Skip to main content\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nStreaming\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nChains\nMore\nLangServe\nLangSmith\nLangGraph\nLangChain Expression Language\nLangChain Expression Language (LCEL)\n\nLangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\n\nStreaming support When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\n\nAsync support Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.\n\nOptimized parallel execution Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\n\nRetries and fallbacks Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\n\nAccess intermediate results For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.\n\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\n\nSeamless LangSmith tracing integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\n\nSeamless LangServe deployment integration Any chain created with LCEL can be easily deployed using LangServe.\n\nHelp us out by providing feedback on this documentation page:\nPrevious\nSecurity\nNext\nGet started\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nYouTube\nCopyright © 2024 LangChain, Inc."
  }
]